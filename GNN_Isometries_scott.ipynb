{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is a library of functions for numerics of the incoherence simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using JLD.@load in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "using BSON: @load\n",
    "using Flux\n",
    "using Flux.Optimise\n",
    "using Flux.Optimise: update!\n",
    "using Flux.Data: DataLoader\n",
    "using ImageFiltering\n",
    "using Images\n",
    "using ImageIO\n",
    "using MLDatasets: FashionMNIST\n",
    "using LinearAlgebra\n",
    "using MLDatasets\n",
    "using Plots\n",
    "using Zygote\n",
    "using FFTW\n",
    "using Distributions\n",
    "using SparseArrays\n",
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimise"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    optimise(init_z, loss, opt, tolerance, [out_toggle = 0,][max_iter = 1_000_000])\n",
    "\n",
    "    Optimization that stops when the gradient is small enough\n",
    "\"\"\"\n",
    "function optimise(opt, loss; init, tolerance, out_toggle = 0, max_iter = 500_000)\n",
    "    tol2 = tolerance^2\n",
    "    z = init\n",
    "    ps = Flux.params(z)\n",
    "    iter=1\n",
    "    succ_error = Inf\n",
    "    while succ_error > tol2 \n",
    "        if iter > max_iter\n",
    "            @warn \"Max num. iterations reached\"\n",
    "            return nothing\n",
    "        end\n",
    "        grads = gradient(() -> loss(z), ps)\n",
    "        update!(opt, z, grads[z])\n",
    "        succ_error = sum(abs2, grads[z])\n",
    "        if out_toggle != 0 && iter % out_toggle == 0\n",
    "            println(\"====> In Gradient: Iteration: $iter grad_size: $(sqrt(succ_error)) tolerance: $tolerance  Loss: \", string(loss(z)))\n",
    "        end\n",
    "        iter += 1\n",
    "    end\n",
    "    return z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_aligned_models (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function matrix_alignment(mat, basis)\n",
    "    coherence = 0\n",
    "    for v in eachcol(basis)\n",
    "        guess = dot(mat*transpose(mat)*v, v)/ (norm(mat*transpose(mat)*v) * norm(v))\n",
    "        coherence = max(guess, coherence)\n",
    "    end\n",
    "    return coherence\n",
    "end\n",
    "\n",
    "function get_rand_model(k,mid,n)\n",
    "    W₂ = randn(n, mid)/sqrt(n)\n",
    "    W₁ = randn(mid,k)/sqrt(mid)\n",
    "    return x -> W₂*relu.(W₁*x)\n",
    "end\n",
    "\n",
    "function get_aligned_models(k,mid,n, β_array)\n",
    "    W₁ = randn(mid, k)/sqrt(mid)\n",
    "    full_F = dct(diagm(ones(n)),2)\n",
    "    F = full_F[:,1:mid]\n",
    "    A = randn(n, mid)/sqrt(n)\n",
    "    models  = []\n",
    "    for β in β_array\n",
    "        W₂ = β*F + (1-β)*A\n",
    "        push!(models, (x -> W₂*relu.(W₁*x), matrix_alignment(W₂, full_F)))\n",
    "    end\n",
    "    return models\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recover_signal (generic function with 1 method)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function recover_signal(measurements, measure, model, code_dim)\n",
    "    function loss(signal_guess)\n",
    "        return sum(abs2, measure(model(signal_guess)) - measurements)\n",
    "    end\n",
    "    opt_code = optimise( Flux.Optimise.Descent(4e-5), loss, init=randn(code_dim)/sqrt(code_dim), tolerance=1e-6, out_toggle=20000)\n",
    "    return opt_code != nothing ? model(opt_code) : nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=12\n",
    "mid=15\n",
    "n=50\n",
    "aimed_m = 20\n",
    "model = get_aligned_models(k, mid, n, [1.0])[1][1]\n",
    "x₀ = model(randn(k)/sqrt(k))\n",
    "\n",
    "m, A = sample_fourier(aimed_m, n)\n",
    "y = A*x₀\n",
    "println(\"signal norm: \", norm(x₀))\n",
    "println(\"with measurements norm: \", norm(y))\n",
    "x̂ = recover_signal(y, x -> A*x, model, k)\n",
    "#println(\"recovered: \", x̂, \" with measurements \", A*x̂)\n",
    "println(\"norm of the error: \", norm(x̂ - x₀))\n",
    "println(\"norm of recovered: \", norm(x̂))\n",
    "#does not seem to work with gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recovery_error (generic function with 1 method)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function recovery_error(x₀, model, A, k)\n",
    "    y = A*x₀\n",
    "    x̂ = recover_signal(y, x -> A*x, model, k)\n",
    "    if x̂ == nothing\n",
    "        return nothing\n",
    "    end\n",
    "    return norm(x̂ - x₀)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_fourier (generic function with 1 method)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sample_fourier(aimed_m, n)\n",
    "    F = dct(diagm(ones(n)),2)\n",
    "    sampling = rand(Bernoulli(aimed_m/n), n)\n",
    "    true_m = sum(sampling)\n",
    "    return (true_m, F[sampling,:])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 0.0001952128224900819 tolerance: 1.0e-6  Loss: 0.27906313435738805\n",
      "====> In Gradient: Iteration: 40000 grad_size: 5.856072974338081e-5 tolerance: 1.0e-6  Loss: 0.27903617339706555\n",
      "====> In Gradient: Iteration: 60000 grad_size: 6.035352629912418e-6 tolerance: 1.0e-6  Loss: 0.27903215273330695\n",
      "====> In Gradient: Iteration: 80000 grad_size: 1.988323189738787e-6 tolerance: 1.0e-6  Loss: 0.2790318330509561\n"
     ]
    }
   ],
   "source": [
    "β_array = 0:0.5:1\n",
    "num_signals = 2\n",
    "k=15\n",
    "mid=20\n",
    "n=100\n",
    "aimed_m = 35\n",
    "plot_pairs = []\n",
    "m, A = sample_fourier(aimed_m, n)\n",
    "\n",
    "for (model, alignment) in get_aligned_models(k, mid,n, β_array)\n",
    "    signal_errors = []\n",
    "    for i in 1:num_signals\n",
    "        x₀ = randn(n)/sqrt(n)\n",
    "        push!(signal_errors, recovery_error(x₀, model, A, k))\n",
    "    end\n",
    "    push!(plot_pairs, (alignment, copy(signal_errors)))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "experiment_1 (generic function with 1 method)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function experiment_1()\n",
    "    β_array = 0:0.05:1\n",
    "    num_signals = 10\n",
    "    k=8\n",
    "    mid=16\n",
    "    n= 32\n",
    "    aimed_m = 16\n",
    "    plot_pairs = []\n",
    "    m, A = sample_fourier(aimed_m, n)\n",
    "    for (model, alignment) in get_aligned_models(k, mid,n, β_array)\n",
    "        signal_errors = []\n",
    "        for i in 1:num_signals\n",
    "            x₀ = randn(n)/sqrt(n)\n",
    "            push!(signal_errors, recovery_error(x₀, model, A, k))\n",
    "        end\n",
    "        push!(plot_pairs, (alignment, copy(signal_errors)))\n",
    "    end\n",
    "    return plot_pairs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 6.075129912135128e-6 tolerance: 1.0e-6  Loss: 0.3352366891108532\n",
      "====> In Gradient: Iteration: 40000 grad_size: 4.591219172808774e-6 tolerance: 1.0e-6  Loss: 0.3211911492121287\n",
      "====> In Gradient: Iteration: 60000 grad_size: 3.5846039620405412e-6 tolerance: 1.0e-6  Loss: 0.3129194875447798\n",
      "====> In Gradient: Iteration: 80000 grad_size: 2.87835073070835e-6 tolerance: 1.0e-6  Loss: 0.30774193033634856\n",
      "====> In Gradient: Iteration: 100000 grad_size: 1.036602740794211e-5 tolerance: 1.0e-6  Loss: 0.2868459518695289\n",
      "====> In Gradient: Iteration: 120000 grad_size: 5.771265682304706e-6 tolerance: 1.0e-6  Loss: 0.25666556263683393\n",
      "====> In Gradient: Iteration: 140000 grad_size: 4.824958743784297e-6 tolerance: 1.0e-6  Loss: 0.24032765325832933\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.6522198271102506e-6 tolerance: 1.0e-6  Loss: 0.23150519410309278\n",
      "====> In Gradient: Iteration: 180000 grad_size: 2.956022955376718e-6 tolerance: 1.0e-6  Loss: 0.22612507468437842\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.540297087289264e-6 tolerance: 1.0e-6  Loss: 0.22239588405677987\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.2981874923296447e-6 tolerance: 1.0e-6  Loss: 0.21970920714599168\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.187815934585082e-6 tolerance: 1.0e-6  Loss: 0.21763514267336848\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.6834186330624743e-6 tolerance: 1.0e-6  Loss: 0.2159413753301605\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.520488623054761e-6 tolerance: 1.0e-6  Loss: 0.2144991508468794\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.386470703377345e-6 tolerance: 1.0e-6  Loss: 0.21323405515585434\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.2440922697755133e-6 tolerance: 1.0e-6  Loss: 0.21210126268393803\n",
      "====> In Gradient: Iteration: 340000 grad_size: 2.173301309534502e-6 tolerance: 1.0e-6  Loss: 0.21107293461733143\n",
      "====> In Gradient: Iteration: 360000 grad_size: 2.0847651858233624e-6 tolerance: 1.0e-6  Loss: 0.21013063107784904\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.00449001655079e-6 tolerance: 1.0e-6  Loss: 0.20926180040943593\n",
      "====> In Gradient: Iteration: 400000 grad_size: 2.449447077021218e-6 tolerance: 1.0e-6  Loss: 0.2084575693124776\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.847052021641732e-6 tolerance: 1.0e-6  Loss: 0.20775893512596244\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.7166576086860073e-6 tolerance: 1.0e-6  Loss: 0.20713482918780324\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.642923298421424e-6 tolerance: 1.0e-6  Loss: 0.206571723389534\n",
      "====> In Gradient: Iteration: 480000 grad_size: 1.5774223477286702e-6 tolerance: 1.0e-6  Loss: 0.2060623158639014\n",
      "====> In Gradient: Iteration: 500000 grad_size: 1.5181604481236255e-6 tolerance: 1.0e-6  Loss: 0.2056006221209316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 9.964779524508077e-6 tolerance: 1.0e-6  Loss: 0.6210842184076643\n",
      "====> In Gradient: Iteration: 40000 grad_size: 6.390693870544772e-6 tolerance: 1.0e-6  Loss: 0.5911183978826376\n",
      "====> In Gradient: Iteration: 60000 grad_size: 5.374237463630806e-6 tolerance: 1.0e-6  Loss: 0.576542094846838\n",
      "====> In Gradient: Iteration: 80000 grad_size: 4.605433209529434e-6 tolerance: 1.0e-6  Loss: 0.5664111682978317\n",
      "====> In Gradient: Iteration: 100000 grad_size: 4.141607456824979e-6 tolerance: 1.0e-6  Loss: 0.558389698012828\n",
      "====> In Gradient: Iteration: 120000 grad_size: 3.828271027949752e-6 tolerance: 1.0e-6  Loss: 0.5515601001406009\n",
      "====> In Gradient: Iteration: 140000 grad_size: 3.591712961734433e-6 tolerance: 1.0e-6  Loss: 0.5455260670484613\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.397375244014204e-6 tolerance: 1.0e-6  Loss: 0.5400946125750755\n",
      "====> In Gradient: Iteration: 180000 grad_size: 5.186277950575363e-6 tolerance: 1.0e-6  Loss: 0.5351565621579005\n",
      "====> In Gradient: Iteration: 200000 grad_size: 5.197787812376047e-6 tolerance: 1.0e-6  Loss: 0.5306401093087616\n",
      "====> In Gradient: Iteration: 220000 grad_size: 5.527418387405817e-6 tolerance: 1.0e-6  Loss: 0.5261774946263771\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.9259687872361646e-6 tolerance: 1.0e-6  Loss: 0.5216695634952072\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.714327195753243e-6 tolerance: 1.0e-6  Loss: 0.5179117924254383\n",
      "====> In Gradient: Iteration: 280000 grad_size: 5.10686495138277e-6 tolerance: 1.0e-6  Loss: 0.514654987413985\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.459162420871654e-6 tolerance: 1.0e-6  Loss: 0.5118167460058023\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.266701648848408e-6 tolerance: 1.0e-6  Loss: 0.5092973286875097\n",
      "====> In Gradient: Iteration: 340000 grad_size: 2.1533631628350677e-6 tolerance: 1.0e-6  Loss: 0.5070459672582245\n",
      "====> In Gradient: Iteration: 360000 grad_size: 2.0522695356928606e-6 tolerance: 1.0e-6  Loss: 0.5050245756714707\n",
      "====> In Gradient: Iteration: 380000 grad_size: 4.7623469033253315e-6 tolerance: 1.0e-6  Loss: 0.5032030914820713\n",
      "====> In Gradient: Iteration: 400000 grad_size: 1.879793187800427e-6 tolerance: 1.0e-6  Loss: 0.5015567858220503\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.8061857551800782e-6 tolerance: 1.0e-6  Loss: 0.5000653058914029\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.7929530672853833e-6 tolerance: 1.0e-6  Loss: 0.49871118641991097\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.6806375451754237e-6 tolerance: 1.0e-6  Loss: 0.4974792341047722\n",
      "====> In Gradient: Iteration: 480000 grad_size: 1.6276089250164824e-6 tolerance: 1.0e-6  Loss: 0.49635645912056364\n",
      "====> In Gradient: Iteration: 500000 grad_size: 1.660480082639305e-6 tolerance: 1.0e-6  Loss: 0.49533125227199615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 5.2378556789378025e-6 tolerance: 1.0e-6  Loss: 0.5575628554379288\n",
      "====> In Gradient: Iteration: 40000 grad_size: 5.9552731496392484e-6 tolerance: 1.0e-6  Loss: 0.549858775605879\n",
      "====> In Gradient: Iteration: 60000 grad_size: 2.8699359190951195e-6 tolerance: 1.0e-6  Loss: 0.5459717322188836\n",
      "====> In Gradient: Iteration: 80000 grad_size: 2.333815603127386e-6 tolerance: 1.0e-6  Loss: 0.543749626182733\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.2934911424778045e-6 tolerance: 1.0e-6  Loss: 0.5412015346440234\n",
      "====> In Gradient: Iteration: 120000 grad_size: 2.563047917699613e-6 tolerance: 1.0e-6  Loss: 0.5377159253031735\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.004338489732184e-6 tolerance: 1.0e-6  Loss: 0.5353865827917562\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.3271363609182226e-6 tolerance: 1.0e-6  Loss: 0.534077424096226\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.0445015564087991e-6 tolerance: 1.0e-6  Loss: 0.5333955570257077\n",
      "====> In Gradient: Iteration: 20000 grad_size: 1.3944429403799497e-5 tolerance: 1.0e-6  Loss: 0.20450135272456232\n",
      "====> In Gradient: Iteration: 40000 grad_size: 6.279521200259089e-6 tolerance: 1.0e-6  Loss: 0.16362441530827446\n",
      "====> In Gradient: Iteration: 60000 grad_size: 5.25774843070001e-6 tolerance: 1.0e-6  Loss: 0.15314124246280722\n",
      "====> In Gradient: Iteration: 80000 grad_size: 5.264640972890044e-6 tolerance: 1.0e-6  Loss: 0.14734081548059608\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.2019744728999865e-6 tolerance: 1.0e-6  Loss: 0.14427985563770038\n",
      "====> In Gradient: Iteration: 120000 grad_size: 1.888741848456071e-6 tolerance: 1.0e-6  Loss: 0.142312994632059\n",
      "====> In Gradient: Iteration: 140000 grad_size: 1.6909835114395646e-6 tolerance: 1.0e-6  Loss: 0.14091701672672693\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.5541546886404779e-6 tolerance: 1.0e-6  Loss: 0.1398952459263896\n",
      "====> In Gradient: Iteration: 180000 grad_size: 4.189936630894782e-6 tolerance: 1.0e-6  Loss: 0.13913672822785297\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.396576451697836e-6 tolerance: 1.0e-6  Loss: 0.13856852322945545\n",
      "====> In Gradient: Iteration: 220000 grad_size: 1.3568173469865387e-6 tolerance: 1.0e-6  Loss: 0.13814008673922687\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.334029706369952e-6 tolerance: 1.0e-6  Loss: 0.13781527110311334\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.3230482144459278e-6 tolerance: 1.0e-6  Loss: 0.13756774637367972\n",
      "====> In Gradient: Iteration: 280000 grad_size: 3.829703264839701e-6 tolerance: 1.0e-6  Loss: 0.13753881271384935\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.3139817035764612e-6 tolerance: 1.0e-6  Loss: 0.13752704738928917\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.024790540110557e-6 tolerance: 1.0e-6  Loss: 0.13751817021885482\n",
      "====> In Gradient: Iteration: 340000 grad_size: 3.9902965493996376e-6 tolerance: 1.0e-6  Loss: 0.13751066056045955\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.3077213074580706e-6 tolerance: 1.0e-6  Loss: 0.13750396355847863\n",
      "====> In Gradient: Iteration: 380000 grad_size: 3.992497189527386e-6 tolerance: 1.0e-6  Loss: 0.13749817867292624\n",
      "====> In Gradient: Iteration: 400000 grad_size: 1.0143029355504278e-6 tolerance: 1.0e-6  Loss: 0.13749294944146292\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.300276384644298e-6 tolerance: 1.0e-6  Loss: 0.13748827169944738\n",
      "====> In Gradient: Iteration: 440000 grad_size: 3.995696112459901e-6 tolerance: 1.0e-6  Loss: 0.13748408174459195\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.0088964788783564e-6 tolerance: 1.0e-6  Loss: 0.13748021664543003\n",
      "====> In Gradient: Iteration: 480000 grad_size: 1.0069895902289037e-6 tolerance: 1.0e-6  Loss: 0.13747670690208627\n",
      "====> In Gradient: Iteration: 500000 grad_size: 1.005219658126291e-6 tolerance: 1.0e-6  Loss: 0.13747347009790517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.2425430875835384e-5 tolerance: 1.0e-6  Loss: 0.6893287132148725\n",
      "====> In Gradient: Iteration: 40000 grad_size: 6.532300656710712e-6 tolerance: 1.0e-6  Loss: 0.66586818467868\n",
      "====> In Gradient: Iteration: 60000 grad_size: 5.802933890718659e-6 tolerance: 1.0e-6  Loss: 0.6531493221275703\n",
      "====> In Gradient: Iteration: 80000 grad_size: 5.326002819712831e-6 tolerance: 1.0e-6  Loss: 0.6427933130283487\n",
      "====> In Gradient: Iteration: 100000 grad_size: 1.5027307174576167e-5 tolerance: 1.0e-6  Loss: 0.6366405582528212\n",
      "====> In Gradient: Iteration: 120000 grad_size: 1.6705690958167855e-5 tolerance: 1.0e-6  Loss: 0.6051940778592358\n",
      "====> In Gradient: Iteration: 140000 grad_size: 1.0664696058646002e-5 tolerance: 1.0e-6  Loss: 0.5928396781105832\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.0064432080679687e-5 tolerance: 1.0e-6  Loss: 0.5867630431665122\n",
      "====> In Gradient: Iteration: 180000 grad_size: 8.923635783986538e-6 tolerance: 1.0e-6  Loss: 0.5830976348225945\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.8681451628829427e-5 tolerance: 1.0e-6  Loss: 0.5805822375562343\n",
      "====> In Gradient: Iteration: 220000 grad_size: 5.433277922933692e-6 tolerance: 1.0e-6  Loss: 0.5786916525156365\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.88777971686455e-5 tolerance: 1.0e-6  Loss: 0.5771771242170575\n",
      "====> In Gradient: Iteration: 260000 grad_size: 3.915047752257067e-6 tolerance: 1.0e-6  Loss: 0.5758949288281563\n",
      "====> In Gradient: Iteration: 280000 grad_size: 9.149935388225401e-6 tolerance: 1.0e-6  Loss: 0.5747727514074782\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.9088358400782428e-5 tolerance: 1.0e-6  Loss: 0.5737640166628443\n",
      "====> In Gradient: Iteration: 320000 grad_size: 9.050255425340306e-6 tolerance: 1.0e-6  Loss: 0.5728360191843088\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.7436527791428774e-5 tolerance: 1.0e-6  Loss: 0.571978722034958\n",
      "====> In Gradient: Iteration: 360000 grad_size: 4.472476572782155e-6 tolerance: 1.0e-6  Loss: 0.5711741576269882\n",
      "====> In Gradient: Iteration: 380000 grad_size: 8.939609552040645e-6 tolerance: 1.0e-6  Loss: 0.5704217239234025\n",
      "====> In Gradient: Iteration: 400000 grad_size: 1.647769482615279e-5 tolerance: 1.0e-6  Loss: 0.5697133104997186\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.942314736701568e-5 tolerance: 1.0e-6  Loss: 0.5690451381493526\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.6615459060019675e-5 tolerance: 1.0e-6  Loss: 0.5684134402307816\n",
      "====> In Gradient: Iteration: 460000 grad_size: 8.832030935230622e-6 tolerance: 1.0e-6  Loss: 0.5678179401831074\n",
      "====> In Gradient: Iteration: 480000 grad_size: 3.941116043057398e-6 tolerance: 1.0e-6  Loss: 0.5672518320809172\n",
      "====> In Gradient: Iteration: 500000 grad_size: 3.863121993827745e-6 tolerance: 1.0e-6  Loss: 0.5667194389354571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 8.951259809166154e-6 tolerance: 1.0e-6  Loss: 0.5144069438935014\n",
      "====> In Gradient: Iteration: 40000 grad_size: 3.5860198844277294e-6 tolerance: 1.0e-6  Loss: 0.5038175136408684\n",
      "====> In Gradient: Iteration: 60000 grad_size: 6.7946576913146095e-6 tolerance: 1.0e-6  Loss: 0.4992759581971462\n",
      "====> In Gradient: Iteration: 80000 grad_size: 4.211591308863786e-6 tolerance: 1.0e-6  Loss: 0.4914621334629818\n",
      "====> In Gradient: Iteration: 100000 grad_size: 3.7122052209733096e-6 tolerance: 1.0e-6  Loss: 0.484002084557614\n",
      "====> In Gradient: Iteration: 120000 grad_size: 3.3727376179649915e-6 tolerance: 1.0e-6  Loss: 0.4782606966142754\n",
      "====> In Gradient: Iteration: 140000 grad_size: 3.1242629811106045e-6 tolerance: 1.0e-6  Loss: 0.47365496487429193\n",
      "====> In Gradient: Iteration: 160000 grad_size: 2.936471701922495e-6 tolerance: 1.0e-6  Loss: 0.4698505977715374\n",
      "====> In Gradient: Iteration: 180000 grad_size: 2.791599033642652e-6 tolerance: 1.0e-6  Loss: 0.4666321450092014\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.6773270607900286e-6 tolerance: 1.0e-6  Loss: 0.4638532297907773\n",
      "====> In Gradient: Iteration: 220000 grad_size: 5.412128021988225e-6 tolerance: 1.0e-6  Loss: 0.46141237585497685\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.5071474158048755e-6 tolerance: 1.0e-6  Loss: 0.4592372459720462\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.4398557833252403e-6 tolerance: 1.0e-6  Loss: 0.4572761619513788\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.3795883663709504e-6 tolerance: 1.0e-6  Loss: 0.4554916743776575\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.323968120041403e-6 tolerance: 1.0e-6  Loss: 0.45385553356777275\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.2714452806632153e-6 tolerance: 1.0e-6  Loss: 0.45234663275829157\n",
      "====> In Gradient: Iteration: 340000 grad_size: 2.220978294073215e-6 tolerance: 1.0e-6  Loss: 0.45094863532154744\n",
      "====> In Gradient: Iteration: 360000 grad_size: 2.1718198497837626e-6 tolerance: 1.0e-6  Loss: 0.44964864282107225\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.123643267116994e-6 tolerance: 1.0e-6  Loss: 0.4484361598741746\n",
      "====> In Gradient: Iteration: 400000 grad_size: 2.076182120127317e-6 tolerance: 1.0e-6  Loss: 0.447302786431875\n",
      "====> In Gradient: Iteration: 420000 grad_size: 2.029177209335004e-6 tolerance: 1.0e-6  Loss: 0.4462413001567492\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.9826613107299964e-6 tolerance: 1.0e-6  Loss: 0.4452457857427261\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.93655409952821e-6 tolerance: 1.0e-6  Loss: 0.4443108468614719\n",
      "====> In Gradient: Iteration: 480000 grad_size: 5.084769813439097e-6 tolerance: 1.0e-6  Loss: 0.44343210448091347\n",
      "====> In Gradient: Iteration: 500000 grad_size: 5.087650733300164e-6 tolerance: 1.0e-6  Loss: 0.4426052075977319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.4568933916084341e-5 tolerance: 1.0e-6  Loss: 0.5427491810306385\n",
      "====> In Gradient: Iteration: 40000 grad_size: 8.365750819776798e-6 tolerance: 1.0e-6  Loss: 0.47514011213612245\n",
      "====> In Gradient: Iteration: 20000 grad_size: 9.178127948709198e-6 tolerance: 1.0e-6  Loss: 0.4503471639915937\n",
      "====> In Gradient: Iteration: 40000 grad_size: 3.620612259362871e-6 tolerance: 1.0e-6  Loss: 0.439384689655321\n",
      "====> In Gradient: Iteration: 60000 grad_size: 3.141559151643617e-6 tolerance: 1.0e-6  Loss: 0.4337278163733512\n",
      "====> In Gradient: Iteration: 80000 grad_size: 5.079922994421464e-6 tolerance: 1.0e-6  Loss: 0.42844253985976244\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.413168028980254e-6 tolerance: 1.0e-6  Loss: 0.42232660692093155\n",
      "====> In Gradient: Iteration: 120000 grad_size: 1.6903047083049211e-6 tolerance: 1.0e-6  Loss: 0.4204937220253433\n",
      "====> In Gradient: Iteration: 140000 grad_size: 1.516098930724917e-6 tolerance: 1.0e-6  Loss: 0.41922870897092274\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.4241807824317062e-6 tolerance: 1.0e-6  Loss: 0.4181562779881478\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.3652060140929052e-6 tolerance: 1.0e-6  Loss: 0.41718797169336236\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.6606702175946232e-6 tolerance: 1.0e-6  Loss: 0.41649115166398426\n",
      "====> In Gradient: Iteration: 20000 grad_size: 7.333454123250068e-6 tolerance: 1.0e-6  Loss: 0.5595115962699315\n",
      "====> In Gradient: Iteration: 40000 grad_size: 5.177812206686723e-6 tolerance: 1.0e-6  Loss: 0.5402733183025491\n",
      "====> In Gradient: Iteration: 60000 grad_size: 1.1143250125854966e-5 tolerance: 1.0e-6  Loss: 0.514143073307499\n",
      "====> In Gradient: Iteration: 80000 grad_size: 8.010911984115355e-6 tolerance: 1.0e-6  Loss: 0.46954941972446473\n",
      "====> In Gradient: Iteration: 100000 grad_size: 6.421714753505328e-6 tolerance: 1.0e-6  Loss: 0.4440429948649264\n",
      "====> In Gradient: Iteration: 120000 grad_size: 5.479173987775758e-6 tolerance: 1.0e-6  Loss: 0.4264830376329833\n",
      "====> In Gradient: Iteration: 140000 grad_size: 4.860129042291975e-6 tolerance: 1.0e-6  Loss: 0.4151048792394749\n",
      "====> In Gradient: Iteration: 160000 grad_size: 4.359167095814174e-6 tolerance: 1.0e-6  Loss: 0.4088720629112934\n",
      "====> In Gradient: Iteration: 180000 grad_size: 4.195832411635344e-6 tolerance: 1.0e-6  Loss: 0.4062780724809164\n",
      "====> In Gradient: Iteration: 200000 grad_size: 4.11151875521221e-6 tolerance: 1.0e-6  Loss: 0.4045167411103194\n",
      "====> In Gradient: Iteration: 220000 grad_size: 3.2397744972526644e-6 tolerance: 1.0e-6  Loss: 0.4034074074248763\n",
      "====> In Gradient: Iteration: 240000 grad_size: 3.1281281268834036e-6 tolerance: 1.0e-6  Loss: 0.40255492247895963\n",
      "====> In Gradient: Iteration: 260000 grad_size: 3.0417490643707693e-6 tolerance: 1.0e-6  Loss: 0.40185122633724807\n",
      "====> In Gradient: Iteration: 280000 grad_size: 5.1849479024687705e-6 tolerance: 1.0e-6  Loss: 0.4002699185968752\n",
      "====> In Gradient: Iteration: 300000 grad_size: 5.20951384726333e-6 tolerance: 1.0e-6  Loss: 0.3946482898579049\n",
      "====> In Gradient: Iteration: 320000 grad_size: 4.959563963300386e-6 tolerance: 1.0e-6  Loss: 0.39109771269183696\n",
      "====> In Gradient: Iteration: 340000 grad_size: 6.431600513559418e-6 tolerance: 1.0e-6  Loss: 0.3885548593269115\n",
      "====> In Gradient: Iteration: 360000 grad_size: 6.278682466341755e-6 tolerance: 1.0e-6  Loss: 0.38662425674309026\n",
      "====> In Gradient: Iteration: 380000 grad_size: 4.590753320108322e-6 tolerance: 1.0e-6  Loss: 0.385032334511965\n",
      "====> In Gradient: Iteration: 400000 grad_size: 6.359915019647711e-6 tolerance: 1.0e-6  Loss: 0.38356668470297467\n",
      "====> In Gradient: Iteration: 420000 grad_size: 4.619089997581737e-6 tolerance: 1.0e-6  Loss: 0.3821766244758651\n",
      "====> In Gradient: Iteration: 440000 grad_size: 5.7485659339806036e-6 tolerance: 1.0e-6  Loss: 0.38114606744724555\n",
      "====> In Gradient: Iteration: 460000 grad_size: 3.846562023401584e-6 tolerance: 1.0e-6  Loss: 0.3803287596289952\n",
      "====> In Gradient: Iteration: 480000 grad_size: 5.510184942569294e-6 tolerance: 1.0e-6  Loss: 0.3796621847227435\n",
      "====> In Gradient: Iteration: 500000 grad_size: 5.193674436849739e-6 tolerance: 1.0e-6  Loss: 0.3791110522281175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.0897812234073063e-5 tolerance: 1.0e-6  Loss: 0.5151526096086841\n",
      "====> In Gradient: Iteration: 40000 grad_size: 4.267297060245939e-6 tolerance: 1.0e-6  Loss: 0.4792287922875792\n",
      "====> In Gradient: Iteration: 60000 grad_size: 2.4149441408754903e-6 tolerance: 1.0e-6  Loss: 0.4752561008513683\n",
      "====> In Gradient: Iteration: 80000 grad_size: 2.2682291877955916e-6 tolerance: 1.0e-6  Loss: 0.474166861521064\n",
      "====> In Gradient: Iteration: 100000 grad_size: 1.5914684057553353e-5 tolerance: 1.0e-6  Loss: 0.47377086963835957\n",
      "====> In Gradient: Iteration: 120000 grad_size: 2.350446240885122e-6 tolerance: 1.0e-6  Loss: 0.4735139771136514\n",
      "====> In Gradient: Iteration: 140000 grad_size: 1.949390852890803e-6 tolerance: 1.0e-6  Loss: 0.4733352763483806\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.9081807726517405e-6 tolerance: 1.0e-6  Loss: 0.47320846785399207\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.8776635445560118e-6 tolerance: 1.0e-6  Loss: 0.4731156259281227\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.8548261518984069e-6 tolerance: 1.0e-6  Loss: 0.4730457196382694\n",
      "====> In Gradient: Iteration: 220000 grad_size: 3.7970846759671854e-6 tolerance: 1.0e-6  Loss: 0.4729911531798453\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.8223429525231818e-6 tolerance: 1.0e-6  Loss: 0.47294686922376267\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.2779868420321927e-6 tolerance: 1.0e-6  Loss: 0.4729093896902701\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.2801232735383517e-6 tolerance: 1.0e-6  Loss: 0.47287700766737895\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.2829455456217422e-6 tolerance: 1.0e-6  Loss: 0.4728475823764824\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.2091593406967492e-6 tolerance: 1.0e-6  Loss: 0.47282052510827727\n",
      "====> In Gradient: Iteration: 340000 grad_size: 3.863997124365568e-6 tolerance: 1.0e-6  Loss: 0.4727950516689582\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.7602409350795782e-6 tolerance: 1.0e-6  Loss: 0.4727709726094331\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.298853653034973e-6 tolerance: 1.0e-6  Loss: 0.47274734963704107\n",
      "====> In Gradient: Iteration: 400000 grad_size: 1.7420301211451381e-6 tolerance: 1.0e-6  Loss: 0.47272480564316327\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.7328602261006174e-6 tolerance: 1.0e-6  Loss: 0.472702489796775\n",
      "====> In Gradient: Iteration: 440000 grad_size: 2.312554179382969e-6 tolerance: 1.0e-6  Loss: 0.47268082975781833\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.7144233342893841e-6 tolerance: 1.0e-6  Loss: 0.4726592797958048\n",
      "====> In Gradient: Iteration: 480000 grad_size: 1.7053902255523341e-6 tolerance: 1.0e-6  Loss: 0.4726382218173229\n",
      "====> In Gradient: Iteration: 500000 grad_size: 1.6961887767566646e-6 tolerance: 1.0e-6  Loss: 0.47261739278605547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.2187485162100817e-5 tolerance: 1.0e-6  Loss: 0.670278295370058\n",
      "====> In Gradient: Iteration: 40000 grad_size: 4.145808278143848e-6 tolerance: 1.0e-6  Loss: 0.6425158149379706\n",
      "====> In Gradient: Iteration: 60000 grad_size: 2.0614044269206787e-6 tolerance: 1.0e-6  Loss: 0.638302509431972\n",
      "====> In Gradient: Iteration: 80000 grad_size: 1.4757716748672445e-6 tolerance: 1.0e-6  Loss: 0.6367956579448831\n",
      "====> In Gradient: Iteration: 100000 grad_size: 1.1375746673440542e-6 tolerance: 1.0e-6  Loss: 0.6359494725167347\n",
      "====> In Gradient: Iteration: 20000 grad_size: 1.081830714336834e-5 tolerance: 1.0e-6  Loss: 0.5126870359445103\n",
      "====> In Gradient: Iteration: 40000 grad_size: 8.853440654826845e-6 tolerance: 1.0e-6  Loss: 0.49609776100850383\n",
      "====> In Gradient: Iteration: 60000 grad_size: 8.148262905416103e-6 tolerance: 1.0e-6  Loss: 0.4880368469248526\n",
      "====> In Gradient: Iteration: 80000 grad_size: 1.2165135347801078e-5 tolerance: 1.0e-6  Loss: 0.4806208830119145\n",
      "====> In Gradient: Iteration: 100000 grad_size: 1.1452772401135735e-5 tolerance: 1.0e-6  Loss: 0.4727283262332292\n",
      "====> In Gradient: Iteration: 120000 grad_size: 1.0244273315449698e-5 tolerance: 1.0e-6  Loss: 0.46635694062353084\n",
      "====> In Gradient: Iteration: 140000 grad_size: 1.0343408042153811e-5 tolerance: 1.0e-6  Loss: 0.46134226030087305\n",
      "====> In Gradient: Iteration: 160000 grad_size: 4.72258308002949e-6 tolerance: 1.0e-6  Loss: 0.45806011515456635\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.003761946173714e-5 tolerance: 1.0e-6  Loss: 0.4557802468107046\n",
      "====> In Gradient: Iteration: 200000 grad_size: 3.8404099654553685e-6 tolerance: 1.0e-6  Loss: 0.4540975539516877\n",
      "====> In Gradient: Iteration: 220000 grad_size: 3.5411940637942355e-6 tolerance: 1.0e-6  Loss: 0.45277858822271855\n",
      "====> In Gradient: Iteration: 240000 grad_size: 3.3062064228201197e-6 tolerance: 1.0e-6  Loss: 0.4516868349352932\n",
      "====> In Gradient: Iteration: 260000 grad_size: 9.567087832408049e-6 tolerance: 1.0e-6  Loss: 0.45074115942557924\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.9688946725644465e-6 tolerance: 1.0e-6  Loss: 0.4498901144809815\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.8460379164691638e-6 tolerance: 1.0e-6  Loss: 0.4491057338195201\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.743988966018758e-6 tolerance: 1.0e-6  Loss: 0.4483688178226646\n",
      "====> In Gradient: Iteration: 340000 grad_size: 9.389022765804166e-6 tolerance: 1.0e-6  Loss: 0.4476676705509181\n",
      "====> In Gradient: Iteration: 360000 grad_size: 2.5848358501438114e-6 tolerance: 1.0e-6  Loss: 0.44699478169773355\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.521499951493487e-6 tolerance: 1.0e-6  Loss: 0.4463450899012689\n",
      "====> In Gradient: Iteration: 400000 grad_size: 2.4658622266441007e-6 tolerance: 1.0e-6  Loss: 0.4457155191577437\n",
      "====> In Gradient: Iteration: 420000 grad_size: 2.4164377653049403e-6 tolerance: 1.0e-6  Loss: 0.44510353746592785\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.1813272484794697e-5 tolerance: 1.0e-6  Loss: 0.444518764128244\n",
      "====> In Gradient: Iteration: 460000 grad_size: 9.252152563918424e-6 tolerance: 1.0e-6  Loss: 0.44395925149733434\n",
      "====> In Gradient: Iteration: 480000 grad_size: 2.7660477006521134e-6 tolerance: 1.0e-6  Loss: 0.4434216054775486\n",
      "====> In Gradient: Iteration: 500000 grad_size: 2.195247778314109e-6 tolerance: 1.0e-6  Loss: 0.4429042894539594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 5.367558356463121e-6 tolerance: 1.0e-6  Loss: 0.42012480035521416\n",
      "====> In Gradient: Iteration: 40000 grad_size: 4.107587092054387e-6 tolerance: 1.0e-6  Loss: 0.4094491166776419\n",
      "====> In Gradient: Iteration: 60000 grad_size: 3.47319717068645e-6 tolerance: 1.0e-6  Loss: 0.40231400705083326\n",
      "====> In Gradient: Iteration: 80000 grad_size: 3.0071610631479475e-6 tolerance: 1.0e-6  Loss: 0.39709253872332245\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.6553151214720632e-6 tolerance: 1.0e-6  Loss: 0.3931015513296876\n",
      "====> In Gradient: Iteration: 120000 grad_size: 2.3817427950379766e-6 tolerance: 1.0e-6  Loss: 0.3899404733032166\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.1623108236810636e-6 tolerance: 1.0e-6  Loss: 0.38736594227699994\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.981130387917996e-6 tolerance: 1.0e-6  Loss: 0.3852241442169397\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.8278525158534002e-6 tolerance: 1.0e-6  Loss: 0.3834134417598582\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.6957262843263455e-6 tolerance: 1.0e-6  Loss: 0.3818635010698947\n",
      "====> In Gradient: Iteration: 220000 grad_size: 6.070383655279133e-6 tolerance: 1.0e-6  Loss: 0.36715154947580037\n",
      "====> In Gradient: Iteration: 240000 grad_size: 5.119972906457023e-6 tolerance: 1.0e-6  Loss: 0.3517430773489277\n",
      "====> In Gradient: Iteration: 260000 grad_size: 4.5237869242439576e-6 tolerance: 1.0e-6  Loss: 0.34018436055131185\n",
      "====> In Gradient: Iteration: 280000 grad_size: 5.515574846009984e-6 tolerance: 1.0e-6  Loss: 0.3204349208663589\n",
      "====> In Gradient: Iteration: 300000 grad_size: 4.566795951251638e-6 tolerance: 1.0e-6  Loss: 0.30788353932157925\n",
      "====> In Gradient: Iteration: 320000 grad_size: 3.934178534484304e-6 tolerance: 1.0e-6  Loss: 0.29890846640634283\n",
      "====> In Gradient: Iteration: 340000 grad_size: 3.4663975449379344e-6 tolerance: 1.0e-6  Loss: 0.2920929336472556\n",
      "====> In Gradient: Iteration: 360000 grad_size: 3.1067519155374803e-6 tolerance: 1.0e-6  Loss: 0.286711653280191\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.8257827459285756e-6 tolerance: 1.0e-6  Loss: 0.28232574788551124\n",
      "====> In Gradient: Iteration: 400000 grad_size: 2.6038748522177087e-6 tolerance: 1.0e-6  Loss: 0.2786501164974444\n",
      "====> In Gradient: Iteration: 420000 grad_size: 2.42650354347871e-6 tolerance: 1.0e-6  Loss: 0.2754937936171323\n",
      "====> In Gradient: Iteration: 440000 grad_size: 2.282644502465535e-6 tolerance: 1.0e-6  Loss: 0.2727266058463915\n",
      "====> In Gradient: Iteration: 460000 grad_size: 2.21136407534265e-6 tolerance: 1.0e-6  Loss: 0.270329468995431\n",
      "====> In Gradient: Iteration: 480000 grad_size: 2.291890922715116e-6 tolerance: 1.0e-6  Loss: 0.2682711386821501\n",
      "====> In Gradient: Iteration: 500000 grad_size: 2.3120604798670735e-6 tolerance: 1.0e-6  Loss: 0.2664399815482667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 6.446296403775231e-6 tolerance: 1.0e-6  Loss: 0.4948256858320469\n",
      "====> In Gradient: Iteration: 20000 grad_size: 2.16275900201126e-5 tolerance: 1.0e-6  Loss: 0.7053913247050784\n",
      "====> In Gradient: Iteration: 40000 grad_size: 1.5399905050341745e-5 tolerance: 1.0e-6  Loss: 0.46355635954735613\n",
      "====> In Gradient: Iteration: 60000 grad_size: 9.870630038316229e-6 tolerance: 1.0e-6  Loss: 0.3927259436263656\n",
      "====> In Gradient: Iteration: 80000 grad_size: 5.7957538507849e-6 tolerance: 1.0e-6  Loss: 0.3701675248858373\n",
      "====> In Gradient: Iteration: 100000 grad_size: 4.417516828612417e-6 tolerance: 1.0e-6  Loss: 0.3570296263438191\n",
      "====> In Gradient: Iteration: 120000 grad_size: 3.831537658003643e-6 tolerance: 1.0e-6  Loss: 0.34917112572120995\n",
      "====> In Gradient: Iteration: 140000 grad_size: 3.3748704946670596e-6 tolerance: 1.0e-6  Loss: 0.3431428023152054\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.000767618983893e-6 tolerance: 1.0e-6  Loss: 0.3384265454645602\n",
      "====> In Gradient: Iteration: 180000 grad_size: 2.6849582554009227e-6 tolerance: 1.0e-6  Loss: 0.33469833440136054\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.4137004407978108e-6 tolerance: 1.0e-6  Loss: 0.33173263299985367\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.178461912301015e-6 tolerance: 1.0e-6  Loss: 0.32936387839514714\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.8806946785631435e-6 tolerance: 1.0e-6  Loss: 0.3277593667932725\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.6368079870574532e-6 tolerance: 1.0e-6  Loss: 0.32721187305664917\n",
      "====> In Gradient: Iteration: 280000 grad_size: 1.4815826315315245e-6 tolerance: 1.0e-6  Loss: 0.32689012732560835\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.9466179266950707e-6 tolerance: 1.0e-6  Loss: 0.32669916770053364\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.3105382411671426e-6 tolerance: 1.0e-6  Loss: 0.32658475704936346\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.2652419709576774e-6 tolerance: 1.0e-6  Loss: 0.32651520806117734\n",
      "====> In Gradient: Iteration: 360000 grad_size: 2.020226111229019e-6 tolerance: 1.0e-6  Loss: 0.3264721586184711\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.0392107401150786e-6 tolerance: 1.0e-6  Loss: 0.32644477014396184\n",
      "====> In Gradient: Iteration: 400000 grad_size: 1.0865595309228711e-5 tolerance: 1.0e-6  Loss: 0.326427079712655\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.1917797300006267e-6 tolerance: 1.0e-6  Loss: 0.32641470727489985\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.1853414771226981e-6 tolerance: 1.0e-6  Loss: 0.32640603229435466\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.180796277566627e-6 tolerance: 1.0e-6  Loss: 0.32639946437135503\n",
      "====> In Gradient: Iteration: 480000 grad_size: 2.0870163134862916e-6 tolerance: 1.0e-6  Loss: 0.3263944694338131\n",
      "====> In Gradient: Iteration: 500000 grad_size: 1.1755792940892932e-6 tolerance: 1.0e-6  Loss: 0.326390374030896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.2516497130050589e-5 tolerance: 1.0e-6  Loss: 0.5747520804848605\n",
      "====> In Gradient: Iteration: 40000 grad_size: 8.088584879534434e-6 tolerance: 1.0e-6  Loss: 0.5247316595772068\n",
      "====> In Gradient: Iteration: 60000 grad_size: 5.894663656438862e-6 tolerance: 1.0e-6  Loss: 0.5009089518485522\n",
      "====> In Gradient: Iteration: 80000 grad_size: 4.586915236710305e-6 tolerance: 1.0e-6  Loss: 0.48737279607435396\n",
      "====> In Gradient: Iteration: 100000 grad_size: 3.8145322518720177e-6 tolerance: 1.0e-6  Loss: 0.47918309375414675\n",
      "====> In Gradient: Iteration: 120000 grad_size: 3.5614426661222856e-6 tolerance: 1.0e-6  Loss: 0.47439642687298267\n",
      "====> In Gradient: Iteration: 140000 grad_size: 3.607922580909155e-6 tolerance: 1.0e-6  Loss: 0.4715861568838593\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.977281007685879e-6 tolerance: 1.0e-6  Loss: 0.4698795365202885\n",
      "====> In Gradient: Iteration: 180000 grad_size: 2.872197130310228e-6 tolerance: 1.0e-6  Loss: 0.46856825391208634\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.6754799832589464e-6 tolerance: 1.0e-6  Loss: 0.46748255011731565\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.535912985783422e-6 tolerance: 1.0e-6  Loss: 0.466540237452999\n",
      "====> In Gradient: Iteration: 240000 grad_size: 4.312153086668484e-6 tolerance: 1.0e-6  Loss: 0.46569705583439414\n",
      "====> In Gradient: Iteration: 260000 grad_size: 4.342411820835966e-6 tolerance: 1.0e-6  Loss: 0.4649263219940767\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.292324683977699e-6 tolerance: 1.0e-6  Loss: 0.4642115630505616\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.2420006376526228e-6 tolerance: 1.0e-6  Loss: 0.46354177087106047\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.2002863633319926e-6 tolerance: 1.0e-6  Loss: 0.4629094563938877\n",
      "====> In Gradient: Iteration: 340000 grad_size: 4.387532799539102e-6 tolerance: 1.0e-6  Loss: 0.46230889627415583\n",
      "====> In Gradient: Iteration: 360000 grad_size: 2.1346242665781106e-6 tolerance: 1.0e-6  Loss: 0.4617361046561731\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.1080507371129795e-6 tolerance: 1.0e-6  Loss: 0.4611881574781468\n",
      "====> In Gradient: Iteration: 400000 grad_size: 2.0845562682608507e-6 tolerance: 1.0e-6  Loss: 0.4606623978272708\n",
      "====> In Gradient: Iteration: 420000 grad_size: 2.0634822649623724e-6 tolerance: 1.0e-6  Loss: 0.46015715326605255\n",
      "====> In Gradient: Iteration: 440000 grad_size: 2.0443755553815454e-6 tolerance: 1.0e-6  Loss: 0.45967079530315613\n",
      "====> In Gradient: Iteration: 460000 grad_size: 4.380228690203191e-6 tolerance: 1.0e-6  Loss: 0.4592017696770696\n",
      "====> In Gradient: Iteration: 480000 grad_size: 1.972933054921233e-6 tolerance: 1.0e-6  Loss: 0.4587718552612975\n",
      "====> In Gradient: Iteration: 500000 grad_size: 4.453837621611621e-6 tolerance: 1.0e-6  Loss: 0.4583744123954398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.6047014940163345e-5 tolerance: 1.0e-6  Loss: 0.3701446121915954\n",
      "====> In Gradient: Iteration: 40000 grad_size: 9.872146300246895e-6 tolerance: 1.0e-6  Loss: 0.290492214917594\n",
      "====> In Gradient: Iteration: 60000 grad_size: 6.872346172407612e-6 tolerance: 1.0e-6  Loss: 0.25596868226091163\n",
      "====> In Gradient: Iteration: 80000 grad_size: 6.49054488032565e-6 tolerance: 1.0e-6  Loss: 0.23751087945632188\n",
      "====> In Gradient: Iteration: 100000 grad_size: 6.077887643024499e-6 tolerance: 1.0e-6  Loss: 0.22339899534837024\n",
      "====> In Gradient: Iteration: 120000 grad_size: 3.850066039397345e-6 tolerance: 1.0e-6  Loss: 0.21139470761131512\n",
      "====> In Gradient: Iteration: 140000 grad_size: 3.2548276060625243e-6 tolerance: 1.0e-6  Loss: 0.20642581538651703\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.868127597993702e-6 tolerance: 1.0e-6  Loss: 0.20411585877863764\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.7971192445611875e-6 tolerance: 1.0e-6  Loss: 0.2017244548065736\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.3662394638722316e-6 tolerance: 1.0e-6  Loss: 0.20062720058647662\n",
      "====> In Gradient: Iteration: 220000 grad_size: 1.2626189190591258e-6 tolerance: 1.0e-6  Loss: 0.19988966469977987\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.2114129662530547e-6 tolerance: 1.0e-6  Loss: 0.19926093695810554\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.9446605697982076e-6 tolerance: 1.0e-6  Loss: 0.19883406229556638\n",
      "====> In Gradient: Iteration: 20000 grad_size: 7.439624291241058e-6 tolerance: 1.0e-6  Loss: 0.42511046278140374\n",
      "====> In Gradient: Iteration: 40000 grad_size: 3.7821643416649087e-6 tolerance: 1.0e-6  Loss: 0.4116355002879254\n",
      "====> In Gradient: Iteration: 60000 grad_size: 3.283593546034374e-6 tolerance: 1.0e-6  Loss: 0.40743080651570823\n",
      "====> In Gradient: Iteration: 80000 grad_size: 4.143222146882655e-6 tolerance: 1.0e-6  Loss: 0.40492783744421207\n",
      "====> In Gradient: Iteration: 100000 grad_size: 3.958287329717422e-6 tolerance: 1.0e-6  Loss: 0.40314612875264144\n",
      "====> In Gradient: Iteration: 120000 grad_size: 4.888709765041456e-6 tolerance: 1.0e-6  Loss: 0.39940487848453643\n",
      "====> In Gradient: Iteration: 140000 grad_size: 4.203725280417756e-6 tolerance: 1.0e-6  Loss: 0.397820495262799\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.7193845330015424e-6 tolerance: 1.0e-6  Loss: 0.3967471815184476\n",
      "====> In Gradient: Iteration: 180000 grad_size: 3.826824832153015e-6 tolerance: 1.0e-6  Loss: 0.3960046786210928\n",
      "====> In Gradient: Iteration: 200000 grad_size: 5.00802329272394e-6 tolerance: 1.0e-6  Loss: 0.3954776849589383\n",
      "====> In Gradient: Iteration: 220000 grad_size: 3.5542477191738817e-6 tolerance: 1.0e-6  Loss: 0.3934288172135291\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.8261474536309568e-6 tolerance: 1.0e-6  Loss: 0.3923108294128801\n",
      "====> In Gradient: Iteration: 260000 grad_size: 3.1989364890072914e-6 tolerance: 1.0e-6  Loss: 0.39174068327115824\n",
      "====> In Gradient: Iteration: 280000 grad_size: 3.199179821134527e-6 tolerance: 1.0e-6  Loss: 0.3913353735053089\n",
      "====> In Gradient: Iteration: 300000 grad_size: 3.21085832176074e-6 tolerance: 1.0e-6  Loss: 0.391016131132335\n",
      "====> In Gradient: Iteration: 320000 grad_size: 3.226696377835564e-6 tolerance: 1.0e-6  Loss: 0.39075482786632076\n",
      "====> In Gradient: Iteration: 340000 grad_size: 3.2446247340260915e-6 tolerance: 1.0e-6  Loss: 0.39053506486561873\n",
      "====> In Gradient: Iteration: 360000 grad_size: 3.263660099596911e-6 tolerance: 1.0e-6  Loss: 0.3903465857393427\n",
      "====> In Gradient: Iteration: 380000 grad_size: 3.2831490794393647e-6 tolerance: 1.0e-6  Loss: 0.39018256704858995\n",
      "====> In Gradient: Iteration: 400000 grad_size: 2.0399118968220583e-6 tolerance: 1.0e-6  Loss: 0.39003743142242653\n",
      "====> In Gradient: Iteration: 420000 grad_size: 3.323019343406588e-6 tolerance: 1.0e-6  Loss: 0.38990807918477793\n",
      "====> In Gradient: Iteration: 440000 grad_size: 3.266108375069885e-6 tolerance: 1.0e-6  Loss: 0.38979185587041315\n",
      "====> In Gradient: Iteration: 460000 grad_size: 5.189747703369983e-6 tolerance: 1.0e-6  Loss: 0.38968666532723084\n",
      "====> In Gradient: Iteration: 480000 grad_size: 3.3833171933733593e-6 tolerance: 1.0e-6  Loss: 0.3895910645293608\n",
      "====> In Gradient: Iteration: 500000 grad_size: 5.2406670797218e-6 tolerance: 1.0e-6  Loss: 0.389503771605816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.9276403493615994e-5 tolerance: 1.0e-6  Loss: 0.8179830102062194\n",
      "====> In Gradient: Iteration: 40000 grad_size: 1.2731188986822476e-5 tolerance: 1.0e-6  Loss: 0.6944290990608798\n",
      "====> In Gradient: Iteration: 60000 grad_size: 7.31876903348765e-6 tolerance: 1.0e-6  Loss: 0.649374150462933\n",
      "====> In Gradient: Iteration: 80000 grad_size: 2.2636817935853183e-6 tolerance: 1.0e-6  Loss: 0.6414708215424669\n",
      "====> In Gradient: Iteration: 100000 grad_size: 6.2819832436794505e-6 tolerance: 1.0e-6  Loss: 0.6392114339120174\n",
      "====> In Gradient: Iteration: 120000 grad_size: 1.8996916638584507e-6 tolerance: 1.0e-6  Loss: 0.6373168531997085\n",
      "====> In Gradient: Iteration: 140000 grad_size: 1.7967364606531715e-6 tolerance: 1.0e-6  Loss: 0.6356466504192937\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.7041444807184359e-6 tolerance: 1.0e-6  Loss: 0.6341527133139252\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.6172797393932548e-6 tolerance: 1.0e-6  Loss: 0.6328111305527987\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.5349684229151303e-6 tolerance: 1.0e-6  Loss: 0.6316049176708819\n",
      "====> In Gradient: Iteration: 220000 grad_size: 1.4568458777474965e-6 tolerance: 1.0e-6  Loss: 0.630520048271905\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.3827146673539559e-6 tolerance: 1.0e-6  Loss: 0.6295442262426842\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.3123836649433401e-6 tolerance: 1.0e-6  Loss: 0.628666336862644\n",
      "====> In Gradient: Iteration: 280000 grad_size: 1.2456779867355147e-6 tolerance: 1.0e-6  Loss: 0.6278765252554845\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.1824224695800808e-6 tolerance: 1.0e-6  Loss: 0.6271659253816567\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.1224400439523798e-6 tolerance: 1.0e-6  Loss: 0.6265265305520837\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.065559246536353e-6 tolerance: 1.0e-6  Loss: 0.6259511431127521\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.0116211076385057e-6 tolerance: 1.0e-6  Loss: 0.6254333247973516\n",
      "====> In Gradient: Iteration: 20000 grad_size: 7.455005743341899e-6 tolerance: 1.0e-6  Loss: 0.6777265772566545\n",
      "====> In Gradient: Iteration: 40000 grad_size: 3.96298963993305e-6 tolerance: 1.0e-6  Loss: 0.6656704612406218\n",
      "====> In Gradient: Iteration: 60000 grad_size: 2.9542636008384228e-6 tolerance: 1.0e-6  Loss: 0.660081284770254\n",
      "====> In Gradient: Iteration: 80000 grad_size: 2.4214404129039496e-6 tolerance: 1.0e-6  Loss: 0.6565927315887901\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.344985548096027e-6 tolerance: 1.0e-6  Loss: 0.6543376935985451\n",
      "====> In Gradient: Iteration: 120000 grad_size: 1.9557027595212185e-6 tolerance: 1.0e-6  Loss: 0.6528003658966919\n",
      "====> In Gradient: Iteration: 140000 grad_size: 1.6395399455961864e-6 tolerance: 1.0e-6  Loss: 0.6517486750850457\n",
      "====> In Gradient: Iteration: 160000 grad_size: 7.390090379643819e-6 tolerance: 1.0e-6  Loss: 0.6511203483835087\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.2487588818033434e-6 tolerance: 1.0e-6  Loss: 0.6507836516074867\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.7301641262432384e-6 tolerance: 1.0e-6  Loss: 0.6505681623717047\n",
      "====> In Gradient: Iteration: 220000 grad_size: 7.459598189879652e-6 tolerance: 1.0e-6  Loss: 0.6504252214040449\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.1450497241377763e-6 tolerance: 1.0e-6  Loss: 0.6503272610269508\n",
      "====> In Gradient: Iteration: 260000 grad_size: 5.57246663369497e-6 tolerance: 1.0e-6  Loss: 0.6502586773231698\n",
      "====> In Gradient: Iteration: 280000 grad_size: 1.1728488623683653e-6 tolerance: 1.0e-6  Loss: 0.6502081934964626\n",
      "====> In Gradient: Iteration: 300000 grad_size: 7.760696089052618e-6 tolerance: 1.0e-6  Loss: 0.6501707298171402\n",
      "====> In Gradient: Iteration: 20000 grad_size: 2.07968348200028e-5 tolerance: 1.0e-6  Loss: 0.5033106450504545\n",
      "====> In Gradient: Iteration: 40000 grad_size: 1.4758001900829863e-6 tolerance: 1.0e-6  Loss: 0.393142530439792\n",
      "====> In Gradient: Iteration: 60000 grad_size: 1.3996647919126555e-6 tolerance: 1.0e-6  Loss: 0.3928435679260124\n",
      "====> In Gradient: Iteration: 80000 grad_size: 2.2836300970130975e-6 tolerance: 1.0e-6  Loss: 0.3925889014184659\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.2342870972105614e-6 tolerance: 1.0e-6  Loss: 0.3923633934834931\n",
      "====> In Gradient: Iteration: 120000 grad_size: 1.265351128376377e-6 tolerance: 1.0e-6  Loss: 0.39215815916301655\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.1559924508249035e-6 tolerance: 1.0e-6  Loss: 0.39196775832555275\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.211385852682724e-6 tolerance: 1.0e-6  Loss: 0.39178935593908554\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.5358140823461366e-5 tolerance: 1.0e-6  Loss: 0.3916208920761268\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.169103737453292e-6 tolerance: 1.0e-6  Loss: 0.39146098653217287\n",
      "====> In Gradient: Iteration: 220000 grad_size: 1.1505606327825409e-6 tolerance: 1.0e-6  Loss: 0.3913089116466937\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.1332322608209866e-6 tolerance: 1.0e-6  Loss: 0.3911637758385151\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.1168874752860257e-6 tolerance: 1.0e-6  Loss: 0.3910252815672396\n",
      "====> In Gradient: Iteration: 280000 grad_size: 1.980468453407802e-6 tolerance: 1.0e-6  Loss: 0.39089280900359674\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.0864673307953616e-6 tolerance: 1.0e-6  Loss: 0.3907658362726788\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.0721825830647744e-6 tolerance: 1.0e-6  Loss: 0.390644345509539\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.0583834594945725e-6 tolerance: 1.0e-6  Loss: 0.39052763442547633\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.045036367860441e-6 tolerance: 1.0e-6  Loss: 0.3904156472255447\n",
      "====> In Gradient: Iteration: 20000 grad_size: 1.621994156607829e-5 tolerance: 1.0e-6  Loss: 0.7077338594060459\n",
      "====> In Gradient: Iteration: 40000 grad_size: 9.852746356863104e-6 tolerance: 1.0e-6  Loss: 0.6165835335797304\n",
      "====> In Gradient: Iteration: 60000 grad_size: 8.108378339256695e-6 tolerance: 1.0e-6  Loss: 0.5892844741521114\n",
      "====> In Gradient: Iteration: 80000 grad_size: 3.076332694699225e-6 tolerance: 1.0e-6  Loss: 0.5870695755971654\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.9001413385454797e-6 tolerance: 1.0e-6  Loss: 0.5851988460980744\n",
      "====> In Gradient: Iteration: 120000 grad_size: 7.68383625962813e-6 tolerance: 1.0e-6  Loss: 0.5834621648882317\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.690297851548033e-6 tolerance: 1.0e-6  Loss: 0.5818167097556254\n",
      "====> In Gradient: Iteration: 160000 grad_size: 2.6059642296418035e-6 tolerance: 1.0e-6  Loss: 0.5802500138267179\n",
      "====> In Gradient: Iteration: 180000 grad_size: 2.5410862126575234e-6 tolerance: 1.0e-6  Loss: 0.5787076225799977\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.4774370804848434e-6 tolerance: 1.0e-6  Loss: 0.577290769364527\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.4199838695533733e-6 tolerance: 1.0e-6  Loss: 0.5759913763698626\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.3650782479946128e-6 tolerance: 1.0e-6  Loss: 0.5747834944621721\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.314334910498022e-6 tolerance: 1.0e-6  Loss: 0.5736571745303507\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.2686045593280456e-6 tolerance: 1.0e-6  Loss: 0.5726046820257449\n",
      "====> In Gradient: Iteration: 300000 grad_size: 7.45931837518949e-6 tolerance: 1.0e-6  Loss: 0.5716198695226727\n",
      "====> In Gradient: Iteration: 320000 grad_size: 3.058169429268682e-6 tolerance: 1.0e-6  Loss: 0.5706969095873943\n",
      "====> In Gradient: Iteration: 340000 grad_size: 2.163355288600072e-6 tolerance: 1.0e-6  Loss: 0.5698309469303376\n",
      "====> In Gradient: Iteration: 360000 grad_size: 9.161030689409805e-6 tolerance: 1.0e-6  Loss: 0.5690174318568864\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.117826639293088e-6 tolerance: 1.0e-6  Loss: 0.5682517468492541\n",
      "====> In Gradient: Iteration: 400000 grad_size: 9.127089544524965e-6 tolerance: 1.0e-6  Loss: 0.5675312037736326\n",
      "====> In Gradient: Iteration: 420000 grad_size: 2.089104555900027e-6 tolerance: 1.0e-6  Loss: 0.5668507475554364\n",
      "====> In Gradient: Iteration: 440000 grad_size: 2.0801820974235853e-6 tolerance: 1.0e-6  Loss: 0.5662085182638537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 460000 grad_size: 2.074512154936995e-6 tolerance: 1.0e-6  Loss: 0.5656014281983262\n",
      "====> In Gradient: Iteration: 480000 grad_size: 2.0718410641866336e-6 tolerance: 1.0e-6  Loss: 0.5650268000140409\n",
      "====> In Gradient: Iteration: 500000 grad_size: 2.567946309989966e-6 tolerance: 1.0e-6  Loss: 0.5644822932097309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.6680585394092385e-5 tolerance: 1.0e-6  Loss: 0.5821771913109572\n",
      "====> In Gradient: Iteration: 40000 grad_size: 7.6063994947901375e-6 tolerance: 1.0e-6  Loss: 0.5328979054027699\n",
      "====> In Gradient: Iteration: 60000 grad_size: 6.4839188011421175e-6 tolerance: 1.0e-6  Loss: 0.5090535469589251\n",
      "====> In Gradient: Iteration: 80000 grad_size: 6.56461714298066e-6 tolerance: 1.0e-6  Loss: 0.5006218079260658\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.9979746459867487e-6 tolerance: 1.0e-6  Loss: 0.4980893700885359\n",
      "====> In Gradient: Iteration: 120000 grad_size: 5.3825500755105595e-6 tolerance: 1.0e-6  Loss: 0.4961957884567836\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.812719232802714e-6 tolerance: 1.0e-6  Loss: 0.49455932365986616\n",
      "====> In Gradient: Iteration: 160000 grad_size: 5.323454085631787e-6 tolerance: 1.0e-6  Loss: 0.4931090875263358\n",
      "====> In Gradient: Iteration: 180000 grad_size: 5.295931904981865e-6 tolerance: 1.0e-6  Loss: 0.49180147106847616\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.8115824384518857e-6 tolerance: 1.0e-6  Loss: 0.4906065306533017\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.832420542361874e-6 tolerance: 1.0e-6  Loss: 0.48950105399979355\n",
      "====> In Gradient: Iteration: 240000 grad_size: 5.8820277341746275e-6 tolerance: 1.0e-6  Loss: 0.48846859894101013\n",
      "====> In Gradient: Iteration: 260000 grad_size: 5.844534195519333e-6 tolerance: 1.0e-6  Loss: 0.48749477505974526\n",
      "====> In Gradient: Iteration: 280000 grad_size: 5.191060718574346e-6 tolerance: 1.0e-6  Loss: 0.48656963488523575\n",
      "====> In Gradient: Iteration: 300000 grad_size: 4.955719147741201e-6 tolerance: 1.0e-6  Loss: 0.48601059335454266\n",
      "====> In Gradient: Iteration: 320000 grad_size: 4.962069469953361e-6 tolerance: 1.0e-6  Loss: 0.48556604241691376\n",
      "====> In Gradient: Iteration: 340000 grad_size: 4.849019290662944e-6 tolerance: 1.0e-6  Loss: 0.48516394366038446\n",
      "====> In Gradient: Iteration: 360000 grad_size: 4.925432283215531e-6 tolerance: 1.0e-6  Loss: 0.48479408136471813\n",
      "====> In Gradient: Iteration: 380000 grad_size: 3.23272753093535e-6 tolerance: 1.0e-6  Loss: 0.48445031228883617\n",
      "====> In Gradient: Iteration: 400000 grad_size: 4.896781257569599e-6 tolerance: 1.0e-6  Loss: 0.4841279720461201\n",
      "====> In Gradient: Iteration: 420000 grad_size: 4.883959803661372e-6 tolerance: 1.0e-6  Loss: 0.4838233273560688\n",
      "====> In Gradient: Iteration: 440000 grad_size: 3.3478808988445305e-6 tolerance: 1.0e-6  Loss: 0.4835334286860411\n",
      "====> In Gradient: Iteration: 460000 grad_size: 3.3807380384649764e-6 tolerance: 1.0e-6  Loss: 0.4832561792378458\n",
      "====> In Gradient: Iteration: 480000 grad_size: 4.849168780198979e-6 tolerance: 1.0e-6  Loss: 0.4829901718660487\n",
      "====> In Gradient: Iteration: 500000 grad_size: 3.438166502057213e-6 tolerance: 1.0e-6  Loss: 0.48273311100867555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.2165022061646788e-5 tolerance: 1.0e-6  Loss: 0.5922826729770433\n",
      "====> In Gradient: Iteration: 40000 grad_size: 5.806945910833114e-6 tolerance: 1.0e-6  Loss: 0.5682085632433377\n",
      "====> In Gradient: Iteration: 60000 grad_size: 3.693478493244514e-6 tolerance: 1.0e-6  Loss: 0.5570632219806517\n",
      "====> In Gradient: Iteration: 80000 grad_size: 3.1864673629079773e-6 tolerance: 1.0e-6  Loss: 0.5514569786984347\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.801209274958535e-6 tolerance: 1.0e-6  Loss: 0.5471452318617487\n",
      "====> In Gradient: Iteration: 120000 grad_size: 2.500797133153383e-6 tolerance: 1.0e-6  Loss: 0.5437291807349688\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.2756510401208093e-6 tolerance: 1.0e-6  Loss: 0.5411010472128432\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.5024631716064624e-6 tolerance: 1.0e-6  Loss: 0.5398250995301356\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.2839899480184978e-6 tolerance: 1.0e-6  Loss: 0.5388845615841016\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.120847080708603e-6 tolerance: 1.0e-6  Loss: 0.5381684651513602\n",
      "====> In Gradient: Iteration: 20000 grad_size: 1.7911729242145008e-5 tolerance: 1.0e-6  Loss: 0.4997577994115551\n",
      "====> In Gradient: Iteration: 40000 grad_size: 9.82390898819722e-6 tolerance: 1.0e-6  Loss: 0.40729770939016674\n",
      "====> In Gradient: Iteration: 60000 grad_size: 1.2555598544285677e-5 tolerance: 1.0e-6  Loss: 0.3392831823859511\n",
      "====> In Gradient: Iteration: 80000 grad_size: 8.631433315082404e-6 tolerance: 1.0e-6  Loss: 0.285302617699276\n",
      "====> In Gradient: Iteration: 100000 grad_size: 6.613988577755817e-6 tolerance: 1.0e-6  Loss: 0.25673164910998153\n",
      "====> In Gradient: Iteration: 120000 grad_size: 4.9211023009981475e-6 tolerance: 1.0e-6  Loss: 0.2402919527893324\n",
      "====> In Gradient: Iteration: 140000 grad_size: 4.013729212031574e-6 tolerance: 1.0e-6  Loss: 0.23043353444116207\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.379600268530194e-6 tolerance: 1.0e-6  Loss: 0.2236364567654242\n",
      "====> In Gradient: Iteration: 180000 grad_size: 2.8883471066894903e-6 tolerance: 1.0e-6  Loss: 0.21874659424848691\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.498615577038675e-6 tolerance: 1.0e-6  Loss: 0.2151321600556268\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.1833063036779283e-6 tolerance: 1.0e-6  Loss: 0.2124004805949305\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.927200889033163e-6 tolerance: 1.0e-6  Loss: 0.21030854275054794\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.7210284145342193e-6 tolerance: 1.0e-6  Loss: 0.20869031993774595\n",
      "====> In Gradient: Iteration: 280000 grad_size: 1.5520309843942173e-6 tolerance: 1.0e-6  Loss: 0.20741769348314987\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.4139064012218713e-6 tolerance: 1.0e-6  Loss: 0.20640701205056572\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.5650930439059832e-6 tolerance: 1.0e-6  Loss: 0.20554621087699435\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.3964538039653481e-6 tolerance: 1.0e-6  Loss: 0.20471918008346845\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.2896635743373767e-6 tolerance: 1.0e-6  Loss: 0.20406361344743276\n",
      "====> In Gradient: Iteration: 380000 grad_size: 1.2871573093945507e-6 tolerance: 1.0e-6  Loss: 0.20353340901161696\n",
      "====> In Gradient: Iteration: 400000 grad_size: 1.1810341869694382e-6 tolerance: 1.0e-6  Loss: 0.20310092886509262\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.0846804630568564e-6 tolerance: 1.0e-6  Loss: 0.20274569297072212\n",
      "====> In Gradient: Iteration: 20000 grad_size: 1.2509483660783209e-5 tolerance: 1.0e-6  Loss: 0.5382602908526053\n",
      "====> In Gradient: Iteration: 40000 grad_size: 7.850654852978416e-6 tolerance: 1.0e-6  Loss: 0.49314891799922\n",
      "====> In Gradient: Iteration: 60000 grad_size: 7.581260398194535e-6 tolerance: 1.0e-6  Loss: 0.45343407757162396\n",
      "====> In Gradient: Iteration: 80000 grad_size: 6.851621982900108e-6 tolerance: 1.0e-6  Loss: 0.4276118554078997\n",
      "====> In Gradient: Iteration: 100000 grad_size: 6.354358650060725e-6 tolerance: 1.0e-6  Loss: 0.4058688733263008\n",
      "====> In Gradient: Iteration: 120000 grad_size: 5.423505394838367e-6 tolerance: 1.0e-6  Loss: 0.38475927177148833\n",
      "====> In Gradient: Iteration: 140000 grad_size: 6.790000693770658e-6 tolerance: 1.0e-6  Loss: 0.37097768246162605\n",
      "====> In Gradient: Iteration: 160000 grad_size: 4.701202565298823e-6 tolerance: 1.0e-6  Loss: 0.3582869429632345\n",
      "====> In Gradient: Iteration: 180000 grad_size: 3.7897651658037918e-6 tolerance: 1.0e-6  Loss: 0.3519964584700177\n",
      "====> In Gradient: Iteration: 200000 grad_size: 3.311006653690777e-6 tolerance: 1.0e-6  Loss: 0.34729514515181165\n",
      "====> In Gradient: Iteration: 220000 grad_size: 3.037602323932046e-6 tolerance: 1.0e-6  Loss: 0.3433439272204632\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.8628961626051957e-6 tolerance: 1.0e-6  Loss: 0.33982924328268915\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.738458788867861e-6 tolerance: 1.0e-6  Loss: 0.3366205799216315\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.6417069196679548e-6 tolerance: 1.0e-6  Loss: 0.33365481895510957\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.5618751064556174e-6 tolerance: 1.0e-6  Loss: 0.330896623517041\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.493361757413978e-6 tolerance: 1.0e-6  Loss: 0.328322775117394\n",
      "====> In Gradient: Iteration: 340000 grad_size: 2.4330223769274732e-6 tolerance: 1.0e-6  Loss: 0.3259162399438239\n",
      "====> In Gradient: Iteration: 360000 grad_size: 5.852516441986886e-6 tolerance: 1.0e-6  Loss: 0.32366313622228526\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.3302704171950074e-6 tolerance: 1.0e-6  Loss: 0.32155147895576075\n",
      "====> In Gradient: Iteration: 400000 grad_size: 2.285958111495476e-6 tolerance: 1.0e-6  Loss: 0.31957132962023466\n",
      "====> In Gradient: Iteration: 420000 grad_size: 2.245534032548619e-6 tolerance: 1.0e-6  Loss: 0.31771334031443427\n",
      "====> In Gradient: Iteration: 440000 grad_size: 2.2086079727312504e-6 tolerance: 1.0e-6  Loss: 0.3159693151290523\n",
      "====> In Gradient: Iteration: 460000 grad_size: 5.629574016361214e-6 tolerance: 1.0e-6  Loss: 0.31433169590529136\n",
      "====> In Gradient: Iteration: 480000 grad_size: 2.1440478328504236e-6 tolerance: 1.0e-6  Loss: 0.312793265179594\n",
      "====> In Gradient: Iteration: 500000 grad_size: 2.406155721813026e-6 tolerance: 1.0e-6  Loss: 0.3108740298707355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 4.843237262597314e-6 tolerance: 1.0e-6  Loss: 0.5422765940585509\n",
      "====> In Gradient: Iteration: 40000 grad_size: 7.339796536007926e-6 tolerance: 1.0e-6  Loss: 0.527487243183098\n",
      "====> In Gradient: Iteration: 60000 grad_size: 1.0541410111741213e-5 tolerance: 1.0e-6  Loss: 0.5105103024200196\n",
      "====> In Gradient: Iteration: 80000 grad_size: 4.840686332046105e-6 tolerance: 1.0e-6  Loss: 0.49881783940483315\n",
      "====> In Gradient: Iteration: 100000 grad_size: 4.852184764576671e-6 tolerance: 1.0e-6  Loss: 0.4906439198751917\n",
      "====> In Gradient: Iteration: 120000 grad_size: 3.9255961393228125e-6 tolerance: 1.0e-6  Loss: 0.485071365841206\n",
      "====> In Gradient: Iteration: 140000 grad_size: 3.678386362237878e-6 tolerance: 1.0e-6  Loss: 0.4806121338905205\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.8018740938994295e-6 tolerance: 1.0e-6  Loss: 0.4782018273980535\n",
      "====> In Gradient: Iteration: 180000 grad_size: 3.168189439568603e-6 tolerance: 1.0e-6  Loss: 0.4765331336014737\n",
      "====> In Gradient: Iteration: 200000 grad_size: 3.0770532898965746e-6 tolerance: 1.0e-6  Loss: 0.4751088503336772\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.997651025602601e-6 tolerance: 1.0e-6  Loss: 0.4738599089704254\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.9294860057698865e-6 tolerance: 1.0e-6  Loss: 0.47274313781206445\n",
      "====> In Gradient: Iteration: 260000 grad_size: 3.4850482850417877e-6 tolerance: 1.0e-6  Loss: 0.47172835851528\n",
      "====> In Gradient: Iteration: 280000 grad_size: 5.60692887241179e-6 tolerance: 1.0e-6  Loss: 0.47079204182498274\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.7853031996263606e-6 tolerance: 1.0e-6  Loss: 0.46991743182810564\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.4094461730979322e-5 tolerance: 1.0e-6  Loss: 0.4691813757977649\n",
      "====> In Gradient: Iteration: 340000 grad_size: 5.65484946648998e-6 tolerance: 1.0e-6  Loss: 0.4685612007298995\n",
      "====> In Gradient: Iteration: 360000 grad_size: 2.7001175627990017e-6 tolerance: 1.0e-6  Loss: 0.46798978110722206\n",
      "====> In Gradient: Iteration: 380000 grad_size: 3.905453360977087e-6 tolerance: 1.0e-6  Loss: 0.4674608308383678\n",
      "====> In Gradient: Iteration: 400000 grad_size: 5.7011934799378166e-6 tolerance: 1.0e-6  Loss: 0.4669683044301067\n",
      "====> In Gradient: Iteration: 420000 grad_size: 2.8002984844348325e-6 tolerance: 1.0e-6  Loss: 0.4665083597132045\n",
      "====> In Gradient: Iteration: 440000 grad_size: 2.7402044604598594e-6 tolerance: 1.0e-6  Loss: 0.46607805738030017\n",
      "====> In Gradient: Iteration: 460000 grad_size: 5.735137805982744e-6 tolerance: 1.0e-6  Loss: 0.46567403636173343\n",
      "====> In Gradient: Iteration: 480000 grad_size: 2.6028990118628386e-6 tolerance: 1.0e-6  Loss: 0.4653505834912596\n",
      "====> In Gradient: Iteration: 500000 grad_size: 2.5721007377153675e-6 tolerance: 1.0e-6  Loss: 0.46506670767619496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 8.768507291669441e-6 tolerance: 1.0e-6  Loss: 0.21291413939752668\n",
      "====> In Gradient: Iteration: 40000 grad_size: 4.948816528118805e-6 tolerance: 1.0e-6  Loss: 0.19151571438278675\n",
      "====> In Gradient: Iteration: 60000 grad_size: 3.8312848914702655e-6 tolerance: 1.0e-6  Loss: 0.18223558696043923\n",
      "====> In Gradient: Iteration: 80000 grad_size: 3.309841795933219e-6 tolerance: 1.0e-6  Loss: 0.175926359217361\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.2318520977649927e-6 tolerance: 1.0e-6  Loss: 0.1713205816430684\n",
      "====> In Gradient: Iteration: 120000 grad_size: 1.891550763796033e-6 tolerance: 1.0e-6  Loss: 0.16921121708490555\n",
      "====> In Gradient: Iteration: 140000 grad_size: 5.4545063416045825e-6 tolerance: 1.0e-6  Loss: 0.1404392656872036\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.426367232910834e-6 tolerance: 1.0e-6  Loss: 0.1314971990432262\n",
      "====> In Gradient: Iteration: 180000 grad_size: 2.7227601378132496e-6 tolerance: 1.0e-6  Loss: 0.12686720820183142\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.305184466589605e-6 tolerance: 1.0e-6  Loss: 0.12373524120346027\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.009577360833387e-6 tolerance: 1.0e-6  Loss: 0.12142001857384847\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.7824346565775818e-6 tolerance: 1.0e-6  Loss: 0.11962895390164022\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.6000673341214508e-6 tolerance: 1.0e-6  Loss: 0.11820263620769708\n",
      "====> In Gradient: Iteration: 280000 grad_size: 1.4494963393550241e-6 tolerance: 1.0e-6  Loss: 0.11704266784463058\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.322692765533927e-6 tolerance: 1.0e-6  Loss: 0.11608377011742622\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.2143493518131581e-6 tolerance: 1.0e-6  Loss: 0.11528044553307776\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.1207922413963818e-6 tolerance: 1.0e-6  Loss: 0.11459976914951675\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.039372713666885e-6 tolerance: 1.0e-6  Loss: 0.11401720123904732\n",
      "====> In Gradient: Iteration: 20000 grad_size: 1.249157294933645e-5 tolerance: 1.0e-6  Loss: 0.30521991348987576\n",
      "====> In Gradient: Iteration: 40000 grad_size: 8.065133824861255e-6 tolerance: 1.0e-6  Loss: 0.2517731752324107\n",
      "====> In Gradient: Iteration: 60000 grad_size: 5.136261994672766e-6 tolerance: 1.0e-6  Loss: 0.23125972760330962\n",
      "====> In Gradient: Iteration: 80000 grad_size: 4.1541145463781416e-6 tolerance: 1.0e-6  Loss: 0.22053387789295423\n",
      "====> In Gradient: Iteration: 100000 grad_size: 3.390491836417522e-6 tolerance: 1.0e-6  Loss: 0.21345197417114084\n",
      "====> In Gradient: Iteration: 120000 grad_size: 2.786650496199314e-6 tolerance: 1.0e-6  Loss: 0.20870288144853866\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.305149677828034e-6 tolerance: 1.0e-6  Loss: 0.20547518693519962\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.9188166978413757e-6 tolerance: 1.0e-6  Loss: 0.20325352544417843\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.6073686723581852e-6 tolerance: 1.0e-6  Loss: 0.20170500221510448\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.35533542707821e-6 tolerance: 1.0e-6  Loss: 0.2006116871257942\n",
      "====> In Gradient: Iteration: 220000 grad_size: 1.1507154423723232e-6 tolerance: 1.0e-6  Loss: 0.19982932699867503\n",
      "====> In Gradient: Iteration: 20000 grad_size: 3.5599110371738316e-6 tolerance: 1.0e-6  Loss: 0.3885675354195274\n",
      "====> In Gradient: Iteration: 40000 grad_size: 3.0036632820038868e-6 tolerance: 1.0e-6  Loss: 0.3836297383438469\n",
      "====> In Gradient: Iteration: 60000 grad_size: 2.770410655561663e-6 tolerance: 1.0e-6  Loss: 0.3800460974030109\n",
      "====> In Gradient: Iteration: 80000 grad_size: 2.6868411003056204e-6 tolerance: 1.0e-6  Loss: 0.37712826118920273\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.566350437363898e-6 tolerance: 1.0e-6  Loss: 0.37465707889060124\n",
      "====> In Gradient: Iteration: 120000 grad_size: 2.7942046139766006e-6 tolerance: 1.0e-6  Loss: 0.3722556321196745\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.9131980051791337e-6 tolerance: 1.0e-6  Loss: 0.36794129881169235\n",
      "====> In Gradient: Iteration: 160000 grad_size: 2.437293915701942e-6 tolerance: 1.0e-6  Loss: 0.36492194441255105\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.9469240048460074e-6 tolerance: 1.0e-6  Loss: 0.3631309882744222\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.6449534401156888e-6 tolerance: 1.0e-6  Loss: 0.3618728909452631\n",
      "====> In Gradient: Iteration: 220000 grad_size: 1.4295270041687124e-6 tolerance: 1.0e-6  Loss: 0.36092327161566434\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.263622388727391e-6 tolerance: 1.0e-6  Loss: 0.3601781173799201\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.130532677232094e-6 tolerance: 1.0e-6  Loss: 0.3595773088426338\n",
      "====> In Gradient: Iteration: 280000 grad_size: 1.8412073638378926e-6 tolerance: 1.0e-6  Loss: 0.35908235821347667\n",
      "====> In Gradient: Iteration: 20000 grad_size: 1.6702135199677012e-5 tolerance: 1.0e-6  Loss: 0.38375839601011563\n",
      "====> In Gradient: Iteration: 40000 grad_size: 5.460905227748289e-6 tolerance: 1.0e-6  Loss: 0.3450113968989467\n",
      "====> In Gradient: Iteration: 60000 grad_size: 3.554033909071642e-6 tolerance: 1.0e-6  Loss: 0.3352988973275504\n",
      "====> In Gradient: Iteration: 80000 grad_size: 4.718429369370487e-6 tolerance: 1.0e-6  Loss: 0.33170550802002907\n",
      "====> In Gradient: Iteration: 100000 grad_size: 4.238149602809962e-6 tolerance: 1.0e-6  Loss: 0.33021296335889533\n",
      "====> In Gradient: Iteration: 120000 grad_size: 2.6627377933330272e-6 tolerance: 1.0e-6  Loss: 0.3294528508792252\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.6812729347000998e-6 tolerance: 1.0e-6  Loss: 0.3289748786149868\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.92648925979787e-6 tolerance: 1.0e-6  Loss: 0.3286223486617785\n",
      "====> In Gradient: Iteration: 180000 grad_size: 2.6843154072812594e-6 tolerance: 1.0e-6  Loss: 0.3283361006126724\n",
      "====> In Gradient: Iteration: 200000 grad_size: 3.934139827049175e-6 tolerance: 1.0e-6  Loss: 0.3280923106798775\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.6619055439273983e-6 tolerance: 1.0e-6  Loss: 0.3278792261924187\n",
      "====> In Gradient: Iteration: 240000 grad_size: 3.9623914939440994e-6 tolerance: 1.0e-6  Loss: 0.32769074411092003\n",
      "====> In Gradient: Iteration: 260000 grad_size: 3.9769644799076405e-6 tolerance: 1.0e-6  Loss: 0.3275224409164883\n",
      "====> In Gradient: Iteration: 280000 grad_size: 3.990632844886351e-6 tolerance: 1.0e-6  Loss: 0.3273717867323477\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.6179056345573035e-6 tolerance: 1.0e-6  Loss: 0.32723621943969694\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.610773678361051e-6 tolerance: 1.0e-6  Loss: 0.3271138649748123\n",
      "====> In Gradient: Iteration: 340000 grad_size: 4.024985325256336e-6 tolerance: 1.0e-6  Loss: 0.3270037260891698\n",
      "====> In Gradient: Iteration: 360000 grad_size: 2.6007064600623303e-6 tolerance: 1.0e-6  Loss: 0.3269037983453614\n",
      "====> In Gradient: Iteration: 380000 grad_size: 4.04255335767109e-6 tolerance: 1.0e-6  Loss: 0.32681319820605514\n",
      "====> In Gradient: Iteration: 400000 grad_size: 4.0499335043382325e-6 tolerance: 1.0e-6  Loss: 0.32673096657335093\n",
      "====> In Gradient: Iteration: 420000 grad_size: 4.056513675815342e-6 tolerance: 1.0e-6  Loss: 0.3266560849290937\n",
      "====> In Gradient: Iteration: 440000 grad_size: 2.5945247916193354e-6 tolerance: 1.0e-6  Loss: 0.3265876967925211\n",
      "====> In Gradient: Iteration: 460000 grad_size: 2.5951270413877675e-6 tolerance: 1.0e-6  Loss: 0.3265253101766469\n",
      "====> In Gradient: Iteration: 480000 grad_size: 2.5963102390952175e-6 tolerance: 1.0e-6  Loss: 0.3264684195056012\n",
      "====> In Gradient: Iteration: 500000 grad_size: 4.075935919938489e-6 tolerance: 1.0e-6  Loss: 0.32641611252310415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 7.792611800978633e-6 tolerance: 1.0e-6  Loss: 1.1172405986595755\n",
      "====> In Gradient: Iteration: 40000 grad_size: 5.73412204049407e-6 tolerance: 1.0e-6  Loss: 1.0961808796066006\n",
      "====> In Gradient: Iteration: 60000 grad_size: 5.414750153302625e-6 tolerance: 1.0e-6  Loss: 1.0806604494202052\n",
      "====> In Gradient: Iteration: 80000 grad_size: 5.132097176218092e-6 tolerance: 1.0e-6  Loss: 1.0667657164715836\n",
      "====> In Gradient: Iteration: 100000 grad_size: 4.874629167850031e-6 tolerance: 1.0e-6  Loss: 1.0542550898865104\n",
      "====> In Gradient: Iteration: 120000 grad_size: 4.636046887269731e-6 tolerance: 1.0e-6  Loss: 1.042952778784763\n",
      "====> In Gradient: Iteration: 140000 grad_size: 4.412775810832809e-6 tolerance: 1.0e-6  Loss: 1.0327208881092849\n",
      "====> In Gradient: Iteration: 160000 grad_size: 4.2026583764857575e-6 tolerance: 1.0e-6  Loss: 1.023445302512242\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.4223833712623961e-5 tolerance: 1.0e-6  Loss: 1.0153380403012604\n",
      "====> In Gradient: Iteration: 200000 grad_size: 3.8817612009247835e-6 tolerance: 1.0e-6  Loss: 1.0093925411865825\n",
      "====> In Gradient: Iteration: 220000 grad_size: 3.76627322112913e-6 tolerance: 1.0e-6  Loss: 1.004007412386474\n",
      "====> In Gradient: Iteration: 240000 grad_size: 3.6591633030755833e-6 tolerance: 1.0e-6  Loss: 0.9990760892888498\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.2152624688854019e-5 tolerance: 1.0e-6  Loss: 0.9880953423577127\n",
      "====> In Gradient: Iteration: 280000 grad_size: 5.2525676178361385e-6 tolerance: 1.0e-6  Loss: 0.9779558273299143\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.179496519261926e-5 tolerance: 1.0e-6  Loss: 0.9350853047652937\n",
      "====> In Gradient: Iteration: 320000 grad_size: 5.708780756475207e-6 tolerance: 1.0e-6  Loss: 0.9095866852079114\n",
      "====> In Gradient: Iteration: 340000 grad_size: 2.9399802608889536e-6 tolerance: 1.0e-6  Loss: 0.9034428554696571\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.7227880542347282e-6 tolerance: 1.0e-6  Loss: 0.9016497930213778\n",
      "====> In Gradient: Iteration: 380000 grad_size: 1.2638788338048315e-6 tolerance: 1.0e-6  Loss: 0.9009309673100865\n",
      "====> In Gradient: Iteration: 400000 grad_size: 1.1958695612856742e-6 tolerance: 1.0e-6  Loss: 0.9004903616795944\n",
      "====> In Gradient: Iteration: 20000 grad_size: 9.699401045250142e-6 tolerance: 1.0e-6  Loss: 0.7046592343057547\n",
      "====> In Gradient: Iteration: 40000 grad_size: 7.236196971074706e-6 tolerance: 1.0e-6  Loss: 0.6694126986717156\n",
      "====> In Gradient: Iteration: 60000 grad_size: 8.809407062177006e-6 tolerance: 1.0e-6  Loss: 0.6405338438807104\n",
      "====> In Gradient: Iteration: 80000 grad_size: 8.875011985031513e-6 tolerance: 1.0e-6  Loss: 0.6218978239421252\n",
      "====> In Gradient: Iteration: 100000 grad_size: 6.759106947214907e-6 tolerance: 1.0e-6  Loss: 0.6031573628733161\n",
      "====> In Gradient: Iteration: 120000 grad_size: 6.054044980919061e-6 tolerance: 1.0e-6  Loss: 0.5895349084991601\n",
      "====> In Gradient: Iteration: 140000 grad_size: 5.6278909091017624e-6 tolerance: 1.0e-6  Loss: 0.5789765493508553\n",
      "====> In Gradient: Iteration: 160000 grad_size: 5.2266093645254e-6 tolerance: 1.0e-6  Loss: 0.5702732165442478\n",
      "====> In Gradient: Iteration: 180000 grad_size: 4.92225208339318e-6 tolerance: 1.0e-6  Loss: 0.5631130399767472\n",
      "====> In Gradient: Iteration: 200000 grad_size: 4.450632117572928e-6 tolerance: 1.0e-6  Loss: 0.5578742105927766\n",
      "====> In Gradient: Iteration: 220000 grad_size: 4.156541167051107e-6 tolerance: 1.0e-6  Loss: 0.5533970659595321\n",
      "====> In Gradient: Iteration: 240000 grad_size: 3.907733257621079e-6 tolerance: 1.0e-6  Loss: 0.5495174594188438\n",
      "====> In Gradient: Iteration: 260000 grad_size: 6.29279549448383e-6 tolerance: 1.0e-6  Loss: 0.5461401918736487\n",
      "====> In Gradient: Iteration: 280000 grad_size: 3.3470578403517047e-6 tolerance: 1.0e-6  Loss: 0.5433179444340683\n",
      "====> In Gradient: Iteration: 300000 grad_size: 3.1507992443824593e-6 tolerance: 1.0e-6  Loss: 0.5409031683577487\n",
      "====> In Gradient: Iteration: 320000 grad_size: 3.1484693557093057e-6 tolerance: 1.0e-6  Loss: 0.5387674370704506\n",
      "====> In Gradient: Iteration: 340000 grad_size: 2.8736865280819178e-6 tolerance: 1.0e-6  Loss: 0.5368639141465124\n",
      "====> In Gradient: Iteration: 360000 grad_size: 2.7578983539909093e-6 tolerance: 1.0e-6  Loss: 0.5351576847170867\n",
      "====> In Gradient: Iteration: 380000 grad_size: 6.485051074670487e-6 tolerance: 1.0e-6  Loss: 0.5336207980651608\n",
      "====> In Gradient: Iteration: 400000 grad_size: 6.504435695348081e-6 tolerance: 1.0e-6  Loss: 0.5322294028495159\n",
      "====> In Gradient: Iteration: 420000 grad_size: 2.4606078653318525e-6 tolerance: 1.0e-6  Loss: 0.5309640693260413\n",
      "====> In Gradient: Iteration: 440000 grad_size: 2.3743507610961244e-6 tolerance: 1.0e-6  Loss: 0.5298091798640275\n",
      "====> In Gradient: Iteration: 460000 grad_size: 6.577787797504626e-6 tolerance: 1.0e-6  Loss: 0.5287511532837268\n",
      "====> In Gradient: Iteration: 480000 grad_size: 6.589923661514667e-6 tolerance: 1.0e-6  Loss: 0.527778069044227\n",
      "====> In Gradient: Iteration: 500000 grad_size: 2.291337973295337e-6 tolerance: 1.0e-6  Loss: 0.5268804201128826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.0825228282062494e-5 tolerance: 1.0e-6  Loss: 0.5824995211169977\n",
      "====> In Gradient: Iteration: 40000 grad_size: 1.0187810419226426e-5 tolerance: 1.0e-6  Loss: 0.5428360441282729\n",
      "====> In Gradient: Iteration: 60000 grad_size: 7.769235525936632e-6 tolerance: 1.0e-6  Loss: 0.5238835910610619\n",
      "====> In Gradient: Iteration: 80000 grad_size: 6.777116900565572e-6 tolerance: 1.0e-6  Loss: 0.5133983751512827\n",
      "====> In Gradient: Iteration: 100000 grad_size: 1.1970213012810032e-5 tolerance: 1.0e-6  Loss: 0.4940202011562252\n",
      "====> In Gradient: Iteration: 120000 grad_size: 8.683406540702745e-6 tolerance: 1.0e-6  Loss: 0.4531300494843998\n",
      "====> In Gradient: Iteration: 140000 grad_size: 7.628516886820854e-6 tolerance: 1.0e-6  Loss: 0.429225380394037\n",
      "====> In Gradient: Iteration: 160000 grad_size: 6.440874091645644e-6 tolerance: 1.0e-6  Loss: 0.41658889356090667\n",
      "====> In Gradient: Iteration: 180000 grad_size: 5.5696354071558365e-6 tolerance: 1.0e-6  Loss: 0.40757979248321075\n",
      "====> In Gradient: Iteration: 200000 grad_size: 7.02292028501179e-6 tolerance: 1.0e-6  Loss: 0.40180395554813547\n",
      "====> In Gradient: Iteration: 220000 grad_size: 1.7940142547824958e-5 tolerance: 1.0e-6  Loss: 0.39773901367580833\n",
      "====> In Gradient: Iteration: 240000 grad_size: 3.96092349354841e-6 tolerance: 1.0e-6  Loss: 0.39476026584393215\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.3623086234986135e-5 tolerance: 1.0e-6  Loss: 0.3925211197290561\n",
      "====> In Gradient: Iteration: 280000 grad_size: 4.069661865569236e-6 tolerance: 1.0e-6  Loss: 0.3907938353874547\n",
      "====> In Gradient: Iteration: 300000 grad_size: 6.090545178389585e-6 tolerance: 1.0e-6  Loss: 0.38942380145707883\n",
      "====> In Gradient: Iteration: 320000 grad_size: 5.977465644552163e-6 tolerance: 1.0e-6  Loss: 0.38829042955021276\n",
      "====> In Gradient: Iteration: 340000 grad_size: 3.066254141395776e-6 tolerance: 1.0e-6  Loss: 0.38731077170082867\n",
      "====> In Gradient: Iteration: 360000 grad_size: 2.983490850342143e-6 tolerance: 1.0e-6  Loss: 0.3864351665930219\n",
      "====> In Gradient: Iteration: 380000 grad_size: 3.1641986185353617e-6 tolerance: 1.0e-6  Loss: 0.38563176539910143\n",
      "====> In Gradient: Iteration: 400000 grad_size: 3.2581816141287363e-6 tolerance: 1.0e-6  Loss: 0.3848813363839902\n",
      "====> In Gradient: Iteration: 420000 grad_size: 3.348901296101875e-6 tolerance: 1.0e-6  Loss: 0.38417157219201187\n",
      "====> In Gradient: Iteration: 440000 grad_size: 3.315842378350257e-6 tolerance: 1.0e-6  Loss: 0.3834945823312161\n",
      "====> In Gradient: Iteration: 460000 grad_size: 3.275524430880775e-6 tolerance: 1.0e-6  Loss: 0.3828449944675703\n",
      "====> In Gradient: Iteration: 480000 grad_size: 2.638865101467909e-6 tolerance: 1.0e-6  Loss: 0.3822196491670601\n",
      "====> In Gradient: Iteration: 500000 grad_size: 5.027307606506055e-6 tolerance: 1.0e-6  Loss: 0.3816162686645531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.3327491049550771e-5 tolerance: 1.0e-6  Loss: 0.49110167612189204\n",
      "====> In Gradient: Iteration: 40000 grad_size: 9.527235394863816e-6 tolerance: 1.0e-6  Loss: 0.4344504269227712\n",
      "====> In Gradient: Iteration: 60000 grad_size: 8.081780850287563e-6 tolerance: 1.0e-6  Loss: 0.39593169624915264\n",
      "====> In Gradient: Iteration: 80000 grad_size: 7.380981592678865e-6 tolerance: 1.0e-6  Loss: 0.3665626805159128\n",
      "====> In Gradient: Iteration: 100000 grad_size: 5.7941617530211275e-6 tolerance: 1.0e-6  Loss: 0.34544254824893933\n",
      "====> In Gradient: Iteration: 120000 grad_size: 4.1417300458925094e-6 tolerance: 1.0e-6  Loss: 0.33467852593148784\n",
      "====> In Gradient: Iteration: 140000 grad_size: 1.4685451002348652e-6 tolerance: 1.0e-6  Loss: 0.3275944602078083\n",
      "====> In Gradient: Iteration: 20000 grad_size: 9.177119875015722e-6 tolerance: 1.0e-6  Loss: 0.47621706036868267\n",
      "====> In Gradient: Iteration: 40000 grad_size: 6.6516637370689775e-6 tolerance: 1.0e-6  Loss: 0.43812727285494885\n",
      "====> In Gradient: Iteration: 60000 grad_size: 5.4322015591193085e-6 tolerance: 1.0e-6  Loss: 0.420062309770097\n",
      "====> In Gradient: Iteration: 80000 grad_size: 4.577825368118959e-6 tolerance: 1.0e-6  Loss: 0.4076157799842374\n",
      "====> In Gradient: Iteration: 100000 grad_size: 3.927986457401137e-6 tolerance: 1.0e-6  Loss: 0.39861078946651995\n",
      "====> In Gradient: Iteration: 120000 grad_size: 3.409410408005843e-6 tolerance: 1.0e-6  Loss: 0.3919030994909021\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.9843709103576125e-6 tolerance: 1.0e-6  Loss: 0.38680701810953055\n",
      "====> In Gradient: Iteration: 160000 grad_size: 2.7045090042165467e-6 tolerance: 1.0e-6  Loss: 0.38383513777432293\n",
      "====> In Gradient: Iteration: 180000 grad_size: 3.480241428651529e-6 tolerance: 1.0e-6  Loss: 0.3768998896325134\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.64180164086227e-6 tolerance: 1.0e-6  Loss: 0.37232149347501736\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.13550430992967e-6 tolerance: 1.0e-6  Loss: 0.36949607119534944\n",
      "====> In Gradient: Iteration: 240000 grad_size: 3.1068711548861278e-6 tolerance: 1.0e-6  Loss: 0.3642966392888932\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.484069769011163e-6 tolerance: 1.0e-6  Loss: 0.360429193564561\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.0331189288826944e-6 tolerance: 1.0e-6  Loss: 0.35789946425112334\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.7034338700451427e-6 tolerance: 1.0e-6  Loss: 0.35616784847045024\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.4658894966826559e-6 tolerance: 1.0e-6  Loss: 0.35492134281558557\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.297650631739288e-6 tolerance: 1.0e-6  Loss: 0.3539727429069649\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.1806521981722956e-6 tolerance: 1.0e-6  Loss: 0.35323416341658187\n",
      "====> In Gradient: Iteration: 380000 grad_size: 1.1187459721056238e-6 tolerance: 1.0e-6  Loss: 0.35279605191165114\n",
      "====> In Gradient: Iteration: 400000 grad_size: 9.782842436052785e-6 tolerance: 1.0e-6  Loss: 0.35246150440632235\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.0601301084736422e-6 tolerance: 1.0e-6  Loss: 0.35219177564123844\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.0445848864821105e-6 tolerance: 1.0e-6  Loss: 0.3519669390454702\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.0334711417826332e-6 tolerance: 1.0e-6  Loss: 0.3517748351709173\n",
      "====> In Gradient: Iteration: 480000 grad_size: 1.0255351404826484e-6 tolerance: 1.0e-6  Loss: 0.35160785270709544\n",
      "====> In Gradient: Iteration: 500000 grad_size: 1.0197250605830062e-6 tolerance: 1.0e-6  Loss: 0.3514612728661119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 8.736875200223887e-6 tolerance: 1.0e-6  Loss: 0.4752373440404529\n",
      "====> In Gradient: Iteration: 40000 grad_size: 6.848808419298821e-6 tolerance: 1.0e-6  Loss: 0.445630515800382\n",
      "====> In Gradient: Iteration: 60000 grad_size: 6.135797018100929e-6 tolerance: 1.0e-6  Loss: 0.4295414289338428\n",
      "====> In Gradient: Iteration: 80000 grad_size: 5.657167412344421e-6 tolerance: 1.0e-6  Loss: 0.4173502947106613\n",
      "====> In Gradient: Iteration: 100000 grad_size: 8.99279767470128e-6 tolerance: 1.0e-6  Loss: 0.4074208665402764\n",
      "====> In Gradient: Iteration: 120000 grad_size: 8.98271829083037e-6 tolerance: 1.0e-6  Loss: 0.39952954017971803\n",
      "====> In Gradient: Iteration: 140000 grad_size: 8.987350818255448e-6 tolerance: 1.0e-6  Loss: 0.39280557081448486\n",
      "====> In Gradient: Iteration: 160000 grad_size: 4.669303139161803e-6 tolerance: 1.0e-6  Loss: 0.3868407662743887\n",
      "====> In Gradient: Iteration: 180000 grad_size: 4.425597244774701e-6 tolerance: 1.0e-6  Loss: 0.38184192195615746\n",
      "====> In Gradient: Iteration: 200000 grad_size: 4.250882957116375e-6 tolerance: 1.0e-6  Loss: 0.37747079709503395\n",
      "====> In Gradient: Iteration: 220000 grad_size: 4.113997909821653e-6 tolerance: 1.0e-6  Loss: 0.3735109974545843\n",
      "====> In Gradient: Iteration: 240000 grad_size: 3.999699425188533e-6 tolerance: 1.0e-6  Loss: 0.3698413297802079\n",
      "====> In Gradient: Iteration: 260000 grad_size: 3.899878819017146e-6 tolerance: 1.0e-6  Loss: 0.3663918674407878\n",
      "====> In Gradient: Iteration: 280000 grad_size: 7.571378108023205e-6 tolerance: 1.0e-6  Loss: 0.36311924180087757\n",
      "====> In Gradient: Iteration: 300000 grad_size: 7.555827927470611e-6 tolerance: 1.0e-6  Loss: 0.35999567689288603\n",
      "====> In Gradient: Iteration: 320000 grad_size: 7.540449592452506e-6 tolerance: 1.0e-6  Loss: 0.3570026845660888\n",
      "====> In Gradient: Iteration: 340000 grad_size: 7.555118959131933e-6 tolerance: 1.0e-6  Loss: 0.3541673509846095\n",
      "====> In Gradient: Iteration: 360000 grad_size: 3.4706305614965816e-6 tolerance: 1.0e-6  Loss: 0.35153300603221166\n",
      "====> In Gradient: Iteration: 380000 grad_size: 3.3973321449506918e-6 tolerance: 1.0e-6  Loss: 0.34902124383113464\n",
      "====> In Gradient: Iteration: 400000 grad_size: 3.333374421616214e-6 tolerance: 1.0e-6  Loss: 0.34662107082699173\n",
      "====> In Gradient: Iteration: 420000 grad_size: 3.2757552707396417e-6 tolerance: 1.0e-6  Loss: 0.3443246093160088\n",
      "====> In Gradient: Iteration: 440000 grad_size: 3.223198501192345e-6 tolerance: 1.0e-6  Loss: 0.3421255308005801\n",
      "====> In Gradient: Iteration: 460000 grad_size: 2.9019472965628015e-6 tolerance: 1.0e-6  Loss: 0.3400184419327167\n",
      "====> In Gradient: Iteration: 480000 grad_size: 8.45662367992003e-6 tolerance: 1.0e-6  Loss: 0.33799898753384516\n",
      "====> In Gradient: Iteration: 500000 grad_size: 3.0901054625010983e-6 tolerance: 1.0e-6  Loss: 0.3360618175156151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 2.0407557227121964e-5 tolerance: 1.0e-6  Loss: 0.2910325158175249\n",
      "====> In Gradient: Iteration: 40000 grad_size: 6.119538484403644e-6 tolerance: 1.0e-6  Loss: 0.2180179992876074\n",
      "====> In Gradient: Iteration: 60000 grad_size: 3.976436621016228e-6 tolerance: 1.0e-6  Loss: 0.20623819981163727\n",
      "====> In Gradient: Iteration: 80000 grad_size: 2.8861563475932133e-6 tolerance: 1.0e-6  Loss: 0.20044084615706986\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.547169703583963e-6 tolerance: 1.0e-6  Loss: 0.19678249276062762\n",
      "====> In Gradient: Iteration: 120000 grad_size: 2.303996162423321e-6 tolerance: 1.0e-6  Loss: 0.19386039153564363\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.1232628676238065e-6 tolerance: 1.0e-6  Loss: 0.1914271316208254\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.9859240095419805e-6 tolerance: 1.0e-6  Loss: 0.18933538489875015\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.24462320285603e-6 tolerance: 1.0e-6  Loss: 0.187604111479358\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.0444992415261828e-6 tolerance: 1.0e-6  Loss: 0.18697933332208228\n",
      "====> In Gradient: Iteration: 20000 grad_size: 2.448507112808813e-5 tolerance: 1.0e-6  Loss: 0.695305090883527\n",
      "====> In Gradient: Iteration: 40000 grad_size: 5.881451414192802e-6 tolerance: 1.0e-6  Loss: 0.62393884519616\n",
      "====> In Gradient: Iteration: 60000 grad_size: 5.4732965135109245e-6 tolerance: 1.0e-6  Loss: 0.6123735299881843\n",
      "====> In Gradient: Iteration: 80000 grad_size: 4.2662414793978015e-6 tolerance: 1.0e-6  Loss: 0.6059201935719034\n",
      "====> In Gradient: Iteration: 100000 grad_size: 4.431085801318901e-6 tolerance: 1.0e-6  Loss: 0.6012746460915905\n",
      "====> In Gradient: Iteration: 120000 grad_size: 4.5458482464246325e-6 tolerance: 1.0e-6  Loss: 0.5993384654155783\n",
      "====> In Gradient: Iteration: 140000 grad_size: 3.4891079518404675e-6 tolerance: 1.0e-6  Loss: 0.598582456152484\n",
      "====> In Gradient: Iteration: 160000 grad_size: 5.0392196274082285e-6 tolerance: 1.0e-6  Loss: 0.5980536183620884\n",
      "====> In Gradient: Iteration: 180000 grad_size: 3.135701841980242e-6 tolerance: 1.0e-6  Loss: 0.5976442684838266\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.9691308845227814e-6 tolerance: 1.0e-6  Loss: 0.5973092355357776\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.8360194323573994e-6 tolerance: 1.0e-6  Loss: 0.5970270486502322\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.7251252435961953e-6 tolerance: 1.0e-6  Loss: 0.5967856742597887\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.6302359247094694e-6 tolerance: 1.0e-6  Loss: 0.5965769445695727\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.547117440485132e-6 tolerance: 1.0e-6  Loss: 0.5963959802282718\n",
      "====> In Gradient: Iteration: 300000 grad_size: 3.646508388695204e-6 tolerance: 1.0e-6  Loss: 0.5962382268968095\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.4061746888534734e-6 tolerance: 1.0e-6  Loss: 0.5961003061990248\n",
      "====> In Gradient: Iteration: 340000 grad_size: 2.3456361248212787e-6 tolerance: 1.0e-6  Loss: 0.5959795636098089\n",
      "====> In Gradient: Iteration: 360000 grad_size: 4.004201712493535e-6 tolerance: 1.0e-6  Loss: 0.595873421258392\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.2398892156894877e-6 tolerance: 1.0e-6  Loss: 0.5957797778592052\n",
      "====> In Gradient: Iteration: 400000 grad_size: 2.193540139242419e-6 tolerance: 1.0e-6  Loss: 0.5956971729612548\n",
      "====> In Gradient: Iteration: 420000 grad_size: 4.090716538938423e-6 tolerance: 1.0e-6  Loss: 0.5956240239667476\n",
      "====> In Gradient: Iteration: 440000 grad_size: 2.1120210707971773e-6 tolerance: 1.0e-6  Loss: 0.5955589355721408\n",
      "====> In Gradient: Iteration: 460000 grad_size: 3.8467805841109664e-6 tolerance: 1.0e-6  Loss: 0.595501039589427\n",
      "====> In Gradient: Iteration: 480000 grad_size: 4.161726596803306e-6 tolerance: 1.0e-6  Loss: 0.5954494488952841\n",
      "====> In Gradient: Iteration: 500000 grad_size: 2.0128251354849626e-6 tolerance: 1.0e-6  Loss: 0.5954032064003506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.2995872842282369e-5 tolerance: 1.0e-6  Loss: 0.2737490334066715\n",
      "====> In Gradient: Iteration: 40000 grad_size: 4.6295251305248045e-6 tolerance: 1.0e-6  Loss: 0.24532301365775333\n",
      "====> In Gradient: Iteration: 60000 grad_size: 5.084802812394123e-6 tolerance: 1.0e-6  Loss: 0.2363197992103876\n",
      "====> In Gradient: Iteration: 80000 grad_size: 5.062648184126111e-6 tolerance: 1.0e-6  Loss: 0.22955015837432133\n",
      "====> In Gradient: Iteration: 100000 grad_size: 3.18605377105378e-6 tolerance: 1.0e-6  Loss: 0.22439577620106343\n",
      "====> In Gradient: Iteration: 120000 grad_size: 4.086490567921769e-6 tolerance: 1.0e-6  Loss: 0.22001766839694065\n",
      "====> In Gradient: Iteration: 140000 grad_size: 3.6824456788394664e-6 tolerance: 1.0e-6  Loss: 0.21626523477905307\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.318931569080456e-6 tolerance: 1.0e-6  Loss: 0.21303519104357907\n",
      "====> In Gradient: Iteration: 180000 grad_size: 2.4950745871123825e-6 tolerance: 1.0e-6  Loss: 0.21024510650441033\n",
      "====> In Gradient: Iteration: 200000 grad_size: 4.201937352039998e-6 tolerance: 1.0e-6  Loss: 0.2078856372860206\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.3911779552681717e-6 tolerance: 1.0e-6  Loss: 0.20596384038711973\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.178843892657597e-6 tolerance: 1.0e-6  Loss: 0.20433457722298923\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.0308360202756977e-6 tolerance: 1.0e-6  Loss: 0.20293977111109662\n",
      "====> In Gradient: Iteration: 280000 grad_size: 1.938547829788123e-6 tolerance: 1.0e-6  Loss: 0.20173504521565214\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.8923440580043097e-6 tolerance: 1.0e-6  Loss: 0.20068523657386222\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.8785572586167078e-6 tolerance: 1.0e-6  Loss: 0.1997592466737097\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.874463768108451e-6 tolerance: 1.0e-6  Loss: 0.1989229035651708\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.872389241157883e-6 tolerance: 1.0e-6  Loss: 0.1981568810701266\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.0865642315032375e-6 tolerance: 1.0e-6  Loss: 0.19745009507541406\n",
      "====> In Gradient: Iteration: 400000 grad_size: 2.018292061695839e-6 tolerance: 1.0e-6  Loss: 0.19679462157941605\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.957237536217222e-6 tolerance: 1.0e-6  Loss: 0.19618391229664345\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.906471206873347e-6 tolerance: 1.0e-6  Loss: 0.19565536383125523\n",
      "====> In Gradient: Iteration: 460000 grad_size: 2.2551812729971524e-6 tolerance: 1.0e-6  Loss: 0.19531070589584523\n",
      "====> In Gradient: Iteration: 480000 grad_size: 2.0788491355904937e-6 tolerance: 1.0e-6  Loss: 0.19505479044806623\n",
      "====> In Gradient: Iteration: 500000 grad_size: 2.266891053386386e-6 tolerance: 1.0e-6  Loss: 0.19486076492065096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.3615893044471085e-5 tolerance: 1.0e-6  Loss: 0.48784346554277025\n",
      "====> In Gradient: Iteration: 40000 grad_size: 5.977879537875858e-6 tolerance: 1.0e-6  Loss: 0.4319525556744813\n",
      "====> In Gradient: Iteration: 60000 grad_size: 5.176316489082146e-6 tolerance: 1.0e-6  Loss: 0.41767709936332303\n",
      "====> In Gradient: Iteration: 80000 grad_size: 5.266299911450154e-6 tolerance: 1.0e-6  Loss: 0.40927377572700385\n",
      "====> In Gradient: Iteration: 100000 grad_size: 3.986934939445882e-6 tolerance: 1.0e-6  Loss: 0.40336591330036103\n",
      "====> In Gradient: Iteration: 120000 grad_size: 3.817173637066013e-6 tolerance: 1.0e-6  Loss: 0.3991705239502873\n",
      "====> In Gradient: Iteration: 140000 grad_size: 6.343282520555608e-6 tolerance: 1.0e-6  Loss: 0.39554306844581716\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.4582258064546034e-6 tolerance: 1.0e-6  Loss: 0.39237213539883387\n",
      "====> In Gradient: Iteration: 180000 grad_size: 3.276317084413212e-6 tolerance: 1.0e-6  Loss: 0.38958506331905784\n",
      "====> In Gradient: Iteration: 200000 grad_size: 3.0983730997605998e-6 tolerance: 1.0e-6  Loss: 0.3871273122908566\n",
      "====> In Gradient: Iteration: 220000 grad_size: 6.161963540542136e-6 tolerance: 1.0e-6  Loss: 0.38495397347574417\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.7665900455767063e-6 tolerance: 1.0e-6  Loss: 0.38302399806246923\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.614915972637142e-6 tolerance: 1.0e-6  Loss: 0.381304007769197\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.4730399610556804e-6 tolerance: 1.0e-6  Loss: 0.3797654210573454\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.340685211895289e-6 tolerance: 1.0e-6  Loss: 0.37838415791693564\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.2175284579254205e-6 tolerance: 1.0e-6  Loss: 0.37713967813440724\n",
      "====> In Gradient: Iteration: 340000 grad_size: 2.102927602213209e-6 tolerance: 1.0e-6  Loss: 0.3760140732934167\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.9964963451377246e-6 tolerance: 1.0e-6  Loss: 0.3749926047304756\n",
      "====> In Gradient: Iteration: 380000 grad_size: 1.8976088743283287e-6 tolerance: 1.0e-6  Loss: 0.3740623241968054\n",
      "====> In Gradient: Iteration: 400000 grad_size: 1.8056397284536158e-6 tolerance: 1.0e-6  Loss: 0.3732121514758251\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.7202634031656025e-6 tolerance: 1.0e-6  Loss: 0.3724324788609985\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.6407707921251745e-6 tolerance: 1.0e-6  Loss: 0.37171523251374855\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.0711452320008063e-5 tolerance: 1.0e-6  Loss: 0.37105336028186825\n",
      "====> In Gradient: Iteration: 480000 grad_size: 1.4978064460799683e-6 tolerance: 1.0e-6  Loss: 0.37044061541745377\n",
      "====> In Gradient: Iteration: 500000 grad_size: 6.163136793229702e-6 tolerance: 1.0e-6  Loss: 0.3698719394344327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 8.336835850869827e-6 tolerance: 1.0e-6  Loss: 0.3570153850733643\n",
      "====> In Gradient: Iteration: 40000 grad_size: 5.745141082291368e-6 tolerance: 1.0e-6  Loss: 0.3338401492289202\n",
      "====> In Gradient: Iteration: 60000 grad_size: 4.899863859076377e-6 tolerance: 1.0e-6  Loss: 0.31996057125685606\n",
      "====> In Gradient: Iteration: 80000 grad_size: 4.4414641166599086e-6 tolerance: 1.0e-6  Loss: 0.3091116098300591\n",
      "====> In Gradient: Iteration: 100000 grad_size: 4.090012133850384e-6 tolerance: 1.0e-6  Loss: 0.30003056607715434\n",
      "====> In Gradient: Iteration: 120000 grad_size: 3.7864204375670748e-6 tolerance: 1.0e-6  Loss: 0.2922842620485215\n",
      "====> In Gradient: Iteration: 140000 grad_size: 3.515810434849001e-6 tolerance: 1.0e-6  Loss: 0.2856249118995412\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.272674926294176e-6 tolerance: 1.0e-6  Loss: 0.2798692900077474\n",
      "====> In Gradient: Iteration: 180000 grad_size: 3.053733635781083e-6 tolerance: 1.0e-6  Loss: 0.2748704029555308\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.856402630622313e-6 tolerance: 1.0e-6  Loss: 0.27050762078323776\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.6784411075177093e-6 tolerance: 1.0e-6  Loss: 0.26668123434714736\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.517852809208932e-6 tolerance: 1.0e-6  Loss: 0.26330855090412\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.571650783949967e-6 tolerance: 1.0e-6  Loss: 0.2603807017375253\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.2754706021398565e-6 tolerance: 1.0e-6  Loss: 0.2577859807695406\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.1763387906854505e-6 tolerance: 1.0e-6  Loss: 0.255449601130112\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.0846096184119984e-6 tolerance: 1.0e-6  Loss: 0.2533281949838591\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.99864623770035e-6 tolerance: 1.0e-6  Loss: 0.2513898275010337\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.9175989691522855e-6 tolerance: 1.0e-6  Loss: 0.24960977223184133\n",
      "====> In Gradient: Iteration: 380000 grad_size: 1.8877836994051295e-6 tolerance: 1.0e-6  Loss: 0.2479685553249645\n",
      "====> In Gradient: Iteration: 400000 grad_size: 1.824664567170553e-6 tolerance: 1.0e-6  Loss: 0.24651343766494957\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.6588472957510305e-6 tolerance: 1.0e-6  Loss: 0.24525798585715702\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.5676697632683764e-6 tolerance: 1.0e-6  Loss: 0.24412439249121046\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.4887743267644969e-6 tolerance: 1.0e-6  Loss: 0.24309232201995265\n",
      "====> In Gradient: Iteration: 480000 grad_size: 1.7354968350524227e-6 tolerance: 1.0e-6  Loss: 0.24214808238056318\n",
      "====> In Gradient: Iteration: 500000 grad_size: 1.6644163253217418e-6 tolerance: 1.0e-6  Loss: 0.2412813879851914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.9817349121974094e-5 tolerance: 1.0e-6  Loss: 0.6165715571414656\n",
      "====> In Gradient: Iteration: 40000 grad_size: 7.63634409765353e-6 tolerance: 1.0e-6  Loss: 0.5079655648127718\n",
      "====> In Gradient: Iteration: 60000 grad_size: 3.1976012844039086e-6 tolerance: 1.0e-6  Loss: 0.4963802577853532\n",
      "====> In Gradient: Iteration: 80000 grad_size: 2.7941900585770315e-6 tolerance: 1.0e-6  Loss: 0.49394032620751654\n",
      "====> In Gradient: Iteration: 100000 grad_size: 4.481316547089494e-6 tolerance: 1.0e-6  Loss: 0.49258193891452356\n",
      "====> In Gradient: Iteration: 120000 grad_size: 4.477780805630112e-6 tolerance: 1.0e-6  Loss: 0.4915728134970825\n",
      "====> In Gradient: Iteration: 140000 grad_size: 4.267663705485025e-6 tolerance: 1.0e-6  Loss: 0.49074793634889613\n",
      "====> In Gradient: Iteration: 160000 grad_size: 4.348310615137491e-6 tolerance: 1.0e-6  Loss: 0.4900391211868602\n",
      "====> In Gradient: Iteration: 180000 grad_size: 4.1196016872186795e-6 tolerance: 1.0e-6  Loss: 0.48957841375604816\n",
      "====> In Gradient: Iteration: 200000 grad_size: 4.052357525683354e-6 tolerance: 1.0e-6  Loss: 0.48928530215055477\n",
      "====> In Gradient: Iteration: 220000 grad_size: 4.007037125529618e-6 tolerance: 1.0e-6  Loss: 0.48905746533527605\n",
      "====> In Gradient: Iteration: 240000 grad_size: 3.0506930692337582e-6 tolerance: 1.0e-6  Loss: 0.48886683523700836\n",
      "====> In Gradient: Iteration: 260000 grad_size: 3.947623987447238e-6 tolerance: 1.0e-6  Loss: 0.4886978600593299\n",
      "====> In Gradient: Iteration: 280000 grad_size: 1.1226283882553557e-5 tolerance: 1.0e-6  Loss: 0.48854183870351875\n",
      "====> In Gradient: Iteration: 300000 grad_size: 3.905470536049958e-6 tolerance: 1.0e-6  Loss: 0.48839399695775054\n",
      "====> In Gradient: Iteration: 320000 grad_size: 3.886920385737027e-6 tolerance: 1.0e-6  Loss: 0.4882525791451137\n",
      "====> In Gradient: Iteration: 340000 grad_size: 3.8691901200109325e-6 tolerance: 1.0e-6  Loss: 0.48811613577850765\n",
      "====> In Gradient: Iteration: 360000 grad_size: 3.0587276556457174e-6 tolerance: 1.0e-6  Loss: 0.4879834682013515\n",
      "====> In Gradient: Iteration: 380000 grad_size: 3.0681110481173744e-6 tolerance: 1.0e-6  Loss: 0.4878544567634446\n",
      "====> In Gradient: Iteration: 400000 grad_size: 9.820376918836651e-6 tolerance: 1.0e-6  Loss: 0.4877293182471294\n",
      "====> In Gradient: Iteration: 420000 grad_size: 3.089166482217212e-6 tolerance: 1.0e-6  Loss: 0.4876063873318549\n",
      "====> In Gradient: Iteration: 440000 grad_size: 3.100335325664071e-6 tolerance: 1.0e-6  Loss: 0.4874869639189333\n",
      "====> In Gradient: Iteration: 460000 grad_size: 3.7729492064198957e-6 tolerance: 1.0e-6  Loss: 0.4873707033910462\n",
      "====> In Gradient: Iteration: 480000 grad_size: 3.757995599534031e-6 tolerance: 1.0e-6  Loss: 0.48725699402454253\n",
      "====> In Gradient: Iteration: 500000 grad_size: 3.7433347606359143e-6 tolerance: 1.0e-6  Loss: 0.48714559621707876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 2.3667841633231026e-5 tolerance: 1.0e-6  Loss: 0.7167602466436362\n",
      "====> In Gradient: Iteration: 40000 grad_size: 1.809980797127246e-5 tolerance: 1.0e-6  Loss: 0.5563328658059021\n",
      "====> In Gradient: Iteration: 60000 grad_size: 8.944388424502702e-6 tolerance: 1.0e-6  Loss: 0.4704006142904294\n",
      "====> In Gradient: Iteration: 80000 grad_size: 5.881011599139052e-6 tolerance: 1.0e-6  Loss: 0.4443307179571938\n",
      "====> In Gradient: Iteration: 100000 grad_size: 5.474971498022922e-6 tolerance: 1.0e-6  Loss: 0.4302553138295673\n",
      "====> In Gradient: Iteration: 120000 grad_size: 4.775048506928355e-6 tolerance: 1.0e-6  Loss: 0.41835447833764416\n",
      "====> In Gradient: Iteration: 140000 grad_size: 4.35857346750683e-6 tolerance: 1.0e-6  Loss: 0.40908467112454844\n",
      "====> In Gradient: Iteration: 160000 grad_size: 9.30892039386311e-6 tolerance: 1.0e-6  Loss: 0.36994025317996676\n",
      "====> In Gradient: Iteration: 180000 grad_size: 4.4091351695712355e-6 tolerance: 1.0e-6  Loss: 0.34938078982721893\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.7436401182385473e-6 tolerance: 1.0e-6  Loss: 0.3434884589089503\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.084529972679464e-6 tolerance: 1.0e-6  Loss: 0.3407856536167331\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.8369462112561385e-6 tolerance: 1.0e-6  Loss: 0.3389909954040046\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.6546152654713656e-6 tolerance: 1.0e-6  Loss: 0.33757243598196485\n",
      "====> In Gradient: Iteration: 280000 grad_size: 1.5932239574629493e-6 tolerance: 1.0e-6  Loss: 0.3363585369521642\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.5229741877266062e-6 tolerance: 1.0e-6  Loss: 0.3352748938786953\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.4328108218646201e-6 tolerance: 1.0e-6  Loss: 0.33428216018831564\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.3922665386204952e-6 tolerance: 1.0e-6  Loss: 0.3333567599874773\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.3597959921047942e-6 tolerance: 1.0e-6  Loss: 0.33248331284905575\n",
      "====> In Gradient: Iteration: 380000 grad_size: 1.3533892399635555e-6 tolerance: 1.0e-6  Loss: 0.33165117678779255\n",
      "====> In Gradient: Iteration: 400000 grad_size: 1.3261603313335284e-6 tolerance: 1.0e-6  Loss: 0.3308527066440038\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.2910875038777185e-6 tolerance: 1.0e-6  Loss: 0.33008226466740803\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.2739152055654671e-6 tolerance: 1.0e-6  Loss: 0.32933561187871496\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.2640871560237848e-6 tolerance: 1.0e-6  Loss: 0.3286095219536629\n",
      "====> In Gradient: Iteration: 480000 grad_size: 1.2442546442128692e-6 tolerance: 1.0e-6  Loss: 0.3279015215180748\n",
      "====> In Gradient: Iteration: 500000 grad_size: 1.2310256693334972e-6 tolerance: 1.0e-6  Loss: 0.32720968267450656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 8.480451460357993e-6 tolerance: 1.0e-6  Loss: 0.5854979176616558\n",
      "====> In Gradient: Iteration: 40000 grad_size: 6.1721830340701394e-6 tolerance: 1.0e-6  Loss: 0.560228642542558\n",
      "====> In Gradient: Iteration: 60000 grad_size: 8.307564938833263e-6 tolerance: 1.0e-6  Loss: 0.5414125021549601\n",
      "====> In Gradient: Iteration: 80000 grad_size: 6.031207862543668e-6 tolerance: 1.0e-6  Loss: 0.5152081129756673\n",
      "====> In Gradient: Iteration: 100000 grad_size: 5.238048814417765e-6 tolerance: 1.0e-6  Loss: 0.5005913776957789\n",
      "====> In Gradient: Iteration: 120000 grad_size: 4.694968805373756e-6 tolerance: 1.0e-6  Loss: 0.48912013616210925\n",
      "====> In Gradient: Iteration: 140000 grad_size: 5.3460457969956625e-6 tolerance: 1.0e-6  Loss: 0.47977334164622115\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.87835614916407e-6 tolerance: 1.0e-6  Loss: 0.47204169923346173\n",
      "====> In Gradient: Iteration: 180000 grad_size: 3.5509691378069816e-6 tolerance: 1.0e-6  Loss: 0.4655830526279813\n",
      "====> In Gradient: Iteration: 200000 grad_size: 3.263836359066228e-6 tolerance: 1.0e-6  Loss: 0.4601426619244368\n",
      "====> In Gradient: Iteration: 220000 grad_size: 3.010723969052235e-6 tolerance: 1.0e-6  Loss: 0.455524647788391\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.7867606652172526e-6 tolerance: 1.0e-6  Loss: 0.4515760752097272\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.5879797285032754e-6 tolerance: 1.0e-6  Loss: 0.4481760614293424\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.411119721653114e-6 tolerance: 1.0e-6  Loss: 0.44522856122721516\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.2534236483812247e-6 tolerance: 1.0e-6  Loss: 0.44265636075545584\n",
      "====> In Gradient: Iteration: 320000 grad_size: 5.0230155624265e-6 tolerance: 1.0e-6  Loss: 0.440397203499369\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.986509973348089e-6 tolerance: 1.0e-6  Loss: 0.43840034082866913\n",
      "====> In Gradient: Iteration: 360000 grad_size: 5.067461513649783e-6 tolerance: 1.0e-6  Loss: 0.43662456422761775\n",
      "====> In Gradient: Iteration: 380000 grad_size: 1.7721808141905678e-6 tolerance: 1.0e-6  Loss: 0.4350358046099479\n",
      "====> In Gradient: Iteration: 400000 grad_size: 5.11098733034617e-6 tolerance: 1.0e-6  Loss: 0.4336061468715073\n",
      "====> In Gradient: Iteration: 420000 grad_size: 1.5990565301308782e-6 tolerance: 1.0e-6  Loss: 0.4323123284927247\n",
      "====> In Gradient: Iteration: 440000 grad_size: 5.1486932738674075e-6 tolerance: 1.0e-6  Loss: 0.4311351393937279\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.4583647444658413e-6 tolerance: 1.0e-6  Loss: 0.4300584099408444\n",
      "====> In Gradient: Iteration: 480000 grad_size: 1.397979751114796e-6 tolerance: 1.0e-6  Loss: 0.42906872940185603\n",
      "====> In Gradient: Iteration: 500000 grad_size: 1.343253889272401e-6 tolerance: 1.0e-6  Loss: 0.4281547281980118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.1090272680154586e-5 tolerance: 1.0e-6  Loss: 0.6927430671259435\n",
      "====> In Gradient: Iteration: 40000 grad_size: 6.831173722421533e-6 tolerance: 1.0e-6  Loss: 0.6624964908339934\n",
      "====> In Gradient: Iteration: 60000 grad_size: 6.2576991294996155e-6 tolerance: 1.0e-6  Loss: 0.641174730131286\n",
      "====> In Gradient: Iteration: 80000 grad_size: 3.20108856309202e-6 tolerance: 1.0e-6  Loss: 0.634837076221103\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.760987513080064e-6 tolerance: 1.0e-6  Loss: 0.6315098416253215\n",
      "====> In Gradient: Iteration: 120000 grad_size: 2.2556972373282463e-6 tolerance: 1.0e-6  Loss: 0.6296529080100132\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.8634375817353557e-6 tolerance: 1.0e-6  Loss: 0.6285710933567487\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.9931216889056022e-6 tolerance: 1.0e-6  Loss: 0.6277715498659031\n",
      "====> In Gradient: Iteration: 180000 grad_size: 2.6585514251438725e-6 tolerance: 1.0e-6  Loss: 0.6271160502129105\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.58535152327545e-6 tolerance: 1.0e-6  Loss: 0.6265538729332477\n",
      "====> In Gradient: Iteration: 220000 grad_size: 1.8787871987628796e-6 tolerance: 1.0e-6  Loss: 0.6260594659004662\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.0176283827832215e-6 tolerance: 1.0e-6  Loss: 0.6256288002143403\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.0538361746350696e-6 tolerance: 1.0e-6  Loss: 0.6254707531693071\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.049608443997576e-6 tolerance: 1.0e-6  Loss: 0.625330974368433\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.9864788735106447e-6 tolerance: 1.0e-6  Loss: 0.6252018798226588\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.9749618455913524e-6 tolerance: 1.0e-6  Loss: 0.6250803350533971\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.9631297326679856e-6 tolerance: 1.0e-6  Loss: 0.6249643730045562\n",
      "====> In Gradient: Iteration: 360000 grad_size: 1.9511489484872544e-6 tolerance: 1.0e-6  Loss: 0.6248530956167792\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.0752047244195666e-6 tolerance: 1.0e-6  Loss: 0.6247456815621715\n",
      "====> In Gradient: Iteration: 400000 grad_size: 7.475690663411958e-6 tolerance: 1.0e-6  Loss: 0.6246421272345903\n",
      "====> In Gradient: Iteration: 420000 grad_size: 2.093120626432244e-6 tolerance: 1.0e-6  Loss: 0.6245414874024017\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.9028641767028869e-6 tolerance: 1.0e-6  Loss: 0.6244442266034593\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.8909147063577973e-6 tolerance: 1.0e-6  Loss: 0.6243496415240873\n",
      "====> In Gradient: Iteration: 480000 grad_size: 2.1219162503026896e-6 tolerance: 1.0e-6  Loss: 0.6242581663729094\n",
      "====> In Gradient: Iteration: 500000 grad_size: 1.8674477341403202e-6 tolerance: 1.0e-6  Loss: 0.6241693952593288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.0013197457871159e-5 tolerance: 1.0e-6  Loss: 0.49095540951519817\n",
      "====> In Gradient: Iteration: 40000 grad_size: 6.729615708542717e-6 tolerance: 1.0e-6  Loss: 0.45787670570704186\n",
      "====> In Gradient: Iteration: 60000 grad_size: 5.087463793192314e-6 tolerance: 1.0e-6  Loss: 0.44068814000734696\n",
      "====> In Gradient: Iteration: 80000 grad_size: 5.978307625244709e-6 tolerance: 1.0e-6  Loss: 0.4295519256054497\n",
      "====> In Gradient: Iteration: 100000 grad_size: 4.339246770457966e-6 tolerance: 1.0e-6  Loss: 0.4165644267579738\n",
      "====> In Gradient: Iteration: 120000 grad_size: 3.3388606255855083e-6 tolerance: 1.0e-6  Loss: 0.4096030036417011\n",
      "====> In Gradient: Iteration: 140000 grad_size: 1.1816741173803353e-5 tolerance: 1.0e-6  Loss: 0.3795520776699139\n",
      "====> In Gradient: Iteration: 160000 grad_size: 6.2764744979297365e-6 tolerance: 1.0e-6  Loss: 0.34283431096721184\n",
      "====> In Gradient: Iteration: 180000 grad_size: 3.925580524309902e-6 tolerance: 1.0e-6  Loss: 0.3305605787389254\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.8547604600792203e-6 tolerance: 1.0e-6  Loss: 0.32504993499270923\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.9230738580192944e-6 tolerance: 1.0e-6  Loss: 0.3216246009619149\n",
      "====> In Gradient: Iteration: 240000 grad_size: 2.3547219005151793e-6 tolerance: 1.0e-6  Loss: 0.31822270506597605\n",
      "====> In Gradient: Iteration: 260000 grad_size: 6.338133136344425e-6 tolerance: 1.0e-6  Loss: 0.31644594921458064\n",
      "====> In Gradient: Iteration: 280000 grad_size: 1.28604594326398e-6 tolerance: 1.0e-6  Loss: 0.3155376158603315\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.0512437869231831e-6 tolerance: 1.0e-6  Loss: 0.3149576120208589\n",
      "====> In Gradient: Iteration: 20000 grad_size: 6.33324282546456e-6 tolerance: 1.0e-6  Loss: 0.4662373125964776\n",
      "====> In Gradient: Iteration: 40000 grad_size: 5.025384641255937e-6 tolerance: 1.0e-6  Loss: 0.45030305792844244\n",
      "====> In Gradient: Iteration: 60000 grad_size: 4.124915340501862e-6 tolerance: 1.0e-6  Loss: 0.4399167525690267\n",
      "====> In Gradient: Iteration: 80000 grad_size: 4.070648372764018e-6 tolerance: 1.0e-6  Loss: 0.43053840971310386\n",
      "====> In Gradient: Iteration: 100000 grad_size: 3.351321293199761e-6 tolerance: 1.0e-6  Loss: 0.42368462258972756\n",
      "====> In Gradient: Iteration: 120000 grad_size: 2.780906039194972e-6 tolerance: 1.0e-6  Loss: 0.41900322941033563\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.3225795992372122e-6 tolerance: 1.0e-6  Loss: 0.4157595764033659\n",
      "====> In Gradient: Iteration: 160000 grad_size: 1.950548198285962e-6 tolerance: 1.0e-6  Loss: 0.41348486326510236\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.6458074080204327e-6 tolerance: 1.0e-6  Loss: 0.4118731903418702\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.3941253276720152e-6 tolerance: 1.0e-6  Loss: 0.41072138166128047\n",
      "====> In Gradient: Iteration: 220000 grad_size: 1.218131060683416e-6 tolerance: 1.0e-6  Loss: 0.41001216856888417\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.1103056638254638e-6 tolerance: 1.0e-6  Loss: 0.40960962429470127\n",
      "====> In Gradient: Iteration: 260000 grad_size: 5.685027244657287e-6 tolerance: 1.0e-6  Loss: 0.40189096164288074\n",
      "====> In Gradient: Iteration: 280000 grad_size: 3.957392851211417e-6 tolerance: 1.0e-6  Loss: 0.3943016373642647\n",
      "====> In Gradient: Iteration: 300000 grad_size: 3.6761122588232047e-6 tolerance: 1.0e-6  Loss: 0.3883135283932453\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.098967646012029e-5 tolerance: 1.0e-6  Loss: 0.3582057374647596\n",
      "====> In Gradient: Iteration: 340000 grad_size: 8.770897692571559e-6 tolerance: 1.0e-6  Loss: 0.31268408979442897\n",
      "====> In Gradient: Iteration: 360000 grad_size: 7.267434261104319e-6 tolerance: 1.0e-6  Loss: 0.2820069938272729\n",
      "====> In Gradient: Iteration: 380000 grad_size: 5.379081054426731e-6 tolerance: 1.0e-6  Loss: 0.2607062095698003\n",
      "====> In Gradient: Iteration: 400000 grad_size: 4.692148393333751e-6 tolerance: 1.0e-6  Loss: 0.2482933950835984\n",
      "====> In Gradient: Iteration: 420000 grad_size: 4.180660630928286e-6 tolerance: 1.0e-6  Loss: 0.23859800360457434\n",
      "====> In Gradient: Iteration: 440000 grad_size: 3.7740418169099877e-6 tolerance: 1.0e-6  Loss: 0.23077591441229447\n",
      "====> In Gradient: Iteration: 460000 grad_size: 5.705666817039333e-6 tolerance: 1.0e-6  Loss: 0.22432447114301224\n",
      "====> In Gradient: Iteration: 480000 grad_size: 3.1574980170498523e-6 tolerance: 1.0e-6  Loss: 0.21891371827701603\n",
      "====> In Gradient: Iteration: 500000 grad_size: 2.9193088812653895e-6 tolerance: 1.0e-6  Loss: 0.21431158669012934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.1206021038003461e-5 tolerance: 1.0e-6  Loss: 0.5593789799218846\n",
      "====> In Gradient: Iteration: 40000 grad_size: 7.161978262505845e-6 tolerance: 1.0e-6  Loss: 0.519259462232829\n",
      "====> In Gradient: Iteration: 60000 grad_size: 5.85056466433751e-6 tolerance: 1.0e-6  Loss: 0.5032756971546305\n",
      "====> In Gradient: Iteration: 80000 grad_size: 3.7033496131121205e-6 tolerance: 1.0e-6  Loss: 0.49490215215987604\n",
      "====> In Gradient: Iteration: 100000 grad_size: 3.1472218331919817e-6 tolerance: 1.0e-6  Loss: 0.48918205937069703\n",
      "====> In Gradient: Iteration: 120000 grad_size: 2.7496308919636654e-6 tolerance: 1.0e-6  Loss: 0.484946094709486\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.439635365603837e-6 tolerance: 1.0e-6  Loss: 0.48166111547075524\n",
      "====> In Gradient: Iteration: 160000 grad_size: 2.185367161335533e-6 tolerance: 1.0e-6  Loss: 0.47904396857702986\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.9712379168182366e-6 tolerance: 1.0e-6  Loss: 0.47692213752343693\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.788481865672625e-6 tolerance: 1.0e-6  Loss: 0.47517927430069606\n",
      "====> In Gradient: Iteration: 220000 grad_size: 1.6314159591798197e-6 tolerance: 1.0e-6  Loss: 0.47373190705423857\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.495855543454704e-6 tolerance: 1.0e-6  Loss: 0.4725177927810078\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.3784783459258522e-6 tolerance: 1.0e-6  Loss: 0.4714895475587582\n",
      "====> In Gradient: Iteration: 280000 grad_size: 1.7737121365340043e-6 tolerance: 1.0e-6  Loss: 0.4683063450667681\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.2094134955818845e-6 tolerance: 1.0e-6  Loss: 0.4672614299290983\n",
      "====> In Gradient: Iteration: 20000 grad_size: 1.4191504226144907e-5 tolerance: 1.0e-6  Loss: 0.7481971537841159\n",
      "====> In Gradient: Iteration: 40000 grad_size: 8.7662577071175e-6 tolerance: 1.0e-6  Loss: 0.7145315058789428\n",
      "====> In Gradient: Iteration: 60000 grad_size: 1.2092554791231502e-5 tolerance: 1.0e-6  Loss: 0.6969117072730007\n",
      "====> In Gradient: Iteration: 80000 grad_size: 6.396605987703125e-6 tolerance: 1.0e-6  Loss: 0.6853544025472406\n",
      "====> In Gradient: Iteration: 100000 grad_size: 5.621189631669416e-6 tolerance: 1.0e-6  Loss: 0.6775522679955169\n",
      "====> In Gradient: Iteration: 120000 grad_size: 1.2371638450603345e-5 tolerance: 1.0e-6  Loss: 0.6722170572912057\n",
      "====> In Gradient: Iteration: 140000 grad_size: 1.2445002710633198e-5 tolerance: 1.0e-6  Loss: 0.6689330866926855\n",
      "====> In Gradient: Iteration: 160000 grad_size: 6.003174699336501e-6 tolerance: 1.0e-6  Loss: 0.6670253746002682\n",
      "====> In Gradient: Iteration: 180000 grad_size: 1.2213995867510324e-5 tolerance: 1.0e-6  Loss: 0.665672711113143\n",
      "====> In Gradient: Iteration: 200000 grad_size: 3.8324011772381165e-6 tolerance: 1.0e-6  Loss: 0.664680997701503\n",
      "====> In Gradient: Iteration: 220000 grad_size: 1.7135702551498515e-5 tolerance: 1.0e-6  Loss: 0.6639378083007696\n",
      "====> In Gradient: Iteration: 240000 grad_size: 5.936971322465947e-6 tolerance: 1.0e-6  Loss: 0.6633640599897903\n",
      "====> In Gradient: Iteration: 260000 grad_size: 3.4480029308383844e-6 tolerance: 1.0e-6  Loss: 0.6629111138284556\n",
      "====> In Gradient: Iteration: 280000 grad_size: 3.3624579321044924e-6 tolerance: 1.0e-6  Loss: 0.6625427198503895\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.1772087962403585e-5 tolerance: 1.0e-6  Loss: 0.6622357010929328\n",
      "====> In Gradient: Iteration: 320000 grad_size: 3.237257658138745e-6 tolerance: 1.0e-6  Loss: 0.6619798222601232\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.0182145896918707e-5 tolerance: 1.0e-6  Loss: 0.6617566288672461\n",
      "====> In Gradient: Iteration: 360000 grad_size: 3.15215539713391e-6 tolerance: 1.0e-6  Loss: 0.6615511136094931\n",
      "====> In Gradient: Iteration: 380000 grad_size: 1.1885305053683501e-5 tolerance: 1.0e-6  Loss: 0.6613635465276216\n",
      "====> In Gradient: Iteration: 400000 grad_size: 1.0162054171957946e-5 tolerance: 1.0e-6  Loss: 0.661187567545799\n",
      "====> In Gradient: Iteration: 420000 grad_size: 3.055522172751457e-6 tolerance: 1.0e-6  Loss: 0.6610224573238452\n",
      "====> In Gradient: Iteration: 440000 grad_size: 3.029208640956065e-6 tolerance: 1.0e-6  Loss: 0.660866243166156\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.7440974473207845e-5 tolerance: 1.0e-6  Loss: 0.6607198386439787\n",
      "====> In Gradient: Iteration: 480000 grad_size: 8.37192370102256e-6 tolerance: 1.0e-6  Loss: 0.6605776526405005\n",
      "====> In Gradient: Iteration: 500000 grad_size: 1.2025696092718669e-5 tolerance: 1.0e-6  Loss: 0.6604430826782774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 8.172236335421923e-6 tolerance: 1.0e-6  Loss: 0.7749036329138819\n",
      "====> In Gradient: Iteration: 40000 grad_size: 9.53036461047883e-6 tolerance: 1.0e-6  Loss: 0.7440067299233635\n",
      "====> In Gradient: Iteration: 60000 grad_size: 8.121216528652968e-6 tolerance: 1.0e-6  Loss: 0.7189709761474645\n",
      "====> In Gradient: Iteration: 80000 grad_size: 8.085305782365179e-6 tolerance: 1.0e-6  Loss: 0.6939372335039133\n",
      "====> In Gradient: Iteration: 100000 grad_size: 5.6983012342883065e-6 tolerance: 1.0e-6  Loss: 0.6778208371754012\n",
      "====> In Gradient: Iteration: 120000 grad_size: 5.721305665528152e-6 tolerance: 1.0e-6  Loss: 0.6670321941147076\n",
      "====> In Gradient: Iteration: 140000 grad_size: 4.9314122319144685e-6 tolerance: 1.0e-6  Loss: 0.6598849980776929\n",
      "====> In Gradient: Iteration: 160000 grad_size: 4.623579442617216e-6 tolerance: 1.0e-6  Loss: 0.6551715942457935\n",
      "====> In Gradient: Iteration: 180000 grad_size: 3.983048521179002e-6 tolerance: 1.0e-6  Loss: 0.6517749017079242\n",
      "====> In Gradient: Iteration: 200000 grad_size: 5.412260363231224e-6 tolerance: 1.0e-6  Loss: 0.649152483598416\n",
      "====> In Gradient: Iteration: 220000 grad_size: 3.735480067130804e-6 tolerance: 1.0e-6  Loss: 0.6470784355256619\n",
      "====> In Gradient: Iteration: 240000 grad_size: 3.3309899555421816e-6 tolerance: 1.0e-6  Loss: 0.6454248736996687\n",
      "====> In Gradient: Iteration: 260000 grad_size: 3.165796526617384e-6 tolerance: 1.0e-6  Loss: 0.6440998518516998\n",
      "====> In Gradient: Iteration: 280000 grad_size: 3.0239857139096803e-6 tolerance: 1.0e-6  Loss: 0.6430341255696054\n",
      "====> In Gradient: Iteration: 300000 grad_size: 3.2353200747031275e-6 tolerance: 1.0e-6  Loss: 0.6421740331491084\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.799435138065309e-6 tolerance: 1.0e-6  Loss: 0.6414779322887095\n",
      "====> In Gradient: Iteration: 340000 grad_size: 6.017670968457353e-6 tolerance: 1.0e-6  Loss: 0.6409126802242194\n",
      "====> In Gradient: Iteration: 360000 grad_size: 3.0409071221100666e-6 tolerance: 1.0e-6  Loss: 0.6404515564524763\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.9976258412415987e-6 tolerance: 1.0e-6  Loss: 0.6400745632724586\n",
      "====> In Gradient: Iteration: 400000 grad_size: 2.520244034866244e-6 tolerance: 1.0e-6  Loss: 0.6397649093760263\n",
      "====> In Gradient: Iteration: 420000 grad_size: 2.4749018834812307e-6 tolerance: 1.0e-6  Loss: 0.6395094089620812\n",
      "====> In Gradient: Iteration: 440000 grad_size: 2.436503343597382e-6 tolerance: 1.0e-6  Loss: 0.6392975671153843\n",
      "====> In Gradient: Iteration: 460000 grad_size: 5.628798006264534e-6 tolerance: 1.0e-6  Loss: 0.6391212703827952\n",
      "====> In Gradient: Iteration: 480000 grad_size: 2.3765705743565132e-6 tolerance: 1.0e-6  Loss: 0.6389732181643804\n",
      "====> In Gradient: Iteration: 500000 grad_size: 2.8684578203737893e-6 tolerance: 1.0e-6  Loss: 0.6388482587605664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.2466319514471344e-5 tolerance: 1.0e-6  Loss: 0.39473260278908734\n",
      "====> In Gradient: Iteration: 40000 grad_size: 1.1802269243637534e-5 tolerance: 1.0e-6  Loss: 0.34834560965454087\n",
      "====> In Gradient: Iteration: 60000 grad_size: 3.4245654451242056e-6 tolerance: 1.0e-6  Loss: 0.3367777498559796\n",
      "====> In Gradient: Iteration: 80000 grad_size: 3.0394916688483345e-6 tolerance: 1.0e-6  Loss: 0.33476572664616994\n",
      "====> In Gradient: Iteration: 100000 grad_size: 2.8218345135785866e-6 tolerance: 1.0e-6  Loss: 0.33328930498376025\n",
      "====> In Gradient: Iteration: 120000 grad_size: 6.334674550890128e-6 tolerance: 1.0e-6  Loss: 0.3321903312548589\n",
      "====> In Gradient: Iteration: 140000 grad_size: 2.512488026949241e-6 tolerance: 1.0e-6  Loss: 0.3313704631223812\n",
      "====> In Gradient: Iteration: 160000 grad_size: 2.405099359145063e-6 tolerance: 1.0e-6  Loss: 0.33075939897051415\n",
      "====> In Gradient: Iteration: 180000 grad_size: 2.321182292869936e-6 tolerance: 1.0e-6  Loss: 0.3303035432570921\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.2559762558204124e-6 tolerance: 1.0e-6  Loss: 0.329962945803243\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.20550773386647e-6 tolerance: 1.0e-6  Loss: 0.3297086440545424\n",
      "====> In Gradient: Iteration: 240000 grad_size: 4.540711726900494e-6 tolerance: 1.0e-6  Loss: 0.32951848180750426\n",
      "====> In Gradient: Iteration: 260000 grad_size: 2.1366879724005867e-6 tolerance: 1.0e-6  Loss: 0.3293765321749828\n",
      "====> In Gradient: Iteration: 280000 grad_size: 2.113691381573345e-6 tolerance: 1.0e-6  Loss: 0.32927022211784374\n",
      "====> In Gradient: Iteration: 300000 grad_size: 4.5001139941265044e-6 tolerance: 1.0e-6  Loss: 0.32919099314179145\n",
      "====> In Gradient: Iteration: 320000 grad_size: 7.103922733749603e-6 tolerance: 1.0e-6  Loss: 0.329131542630681\n",
      "====> In Gradient: Iteration: 340000 grad_size: 2.0720587808516772e-6 tolerance: 1.0e-6  Loss: 0.3290871325151852\n",
      "====> In Gradient: Iteration: 360000 grad_size: 2.0640147182560452e-6 tolerance: 1.0e-6  Loss: 0.32905385161429257\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.0577939569052933e-6 tolerance: 1.0e-6  Loss: 0.3290289667147601\n",
      "====> In Gradient: Iteration: 400000 grad_size: 2.0529703457091855e-6 tolerance: 1.0e-6  Loss: 0.3290103103084501\n",
      "====> In Gradient: Iteration: 420000 grad_size: 2.049200386902696e-6 tolerance: 1.0e-6  Loss: 0.32899632045093913\n",
      "====> In Gradient: Iteration: 440000 grad_size: 9.099899636373784e-6 tolerance: 1.0e-6  Loss: 0.32898603839344187\n",
      "====> In Gradient: Iteration: 460000 grad_size: 4.47128830538553e-6 tolerance: 1.0e-6  Loss: 0.32897804132293584\n",
      "====> In Gradient: Iteration: 480000 grad_size: 2.042049346839415e-6 tolerance: 1.0e-6  Loss: 0.32897195815920083\n",
      "====> In Gradient: Iteration: 500000 grad_size: 2.0405862951242643e-6 tolerance: 1.0e-6  Loss: 0.3289674130619135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 1.0173789265067803e-5 tolerance: 1.0e-6  Loss: 0.48261927190871023\n",
      "====> In Gradient: Iteration: 40000 grad_size: 7.273902062058976e-6 tolerance: 1.0e-6  Loss: 0.44556040413101394\n",
      "====> In Gradient: Iteration: 60000 grad_size: 6.059285553306294e-6 tolerance: 1.0e-6  Loss: 0.4226208595955972\n",
      "====> In Gradient: Iteration: 80000 grad_size: 5.615977092187146e-6 tolerance: 1.0e-6  Loss: 0.40501627897327336\n",
      "====> In Gradient: Iteration: 100000 grad_size: 4.968381883656755e-6 tolerance: 1.0e-6  Loss: 0.3910698798976431\n",
      "====> In Gradient: Iteration: 120000 grad_size: 4.460317953162227e-6 tolerance: 1.0e-6  Loss: 0.379991574245289\n",
      "====> In Gradient: Iteration: 140000 grad_size: 3.8015031184693935e-6 tolerance: 1.0e-6  Loss: 0.37111659434447625\n",
      "====> In Gradient: Iteration: 160000 grad_size: 3.560034313542454e-6 tolerance: 1.0e-6  Loss: 0.3647297535720958\n",
      "====> In Gradient: Iteration: 180000 grad_size: 3.0304711814325408e-6 tolerance: 1.0e-6  Loss: 0.35964281048966246\n",
      "====> In Gradient: Iteration: 200000 grad_size: 2.7568899711168414e-6 tolerance: 1.0e-6  Loss: 0.3554850400682051\n",
      "====> In Gradient: Iteration: 220000 grad_size: 2.536764727954933e-6 tolerance: 1.0e-6  Loss: 0.35200303444637826\n",
      "====> In Gradient: Iteration: 240000 grad_size: 6.824250109192088e-6 tolerance: 1.0e-6  Loss: 0.3128829155219377\n",
      "====> In Gradient: Iteration: 260000 grad_size: 4.25691559686533e-6 tolerance: 1.0e-6  Loss: 0.2984205728763682\n",
      "====> In Gradient: Iteration: 280000 grad_size: 3.5972160809858578e-6 tolerance: 1.0e-6  Loss: 0.2902683313796805\n",
      "====> In Gradient: Iteration: 300000 grad_size: 2.9772513801464483e-6 tolerance: 1.0e-6  Loss: 0.28494649897937263\n",
      "====> In Gradient: Iteration: 320000 grad_size: 2.6126021041860984e-6 tolerance: 1.0e-6  Loss: 0.2810761103689614\n",
      "====> In Gradient: Iteration: 340000 grad_size: 2.3802272380365283e-6 tolerance: 1.0e-6  Loss: 0.2779778227457794\n",
      "====> In Gradient: Iteration: 360000 grad_size: 2.2218082236987555e-6 tolerance: 1.0e-6  Loss: 0.2753400008358838\n",
      "====> In Gradient: Iteration: 380000 grad_size: 2.106126352378362e-6 tolerance: 1.0e-6  Loss: 0.27300386818016825\n",
      "====> In Gradient: Iteration: 400000 grad_size: 2.0287941994508233e-6 tolerance: 1.0e-6  Loss: 0.2709424579733708\n",
      "====> In Gradient: Iteration: 420000 grad_size: 2.1344020208623325e-6 tolerance: 1.0e-6  Loss: 0.26906849146282896\n",
      "====> In Gradient: Iteration: 440000 grad_size: 1.932678838131434e-6 tolerance: 1.0e-6  Loss: 0.267323261033586\n",
      "====> In Gradient: Iteration: 460000 grad_size: 1.8908568470074454e-6 tolerance: 1.0e-6  Loss: 0.2656857499509758\n",
      "====> In Gradient: Iteration: 480000 grad_size: 1.8495514594809898e-6 tolerance: 1.0e-6  Loss: 0.26414402038714185\n",
      "====> In Gradient: Iteration: 500000 grad_size: 1.8084511879665192e-6 tolerance: 1.0e-6  Loss: 0.26269004495062886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Max num. iterations reached\n",
      "└ @ Main In[35]:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 2.1517990477338427e-5 tolerance: 1.0e-6  Loss: 0.8591282596264607\n",
      "====> In Gradient: Iteration: 40000 grad_size: 1.1599212864783404e-5 tolerance: 1.0e-6  Loss: 0.7470966009016627\n",
      "====> In Gradient: Iteration: 60000 grad_size: 4.302900064369176e-6 tolerance: 1.0e-6  Loss: 0.7192972972351399\n",
      "====> In Gradient: Iteration: 80000 grad_size: 4.013163098791934e-6 tolerance: 1.0e-6  Loss: 0.7158403078750727\n",
      "====> In Gradient: Iteration: 100000 grad_size: 3.857026096030813e-6 tolerance: 1.0e-6  Loss: 0.713317441936508\n",
      "====> In Gradient: Iteration: 120000 grad_size: 2.433338637725666e-6 tolerance: 1.0e-6  Loss: 0.7111366088585072\n",
      "====> In Gradient: Iteration: 140000 grad_size: 3.2249798776372383e-6 tolerance: 1.0e-6  Loss: 0.7092299380257345\n",
      "====> In Gradient: Iteration: 160000 grad_size: 2.182419492292065e-6 tolerance: 1.0e-6  Loss: 0.7075462086328055\n",
      "====> In Gradient: Iteration: 180000 grad_size: 2.0750017769670886e-6 tolerance: 1.0e-6  Loss: 0.7060464718291316\n",
      "====> In Gradient: Iteration: 200000 grad_size: 1.9769549527452344e-6 tolerance: 1.0e-6  Loss: 0.7047007215431217\n",
      "====> In Gradient: Iteration: 220000 grad_size: 3.0565413008676334e-6 tolerance: 1.0e-6  Loss: 0.7034855358376391\n",
      "====> In Gradient: Iteration: 240000 grad_size: 1.8034421415161669e-6 tolerance: 1.0e-6  Loss: 0.7023822911342145\n",
      "====> In Gradient: Iteration: 260000 grad_size: 1.7258875614319552e-6 tolerance: 1.0e-6  Loss: 0.7013763814905786\n",
      "====> In Gradient: Iteration: 280000 grad_size: 3.018005640374157e-6 tolerance: 1.0e-6  Loss: 0.7004558648985323\n",
      "====> In Gradient: Iteration: 300000 grad_size: 1.5853896034715013e-6 tolerance: 1.0e-6  Loss: 0.6996107022202357\n",
      "====> In Gradient: Iteration: 320000 grad_size: 1.5193441121276336e-6 tolerance: 1.0e-6  Loss: 0.6988660623131313\n",
      "====> In Gradient: Iteration: 340000 grad_size: 1.4574183540367368e-6 tolerance: 1.0e-6  Loss: 0.6981811965758663\n",
      "====> In Gradient: Iteration: 360000 grad_size: 8.290034219084884e-6 tolerance: 1.0e-6  Loss: 0.6975470132556196\n",
      "====> In Gradient: Iteration: 380000 grad_size: 1.344791297176303e-6 tolerance: 1.0e-6  Loss: 0.6969584576964463\n",
      "====> In Gradient: Iteration: 400000 grad_size: 1.2933200194097167e-6 tolerance: 1.0e-6  Loss: "
     ]
    }
   ],
   "source": [
    "plot_pairs = experiment_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip580\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip580)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip581\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip580)\" d=\"\n",
       "M154.515 1486.45 L2352.76 1486.45 L2352.76 47.2441 L154.515 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip582\">\n",
       "    <rect x=\"154\" y=\"47\" width=\"2199\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  528.072,1486.45 528.072,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  968.69,1486.45 968.69,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1409.31,1486.45 1409.31,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1849.92,1486.45 1849.92,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2290.54,1486.45 2290.54,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  154.515,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  528.072,1486.45 528.072,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  968.69,1486.45 968.69,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1409.31,1486.45 1409.31,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1849.92,1486.45 1849.92,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2290.54,1486.45 2290.54,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip580)\" d=\"M505.376 1517.37 Q501.765 1517.37 499.936 1520.93 Q498.13 1524.47 498.13 1531.6 Q498.13 1538.71 499.936 1542.27 Q501.765 1545.82 505.376 1545.82 Q509.01 1545.82 510.815 1542.27 Q512.644 1538.71 512.644 1531.6 Q512.644 1524.47 510.815 1520.93 Q509.01 1517.37 505.376 1517.37 M505.376 1513.66 Q511.186 1513.66 514.241 1518.27 Q517.32 1522.85 517.32 1531.6 Q517.32 1540.33 514.241 1544.94 Q511.186 1549.52 505.376 1549.52 Q499.566 1549.52 496.487 1544.94 Q493.431 1540.33 493.431 1531.6 Q493.431 1522.85 496.487 1518.27 Q499.566 1513.66 505.376 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M525.538 1542.97 L530.422 1542.97 L530.422 1548.85 L525.538 1548.85 L525.538 1542.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M551.186 1529.7 Q548.037 1529.7 546.186 1531.86 Q544.357 1534.01 544.357 1537.76 Q544.357 1541.49 546.186 1543.66 Q548.037 1545.82 551.186 1545.82 Q554.334 1545.82 556.162 1543.66 Q558.014 1541.49 558.014 1537.76 Q558.014 1534.01 556.162 1531.86 Q554.334 1529.7 551.186 1529.7 M560.468 1515.05 L560.468 1519.31 Q558.709 1518.48 556.903 1518.04 Q555.121 1517.6 553.362 1517.6 Q548.732 1517.6 546.278 1520.72 Q543.848 1523.85 543.5 1530.17 Q544.866 1528.15 546.926 1527.09 Q548.987 1526 551.463 1526 Q556.672 1526 559.681 1529.17 Q562.713 1532.32 562.713 1537.76 Q562.713 1543.08 559.565 1546.3 Q556.417 1549.52 551.186 1549.52 Q545.19 1549.52 542.019 1544.94 Q538.848 1540.33 538.848 1531.6 Q538.848 1523.41 542.737 1518.55 Q546.625 1513.66 553.176 1513.66 Q554.936 1513.66 556.718 1514.01 Q558.524 1514.36 560.468 1515.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M946.525 1517.37 Q942.914 1517.37 941.086 1520.93 Q939.28 1524.47 939.28 1531.6 Q939.28 1538.71 941.086 1542.27 Q942.914 1545.82 946.525 1545.82 Q950.16 1545.82 951.965 1542.27 Q953.794 1538.71 953.794 1531.6 Q953.794 1524.47 951.965 1520.93 Q950.16 1517.37 946.525 1517.37 M946.525 1513.66 Q952.336 1513.66 955.391 1518.27 Q958.47 1522.85 958.47 1531.6 Q958.47 1540.33 955.391 1544.94 Q952.336 1549.52 946.525 1549.52 Q940.715 1549.52 937.637 1544.94 Q934.581 1540.33 934.581 1531.6 Q934.581 1522.85 937.637 1518.27 Q940.715 1513.66 946.525 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M966.687 1542.97 L971.572 1542.97 L971.572 1548.85 L966.687 1548.85 L966.687 1542.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M980.576 1514.29 L1002.8 1514.29 L1002.8 1516.28 L990.252 1548.85 L985.368 1548.85 L997.173 1518.22 L980.576 1518.22 L980.576 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1386.74 1517.37 Q1383.13 1517.37 1381.3 1520.93 Q1379.49 1524.47 1379.49 1531.6 Q1379.49 1538.71 1381.3 1542.27 Q1383.13 1545.82 1386.74 1545.82 Q1390.37 1545.82 1392.18 1542.27 Q1394.01 1538.71 1394.01 1531.6 Q1394.01 1524.47 1392.18 1520.93 Q1390.37 1517.37 1386.74 1517.37 M1386.74 1513.66 Q1392.55 1513.66 1395.6 1518.27 Q1398.68 1522.85 1398.68 1531.6 Q1398.68 1540.33 1395.6 1544.94 Q1392.55 1549.52 1386.74 1549.52 Q1380.93 1549.52 1377.85 1544.94 Q1374.79 1540.33 1374.79 1531.6 Q1374.79 1522.85 1377.85 1518.27 Q1380.93 1513.66 1386.74 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1406.9 1542.97 L1411.78 1542.97 L1411.78 1548.85 L1406.9 1548.85 L1406.9 1542.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1431.97 1532.44 Q1428.64 1532.44 1426.71 1534.22 Q1424.82 1536 1424.82 1539.13 Q1424.82 1542.25 1426.71 1544.03 Q1428.64 1545.82 1431.97 1545.82 Q1435.3 1545.82 1437.22 1544.03 Q1439.14 1542.23 1439.14 1539.13 Q1439.14 1536 1437.22 1534.22 Q1435.33 1532.44 1431.97 1532.44 M1427.29 1530.45 Q1424.28 1529.7 1422.59 1527.64 Q1420.93 1525.58 1420.93 1522.62 Q1420.93 1518.48 1423.87 1516.07 Q1426.83 1513.66 1431.97 1513.66 Q1437.13 1513.66 1440.07 1516.07 Q1443.01 1518.48 1443.01 1522.62 Q1443.01 1525.58 1441.32 1527.64 Q1439.65 1529.7 1436.67 1530.45 Q1440.05 1531.23 1441.92 1533.52 Q1443.82 1535.82 1443.82 1539.13 Q1443.82 1544.15 1440.74 1546.83 Q1437.69 1549.52 1431.97 1549.52 Q1426.25 1549.52 1423.17 1546.83 Q1420.12 1544.15 1420.12 1539.13 Q1420.12 1535.82 1422.02 1533.52 Q1423.91 1531.23 1427.29 1530.45 M1425.58 1523.06 Q1425.58 1525.75 1427.25 1527.25 Q1428.94 1528.76 1431.97 1528.76 Q1434.98 1528.76 1436.67 1527.25 Q1438.38 1525.75 1438.38 1523.06 Q1438.38 1520.38 1436.67 1518.87 Q1434.98 1517.37 1431.97 1517.37 Q1428.94 1517.37 1427.25 1518.87 Q1425.58 1520.38 1425.58 1523.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1827.4 1517.37 Q1823.79 1517.37 1821.96 1520.93 Q1820.16 1524.47 1820.16 1531.6 Q1820.16 1538.71 1821.96 1542.27 Q1823.79 1545.82 1827.4 1545.82 Q1831.04 1545.82 1832.84 1542.27 Q1834.67 1538.71 1834.67 1531.6 Q1834.67 1524.47 1832.84 1520.93 Q1831.04 1517.37 1827.4 1517.37 M1827.4 1513.66 Q1833.21 1513.66 1836.27 1518.27 Q1839.35 1522.85 1839.35 1531.6 Q1839.35 1540.33 1836.27 1544.94 Q1833.21 1549.52 1827.4 1549.52 Q1821.59 1549.52 1818.51 1544.94 Q1815.46 1540.33 1815.46 1531.6 Q1815.46 1522.85 1818.51 1518.27 Q1821.59 1513.66 1827.4 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1847.56 1542.97 L1852.45 1542.97 L1852.45 1548.85 L1847.56 1548.85 L1847.56 1542.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M1862.77 1548.13 L1862.77 1543.87 Q1864.53 1544.7 1866.34 1545.14 Q1868.14 1545.58 1869.88 1545.58 Q1874.51 1545.58 1876.94 1542.48 Q1879.39 1539.36 1879.74 1533.01 Q1878.4 1535.01 1876.34 1536.07 Q1874.28 1537.13 1871.78 1537.13 Q1866.59 1537.13 1863.56 1534.01 Q1860.55 1530.86 1860.55 1525.42 Q1860.55 1520.1 1863.7 1516.88 Q1866.85 1513.66 1872.08 1513.66 Q1878.07 1513.66 1881.22 1518.27 Q1884.39 1522.85 1884.39 1531.6 Q1884.39 1539.77 1880.5 1544.66 Q1876.64 1549.52 1870.09 1549.52 Q1868.33 1549.52 1866.52 1549.17 Q1864.72 1548.82 1862.77 1548.13 M1872.08 1533.48 Q1875.23 1533.48 1877.05 1531.32 Q1878.91 1529.17 1878.91 1525.42 Q1878.91 1521.7 1877.05 1519.54 Q1875.23 1517.37 1872.08 1517.37 Q1868.93 1517.37 1867.08 1519.54 Q1865.25 1521.7 1865.25 1525.42 Q1865.25 1529.17 1867.08 1531.32 Q1868.93 1533.48 1872.08 1533.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M2257.69 1544.91 L2265.33 1544.91 L2265.33 1518.55 L2257.02 1520.21 L2257.02 1515.95 L2265.29 1514.29 L2269.96 1514.29 L2269.96 1544.91 L2277.6 1544.91 L2277.6 1548.85 L2257.69 1548.85 L2257.69 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M2287.05 1542.97 L2291.93 1542.97 L2291.93 1548.85 L2287.05 1548.85 L2287.05 1542.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M2312.12 1517.37 Q2308.5 1517.37 2306.68 1520.93 Q2304.87 1524.47 2304.87 1531.6 Q2304.87 1538.71 2306.68 1542.27 Q2308.5 1545.82 2312.12 1545.82 Q2315.75 1545.82 2317.56 1542.27 Q2319.38 1538.71 2319.38 1531.6 Q2319.38 1524.47 2317.56 1520.93 Q2315.75 1517.37 2312.12 1517.37 M2312.12 1513.66 Q2317.93 1513.66 2320.98 1518.27 Q2324.06 1522.85 2324.06 1531.6 Q2324.06 1540.33 2320.98 1544.94 Q2317.93 1549.52 2312.12 1549.52 Q2306.31 1549.52 2303.23 1544.94 Q2300.17 1540.33 2300.17 1531.6 Q2300.17 1522.85 2303.23 1518.27 Q2306.31 1513.66 2312.12 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  154.515,1290.09 2352.76,1290.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  154.515,1011.02 2352.76,1011.02 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  154.515,731.956 2352.76,731.956 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  154.515,452.89 2352.76,452.89 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip582)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  154.515,173.824 2352.76,173.824 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  154.515,1486.45 154.515,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  154.515,1290.09 173.413,1290.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  154.515,1011.02 173.413,1011.02 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  154.515,731.956 173.413,731.956 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  154.515,452.89 173.413,452.89 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  154.515,173.824 173.413,173.824 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip580)\" d=\"M52.1495 1303.43 L59.7884 1303.43 L59.7884 1277.07 L51.4782 1278.73 L51.4782 1274.48 L59.7421 1272.81 L64.418 1272.81 L64.418 1303.43 L72.0568 1303.43 L72.0568 1307.37 L52.1495 1307.37 L52.1495 1303.43 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M81.5012 1301.49 L86.3855 1301.49 L86.3855 1307.37 L81.5012 1307.37 L81.5012 1301.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M106.571 1275.89 Q102.959 1275.89 101.131 1279.45 Q99.3252 1282.99 99.3252 1290.12 Q99.3252 1297.23 101.131 1300.79 Q102.959 1304.34 106.571 1304.34 Q110.205 1304.34 112.01 1300.79 Q113.839 1297.23 113.839 1290.12 Q113.839 1282.99 112.01 1279.45 Q110.205 1275.89 106.571 1275.89 M106.571 1272.18 Q112.381 1272.18 115.436 1276.79 Q118.515 1281.37 118.515 1290.12 Q118.515 1298.85 115.436 1303.46 Q112.381 1308.04 106.571 1308.04 Q100.76 1308.04 97.6817 1303.46 Q94.6262 1298.85 94.6262 1290.12 Q94.6262 1281.37 97.6817 1276.79 Q100.76 1272.18 106.571 1272.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M53.3764 1024.37 L61.0152 1024.37 L61.0152 998.002 L52.7051 999.668 L52.7051 995.409 L60.9689 993.742 L65.6448 993.742 L65.6448 1024.37 L73.2837 1024.37 L73.2837 1028.3 L53.3764 1028.3 L53.3764 1024.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M82.7281 1022.42 L87.6123 1022.42 L87.6123 1028.3 L82.7281 1028.3 L82.7281 1022.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M98.6076 1024.37 L106.246 1024.37 L106.246 998.002 L97.9363 999.668 L97.9363 995.409 L106.2 993.742 L110.876 993.742 L110.876 1024.37 L118.515 1024.37 L118.515 1028.3 L98.6076 1028.3 L98.6076 1024.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M53.7467 745.301 L61.3856 745.301 L61.3856 718.935 L53.0754 720.602 L53.0754 716.343 L61.3393 714.676 L66.0152 714.676 L66.0152 745.301 L73.654 745.301 L73.654 749.236 L53.7467 749.236 L53.7467 745.301 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M83.0984 743.357 L87.9827 743.357 L87.9827 749.236 L83.0984 749.236 L83.0984 743.357 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M102.196 745.301 L118.515 745.301 L118.515 749.236 L96.5706 749.236 L96.5706 745.301 Q99.2326 742.546 103.816 737.917 Q108.422 733.264 109.603 731.921 Q111.848 729.398 112.728 727.662 Q113.631 725.903 113.631 724.213 Q113.631 721.459 111.686 719.722 Q109.765 717.986 106.663 717.986 Q104.464 717.986 102.01 718.75 Q99.5798 719.514 96.8021 721.065 L96.8021 716.343 Q99.6261 715.209 102.08 714.63 Q104.534 714.051 106.571 714.051 Q111.941 714.051 115.135 716.736 Q118.33 719.422 118.33 723.912 Q118.33 726.042 117.52 727.963 Q116.733 729.861 114.626 732.454 Q114.047 733.125 110.946 736.343 Q107.844 739.537 102.196 745.301 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M52.7977 466.235 L60.4365 466.235 L60.4365 439.869 L52.1264 441.536 L52.1264 437.277 L60.3902 435.61 L65.0661 435.61 L65.0661 466.235 L72.705 466.235 L72.705 470.17 L52.7977 470.17 L52.7977 466.235 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M82.1494 464.291 L87.0336 464.291 L87.0336 470.17 L82.1494 470.17 L82.1494 464.291 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M111.385 451.536 Q114.742 452.254 116.617 454.522 Q118.515 456.791 118.515 460.124 Q118.515 465.24 114.996 468.041 Q111.478 470.841 104.996 470.841 Q102.821 470.841 100.506 470.402 Q98.2141 469.985 95.7604 469.128 L95.7604 464.615 Q97.7048 465.749 100.02 466.328 Q102.334 466.906 104.858 466.906 Q109.256 466.906 111.547 465.17 Q113.862 463.434 113.862 460.124 Q113.862 457.068 111.709 455.355 Q109.58 453.619 105.76 453.619 L101.733 453.619 L101.733 449.777 L105.946 449.777 Q109.395 449.777 111.223 448.411 Q113.052 447.022 113.052 444.43 Q113.052 441.767 111.154 440.355 Q109.279 438.92 105.76 438.92 Q103.839 438.92 101.64 439.337 Q99.4409 439.754 96.8021 440.633 L96.8021 436.467 Q99.4641 435.726 101.779 435.355 Q104.117 434.985 106.177 434.985 Q111.501 434.985 114.603 437.416 Q117.705 439.823 117.705 443.943 Q117.705 446.814 116.061 448.804 Q114.418 450.772 111.385 451.536 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M51.6634 187.169 L59.3023 187.169 L59.3023 160.803 L50.9921 162.47 L50.9921 158.211 L59.256 156.544 L63.9319 156.544 L63.9319 187.169 L71.5707 187.169 L71.5707 191.104 L51.6634 191.104 L51.6634 187.169 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M81.0151 185.224 L85.8993 185.224 L85.8993 191.104 L81.0151 191.104 L81.0151 185.224 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M108.932 160.618 L97.1261 179.067 L108.932 179.067 L108.932 160.618 M107.705 156.544 L113.584 156.544 L113.584 179.067 L118.515 179.067 L118.515 182.956 L113.584 182.956 L113.584 191.104 L108.932 191.104 L108.932 182.956 L93.3299 182.956 L93.3299 178.442 L107.705 156.544 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip582)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  216.729,1052.27 1340.83,87.9763 2290.54,1445.72 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip580)\" d=\"\n",
       "M1982.8 198.898 L2279.48 198.898 L2279.48 95.2176 L1982.8 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1982.8,198.898 2279.48,198.898 2279.48,95.2176 1982.8,95.2176 1982.8,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip580)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2007.23,147.058 2153.78,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip580)\" d=\"M2192.04 166.745 Q2190.24 171.375 2188.53 172.787 Q2186.81 174.199 2183.94 174.199 L2180.54 174.199 L2180.54 170.634 L2183.04 170.634 Q2184.8 170.634 2185.77 169.8 Q2186.74 168.967 2187.92 165.865 L2188.69 163.921 L2178.2 138.412 L2182.71 138.412 L2190.82 158.689 L2198.92 138.412 L2203.43 138.412 L2192.04 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip580)\" d=\"M2210.72 160.402 L2218.36 160.402 L2218.36 134.037 L2210.05 135.703 L2210.05 131.444 L2218.32 129.778 L2222.99 129.778 L2222.99 160.402 L2230.63 160.402 L2230.63 164.338 L2210.72 164.338 L2210.72 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot([(plot_pair[1], mean(plot_pair[2])) for plot_pair in plot_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_fourier_meas_fn(m,n)\n",
    "    true_m, meas_mat = sample_fourier(m, n)\n",
    "    meas_mat = sqrt(n/true_m).*meas_mat\n",
    "    return (meas_mat, x -> meas_mat*x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "mid = 1\n",
    "n = 1\n",
    "aimed_m = 1\n",
    "num_signals = 2\n",
    "meas_fn = get_fourier_meas_fn(aimed_m, n)[2]\n",
    "\n",
    "β_array = [1.0]\n",
    "plot_pairs = []\n",
    "signal_errors = Array{Float64}(undef,num_signals)\n",
    "#model, alignment = get_aligned_models(k, mid, n, [0,])[1]\n",
    "model = get_rand_model(k,mid,n)\n",
    "x₀ = randn(n)/sqrt(n)\n",
    "measurements = meas_fn(x₀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = meas_fn(x₀)\n",
    "println(measurements)\n",
    "x̂ = recover_signal(measurements, meas_fn, model, k)\n",
    "println(meas_fn(x̂))\n",
    "println(err,\"   \", norm(x₀))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "mid = 30\n",
    "n = 100\n",
    "aimed_m = 20\n",
    "num_signals = 2\n",
    "meas_fn = get_fourier_meas_fn(aimed_m, n)\n",
    "β_array = 0:0.1:1\n",
    "plot_pairs = []\n",
    "signal_errors = Array{Float64}(undef,num_signals)\n",
    "for (model, alignment) in get_aligned_models(k, mid,n, β_array)\n",
    "    for i in 1:num_signals\n",
    "        x₀ = randn(n)/sqrt(n)\n",
    "        signal_error[i] = recovery_error(x₀, model, meas_fn, k)\n",
    "    push!(plot_pairs, (alignment, mean(signal_errors)))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Array{Int}(undef,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "n = 100\n",
    "A = randn(n, k)\n",
    "F = dct(diagm(ones(n)),2)\n",
    "matrix_alignment(A, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_alignments()\n",
    "    n= 500\n",
    "    alignments = []\n",
    "    normal_one = []\n",
    "    F = dct(diagm(ones(n)),2)\n",
    "    for k in 1:2:100\n",
    "        A = randn(n, k)\n",
    "        push!(alignments, matrix_alignment(A, F))\n",
    "        push!(normal_one, sqrt(k/n))\n",
    "    end\n",
    "    plot(alignments)\n",
    "    plot!(normal_one*0.9)\n",
    "    xlabel!\n",
    "    \n",
    "end\n",
    "plot_alignments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "n = 100\n",
    "A = randn(n, k)\n",
    "F = dct(diagm(ones(n)),2)\n",
    "get_aligned_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in eachcol(A)\n",
    "    println(i)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = dct(diagm(ones(n)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(Array,0:0.1:1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "mid = 100\n",
    "n = 400\n",
    "aimed_m = 23\n",
    "num_meas, measure_mat = sample_fourier(aimed_m, n)\n",
    "measure(x) = measure_mat*x\n",
    "random_model = get_rand_model(k,mid,n)\n",
    "#signal in range\n",
    "true_signal = random_model(rand(Float64, k))\n",
    "measurements = measure(true_signal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@time recovered_signal = recover_signal(measurements, measure, random_model, k)\n",
    "#println(\"The distance between signals is \", norm(recovered_signal - true_signal))\n",
    "#println(\"The size of the true signal is \", norm(true_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function GD_CS(G, y, A, max_iter, stepsize, tolerance, out_toggle)\n",
    "    _, k = size(Flux.params(G)[1])\n",
    "    z = randn(k)\n",
    "    iter = 1\n",
    "    succ_error = 1\n",
    "    d(z) = gradient(z -> norm(y - A*G(z))^2, z)[1]\n",
    "\n",
    "    while iter <= max_iter && succ_error > tolerance\n",
    "        # d gives the PLUGIn direction\n",
    "        z -= stepsize * d(z)\n",
    "        succ_error = norm(stepsize * d(z))\n",
    "        if iter % out_toggle == 0  \n",
    "            println(\"====> In Gradient: Iteration: $iter Successive error: $succ_error\")\n",
    "        end\n",
    "        iter += 1\n",
    "    end\n",
    "    return z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_aligned_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next experiment, we consider a one hidden layer network of the form $G(z) = A_2σ(A_1z)$. The inner layer matrix $A_1 \\in \\mathbb{R}^{n_1\\times k}$ has $\\mathcal{N}(0,1/n_1)$ entries. For the outer layer matrix $A_2 \\in \\mathbb{R}^{n_2\\times n_1}$ ($n_2 = n$), we take a matrix $A_{21}$ which contains columns randomly subsampled (without replacement) from a $n\\times n$ DCT matrix and a random matrix $A_{22}$ which contains $\\mathcal{N}(0,1/n_2)$ entries and take its convex combination, i.e. \n",
    "$$A_2 = \\beta A_{21} + (1-\\beta) A_{22}$$\n",
    "for $\\beta \\in [0,1]$.\n",
    "\n",
    "For each $\\beta \\in [0.7,1]$, we attempt to recover the code vector $z_0$ from measurement of the form $y = A G(z_0)$ where $A \\in \\mathbb{R}^{m\\times n}$ is subsampled DCT matrix (without replacement) by solve the least squares optimzation problem using the gradient descent algorithm. We show the results from 20 trials for each $\\beta$ and for all trials, $z_0, A_1, A_{21}$, and $A_{22}$ are fixed. For all experiments, $k = 20, n = 400$, $n_1$ is approximately 100, and $m$ is approximately 160."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup a synthetic problem\n",
    "x_dim = 400\n",
    "k = 20\n",
    "\n",
    "# F = real(fft(diagm(ones(x_dim)),2))\n",
    "F = dct(diagm(ones(x_dim)),2)\n",
    "\n",
    "# sampling rows of DCT matrix for outer layer\n",
    "sampling = rand(Bernoulli(100/x_dim), x_dim)\n",
    "sampling = convert.(Int, sampling)\n",
    "ndm1 = sum(sampling) #number of neurons n_1\n",
    "nnzindex = []\n",
    "for i in 1:length(sampling)\n",
    "    if sampling[i] != 0\n",
    "        push!(nnzindex, i)\n",
    "    end\n",
    "end\n",
    "\n",
    "A = F[nnzindex,:]' # subsampled DCT\n",
    "B = randn(x_dim, ndm1)/sqrt(x_dim) #random matrix thats used in convex combination\n",
    "\n",
    "W1 = randn(ndm1, 20)/sqrt(ndm1) #random inner layer weight matrix\n",
    "\n",
    "I(x) = x\n",
    "z = randn(k) # ground code vector\n",
    "\n",
    "β_list = .7:.01:1\n",
    "trials = 20\n",
    "recovery_error_matrix = zeros(length(β_list))\n",
    "recon_error_matrix = zeros(length(β_list))\n",
    "α_list = []\n",
    "\n",
    "for trial in 1:trials\n",
    "    recovery_error_list = []\n",
    "    recon_error_list = []   \n",
    "    α_list = []\n",
    "    for β in β_list\n",
    "        Aint = β*A + (1-β) * B\n",
    "\n",
    "        #setup the generative network \n",
    "        G = Chain(\n",
    "            Dense(20, ndm1, relu, bias = false; init =(out,in) -> W1),\n",
    "            Dense(ndm1, x_dim, I, bias = false; init =(out,in) -> Aint)\n",
    "        )\n",
    "\n",
    "        # subsampling DCT for measurement matrix\n",
    "        sampling = rand(Bernoulli(.4), x_dim)\n",
    "        sampling = convert.(Int, sampling)\n",
    "        m = sum(sampling)\n",
    "        nnzindex = []\n",
    "        for i in 1:length(sampling)\n",
    "            if sampling[i] != 0\n",
    "                push!(nnzindex, i)\n",
    "            end\n",
    "        end\n",
    "        F_sub = F[nnzindex,:]*sqrt(x_dim)/sqrt(m)\n",
    "\n",
    "        # measurement vector using subsampled DCT matrix\n",
    "        y = F_sub*G(z)\n",
    "\n",
    "        stepsize = .5\n",
    "        tolerance = 1e-14\n",
    "        max_iter = 2000\n",
    "        out_toggle = 2001\n",
    "\n",
    "        # run gradient descent to solve the least squares problem\n",
    "        z_rec = GD_CS(G,y,F_sub, max_iter, stepsize, tolerance, out_toggle)\n",
    "        recov_error = norm(z - real(z_rec))/norm(z)\n",
    "        recon_error = norm(G(z) - G(real(z_rec)))/norm(G(z))\n",
    "        Fn = F  \n",
    "        α = maximum(sum((Fn*Aint)' .* (Fn*Aint)', dims = 1) ./ sqrt.(sum((Fn*Aint*Aint')' .* (Fn*Aint*Aint')', dims = 1)))\n",
    "\n",
    "        # save results\n",
    "        push!(recovery_error_list, recov_error)\n",
    "        push!(recon_error_list, recon_error)\n",
    "        push!(α_list, α)\n",
    "\n",
    "    end\n",
    "    #save results\n",
    "    recovery_error_matrix =  hcat(recovery_error_matrix, recovery_error_list)\n",
    "    recon_error_matrix = hcat(recon_error_matrix, recon_error_list)\n",
    "\n",
    "\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.5",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
