{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is a library of functions for numerics of the incoherence simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using JLD.@load in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "using BSON: @load\n",
    "using Flux\n",
    "using Flux.Optimise\n",
    "using Flux.Optimise: update!\n",
    "using Flux.Data: DataLoader\n",
    "using ImageFiltering\n",
    "using Images\n",
    "using ImageIO\n",
    "using MLDatasets: FashionMNIST\n",
    "using LinearAlgebra\n",
    "using MLDatasets\n",
    "using Plots\n",
    "using Zygote\n",
    "using FFTW\n",
    "using Distributions\n",
    "using SparseArrays\n",
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimise"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    optimise(init_z, loss, opt, tolerance, [out_toggle = 0,][max_iter = 1_000_000])\n",
    "\n",
    "    Optimization that stops when the gradient is small enough\n",
    "\"\"\"\n",
    "function optimise(opt, loss; init, tolerance, out_toggle = 0, max_iter = 1_000_000)\n",
    "    tol2 = tolerance^2\n",
    "    z = init\n",
    "    ps = Flux.params(z)\n",
    "    iter=1\n",
    "    succ_error = 1.0\n",
    "    while succ_error > tol2 && (iter <= max_iter || @warn \"Max num. iterations reached\")\n",
    "        grads = gradient(() -> loss(z), ps)\n",
    "        update!(opt, z, grads[z])\n",
    "        succ_error = sum(abs2, grads[z])\n",
    "        if out_toggle != 0 && iter % out_toggle == 0\n",
    "            println(\"====> In Gradient: Iteration: $iter grad_size: $(sqrt(succ_error)) tolerance: $tolerance  Loss: \", string(loss(z)))\n",
    "        end\n",
    "        iter += 1\n",
    "    end\n",
    "    return z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "println(\"$(sqrt(2))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_rand_model (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_rand_model(k,mid,n)\n",
    "    W₂ = rand(Float64, (n, mid))\n",
    "    W₁ = rand(Float64, (mid,k))\n",
    "    return x -> W₂*relu.(W₁*x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_fourier (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sample_fourier(aimed_m, n)\n",
    "    F = dct(diagm(ones(n)),2)\n",
    "    sampling = rand(Bernoulli(aimed_m/n), n)\n",
    "    true_m = sum(sampling)\n",
    "    return (true_m, F[sampling,:])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recover_signal (generic function with 1 method)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function recover_signal(measurements, measure, model, code_dim)\n",
    "    function loss(signal_guess)\n",
    "        return sum(abs2, measure(model(signal_guess)) - measurements)\n",
    "    end\n",
    "    \n",
    "    return model(optimise( Flux.Optimise.Descent(1.2e-4), loss, init=rand(code_dim), tolerance=1e-10, out_toggle=20000))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "mid = 100\n",
    "n = 400\n",
    "aimed_m = 23\n",
    "num_meas, measure_mat = sample_fourier(aimed_m, n)\n",
    "measure(x) = measure_mat*x\n",
    "random_model = get_rand_model(k,mid,n)\n",
    "#signal in range\n",
    "true_signal = random_model(rand(Float64, k))\n",
    "measurements = measure(true_signal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> In Gradient: Iteration: 20000 grad_size: 5.486575571612964e-6 tolerance: 1.0e-10  Loss: 0.0009341807618805881\n",
      "====> In Gradient: Iteration: 40000 grad_size: 3.752344290413065e-7 tolerance: 1.0e-10  Loss: 4.375763743644386e-6\n",
      "====> In Gradient: Iteration: 60000 grad_size: 2.5704813596449484e-8 tolerance: 1.0e-10  Loss: 2.0534732135462785e-8\n",
      "====> In Gradient: Iteration: 80000 grad_size: 1.7609202211758725e-9 tolerance: 1.0e-10  Loss: 9.636952319295716e-11\n",
      "====> In Gradient: Iteration: 100000 grad_size: 1.2063273262357786e-10 tolerance: 1.0e-10  Loss: 4.522626202637418e-13\n",
      " 11.216442 seconds (27.45 M allocations: 42.674 GiB, 6.97% gc time, 3.11% compilation time)\n",
      "The distance between signals is 1.7822367146940335e-5\n",
      "The size of the true signal is 5557.832420673661\n"
     ]
    }
   ],
   "source": [
    "#@time recovered_signal = recover_signal(measurements, measure, random_model, k)\n",
    "#println(\"The distance between signals is \", norm(recovered_signal - true_signal))\n",
    "#println(\"The size of the true signal is \", norm(true_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_Fourier (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function GD_CS(G, y, A, max_iter, stepsize, tolerance, out_toggle)\n",
    "    _, k = size(Flux.params(G)[1])\n",
    "    z = randn(k)\n",
    "    iter = 1\n",
    "    succ_error = 1\n",
    "    d(z) = gradient(z -> norm(y - A*G(z))^2, z)[1]\n",
    "\n",
    "    while iter <= max_iter && succ_error > tolerance\n",
    "        # d gives the PLUGIn direction\n",
    "        z -= stepsize * d(z)\n",
    "        succ_error = norm(stepsize * d(z))\n",
    "        if iter % out_toggle == 0  \n",
    "            println(\"====> In Gradient: Iteration: $iter Successive error: $succ_error\")\n",
    "        end\n",
    "        iter += 1\n",
    "    end\n",
    "    return z\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next experiment, we consider a one hidden layer network of the form $G(z) = A_2σ(A_1z)$. The inner layer matrix $A_1 \\in \\mathbb{R}^{n_1\\times k}$ has $\\mathcal{N}(0,1/n_1)$ entries. For the outer layer matrix $A_2 \\in \\mathbb{R}^{n_2\\times n_1}$ ($n_2 = n$), we take a matrix $A_{21}$ which contains columns randomly subsampled (without replacement) from a $n\\times n$ DCT matrix and a random matrix $A_{22}$ which contains $\\mathcal{N}(0,1/n_2)$ entries and take its convex combination, i.e. \n",
    "$$A_2 = \\beta A_{21} + (1-\\beta) A_{22}$$\n",
    "for $\\beta \\in [0,1]$.\n",
    "\n",
    "For each $\\beta \\in [0.7,1]$, we attempt to recover the code vector $z_0$ from measurement of the form $y = A G(z_0)$ where $A \\in \\mathbb{R}^{m\\times n}$ is subsampled DCT matrix (without replacement) by solve the least squares optimzation problem using the gradient descent algorithm. We show the results from 20 trials for each $\\beta$ and for all trials, $z_0, A_1, A_{21}$, and $A_{22}$ are fixed. For all experiments, $k = 20, n = 400$, $n_1$ is approximately 100, and $m$ is approximately 160."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #setup a synthetic problem\n",
    "# x_dim = 400\n",
    "# k = 20\n",
    "\n",
    "# # F = real(fft(diagm(ones(x_dim)),2))\n",
    "# F = dct(diagm(ones(x_dim)),2)\n",
    "\n",
    "# # sampling rows of DCT matrix for outer layer\n",
    "# sampling = rand(Bernoulli(100/x_dim), x_dim)\n",
    "# sampling = convert.(Int, sampling)\n",
    "# ndm1 = sum(sampling) #number of neurons n_1\n",
    "# nnzindex = []\n",
    "# for i in 1:length(sampling)\n",
    "#     if sampling[i] != 0\n",
    "#         push!(nnzindex, i)\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# A = F[nnzindex,:]' # subsampled DCT\n",
    "# B = randn(x_dim, ndm1)/sqrt(x_dim) #random matrix thats used in convex combination\n",
    "\n",
    "# W1 = randn(ndm1, 20)/sqrt(ndm1) #random inner layer weight matrix\n",
    "\n",
    "# I(x) = x\n",
    "# z = randn(k) # ground code vector\n",
    "\n",
    "# β_list = .7:.01:1\n",
    "# trials = 20\n",
    "# recovery_error_matrix = zeros(length(β_list))\n",
    "# recon_error_matrix = zeros(length(β_list))\n",
    "# α_list = []\n",
    "\n",
    "# for trial in 1:trials\n",
    "#     recovery_error_list = []\n",
    "#     recon_error_list = []   \n",
    "#     α_list = []\n",
    "#     for β in β_list\n",
    "#         Aint = β*A + (1-β) * B\n",
    "\n",
    "#         #setup the generative network \n",
    "#         G = Chain(\n",
    "#             Dense(20, ndm1, relu, bias = false; init =(out,in) -> W1),\n",
    "#             Dense(ndm1, x_dim, I, bias = false; init =(out,in) -> Aint)\n",
    "#         )\n",
    "\n",
    "#         # subsampling DCT for measurement matrix\n",
    "#         sampling = rand(Bernoulli(.4), x_dim)\n",
    "#         sampling = convert.(Int, sampling)\n",
    "#         m = sum(sampling)\n",
    "#         nnzindex = []\n",
    "#         for i in 1:length(sampling)\n",
    "#             if sampling[i] != 0\n",
    "#                 push!(nnzindex, i)\n",
    "#             end\n",
    "#         end\n",
    "#         F_sub = F[nnzindex,:]*sqrt(x_dim)/sqrt(m)\n",
    "\n",
    "#         # measurement vector using subsampled DCT matrix\n",
    "#         y = F_sub*G(z)\n",
    "\n",
    "#         stepsize = .5\n",
    "#         tolerance = 1e-14\n",
    "#         max_iter = 2000\n",
    "#         out_toggle = 2001\n",
    "\n",
    "#         # run gradient descent to solve the least squares problem\n",
    "#         z_rec = GD_CS(G,y,F_sub, max_iter, stepsize, tolerance, out_toggle)\n",
    "#         recov_error = norm(z - real(z_rec))/norm(z)\n",
    "#         recon_error = norm(G(z) - G(real(z_rec)))/norm(G(z))\n",
    "#         Fn = F  \n",
    "#         α = maximum(sum((Fn*Aint)' .* (Fn*Aint)', dims = 1) ./ sqrt.(sum((Fn*Aint*Aint')' .* (Fn*Aint*Aint')', dims = 1)))\n",
    "\n",
    "#         # save results\n",
    "#         push!(recovery_error_list, recov_error)\n",
    "#         push!(recon_error_list, recon_error)\n",
    "#         push!(α_list, α)\n",
    "\n",
    "#     end\n",
    "#     #save results\n",
    "#     recovery_error_matrix =  hcat(recovery_error_matrix, recovery_error_list)\n",
    "#     recon_error_matrix = hcat(recon_error_matrix, recon_error_list)\n",
    "\n",
    "\n",
    "# end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.5",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
