{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87388d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BSON: @save\n",
    "using BSON: @load\n",
    "using CSV\n",
    "using DataFrames: DataFrame\n",
    "using Flux\n",
    "using Flux: logitbinarycrossentropy, binarycrossentropy\n",
    "using Flux.Data: DataLoader\n",
    "using Flux: chunk\n",
    "using ImageFiltering\n",
    "using MLDatasets: FashionMNIST\n",
    "using ProgressMeter: Progress, next!\n",
    "using Random\n",
    "using Zygote\n",
    "using MLDatasets\n",
    "using Images\n",
    "using ImageIO\n",
    "using LinearAlgebra\n",
    "using FFTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0efe9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a reshape layer to use in our decoder\n",
    "struct Reshape\n",
    "    shape\n",
    "end\n",
    "Reshape(args...) = Reshape(args)\n",
    "(r::Reshape)(x) = reshape(x, r.shape)\n",
    "Flux.@functor Reshape ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fbf96e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logit_BCE (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function get_train_loader(batch_size, shuffle::Bool)\n",
    "    # The MNIST training set is made up of 60k 28 by 28 greyscale images\n",
    "    train_x, train_y = MNIST.traindata(Float32)\n",
    "    train_x = 1 .- reshape(train_x, (784, :))\n",
    "    return DataLoader((train_x, train_y), batchsize=batch_size, shuffle=shuffle, partial=false)\n",
    "end\n",
    "\n",
    "function save_model(encoder_μ, encoder_logvar, W1, W2, W3, Q, save_dir::String, epoch::Int)\n",
    "    print(\"Saving model...\")\n",
    "    let encoder_μ = cpu(encoder_μ), encoder_logvar = cpu(encoder_logvar), W1 = cpu(W1), W2 = cpu(W2), W3 = cpu(W3), Q = cpu(Q)\n",
    "        @save joinpath(save_dir, \"model-$epoch.bson\") encoder_μ encoder_logvar W1 W2 W3 Q\n",
    "    end\n",
    "    println(\"Done\")\n",
    "end\n",
    "\n",
    "function create_vae()\n",
    "    # Define the encoder and decoder networks\n",
    "    encoder_features = Chain(\n",
    "        Dense(784,500, relu),\n",
    "        Dense(500,500, relu)\n",
    "    )\n",
    "    encoder_μ = Chain(encoder_features, Dense(500, 20))\n",
    "    encoder_logvar = Chain(encoder_features, Dense(500, 20))\n",
    "\n",
    "    W1 = randn(500,20)\n",
    "    W2 = randn(500,500)\n",
    "    W3 = randn(500,500)\n",
    "    Q = randn(784,500)\n",
    "\n",
    "    return encoder_μ, encoder_logvar, W1, W2, W3, Q\n",
    "end\n",
    "\n",
    "function BCE_own(x̂,x)\n",
    "    return sum( @.( -(1-x)*log(1 - (sigmoid(abs(1-x̂)) .- .5) * min((abs(x̂))^1.01, 1) + 1e-5)  - x*( log(1 - (sigmoid(abs(1-x̂)) .- .5)* min((abs(1-x̂))^1.01, 1)  + 1e-5) ) ) )/784\n",
    "end\n",
    "\n",
    "function  logit_BCE(x̂,x) \n",
    "    return sum(@.(- x * log( sigmoid(4x̂) + 1e-5) - (1-x)* log( 1 - sigmoid(4x̂) + 1e-5))) / size(x,2)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4fca59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function vae_loss(encoder_μ, encoder_logvar, W1, W2, W3, Q, x, β, λ, F)\n",
    "    batch_size = size(x)[end]\n",
    "    @assert batch_size != 0\n",
    "\n",
    "    # Forward propagate through mean encoder and std encoders\n",
    "    μ = encoder_μ(x)\n",
    "    logvar = encoder_logvar(x)\n",
    "    # Apply reparameterisation trick to sample latent\n",
    "    z = μ + randn(Float32, size(logvar)) .* exp.(0.5f0 * logvar)\n",
    "    # Reconstruct from latent sample\n",
    "\n",
    "    x̂ = Q*W3*relu(W2*relu(W1*z))\n",
    "\n",
    "    loss_α(F,A) = maximum(sqrt.(sum((F*A).*(F*A), dims = 2))) + 100*norm(A'*A - I(500),2)^2\n",
    "    α = loss_α(F, Q) \n",
    "\n",
    "    logp_x_z = sum(logit_BCE.(x̂, x))\n",
    "    loss_mse = sum(BCE_own.(x̂,x))\n",
    "    # Negative reconstruction loss Ε_q[logp_x_z]\n",
    "    # KL(qᵩ(z|x)||p(z)) where p(z)=N(0,1) and qᵩ(z|x) models the encoder i.e. reverse KL\n",
    "    # The @. macro makes sure that all operates are elementwise\n",
    "    kl_q_p = 0.5f0 * sum(@. (exp(logvar) + μ^2 - logvar - 1f0)) \n",
    "    # Weight decay regularisation term\n",
    "    reg = λ * sum(x->sum(x.^2), Flux.params(encoder_μ, encoder_logvar, W1, W2, W3, Q))\n",
    "    # We want to maximise the evidence lower bound (ELBO)\n",
    "    elbo = - β .* kl_q_p\n",
    "    # So we minimise the sum of the negative ELBO and a weight penalty\n",
    "    return -elbo + reg + 7000 * α + loss_mse + logp_x_z\n",
    "end\n",
    "\n",
    "function train(encoder_μ, encoder_logvar, W1, W2, W3, Q, dataloader, num_epochs, λ, β, optimiser, save_dir)\n",
    "    # The training loop for the model\n",
    "    trainable_params = Flux.params(encoder_μ, encoder_logvar, W1, W2, W3, Q)\n",
    "    progress_tracker = Progress(num_epochs, \"Training a epoch done\")\n",
    "    F = dct(diagm(ones(784)),2);\n",
    "\n",
    "    for epoch_num = 1:num_epochs\n",
    "        acc_loss = 0.0\n",
    "        loss = 0\n",
    "        for (x_batch, y_batch) in dataloader\n",
    "            # pullback function returns the result (loss) and a pullback operator (back)\n",
    "            loss, back = pullback(trainable_params) do\n",
    "                vae_loss(encoder_μ, encoder_logvar, W1, W2, W3, Q, x_batch, β, λ, F)\n",
    "            end\n",
    "            # Feed the pullback 1 to obtain the gradients and update then model parameters\n",
    "            gradients = back(1f0)\n",
    "            Flux.Optimise.update!(optimiser, trainable_params, gradients)\n",
    "            if isnan(loss)\n",
    "                break\n",
    "            end\n",
    "            acc_loss += loss\n",
    "        end\n",
    "        next!(progress_tracker; showvalues=[(:loss, loss)])\n",
    "        @assert length(dataloader) > 0\n",
    "        avg_loss = acc_loss / length(dataloader)\n",
    "        metrics = DataFrame(epoch=epoch_num, negative_elbo=avg_loss)\n",
    "        # println(metrics)\n",
    "        CSV.write(joinpath(save_dir, \"metrics.csv\"), metrics, header=(epoch_num==1), append=true)\n",
    "        save_model(encoder_μ, encoder_logvar, W1, W2, W3, Q, save_dir, epoch_num)\n",
    "    end\n",
    "    println(\"Training complete!\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "339ecd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: MNIST.traindata() is deprecated, use `MNIST(split=:train)[:]` instead.\n",
      "└ @ MLDatasets C:\\Users\\Babhru\\.julia\\packages\\MLDatasets\\Xb4Lh\\src\\datasets\\vision\\mnist.jl:187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining a epoch done   5%|██                            |  ETA: 0:46:36\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  9.202808451942344e12\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done   8%|███                           |  ETA: 0:45:22\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  3.077281112114e12\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  10%|████                          |  ETA: 0:44:13\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  1.1724977530470352e12\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  12%|████                          |  ETA: 0:42:55\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  4.7574395055131036e11\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  15%|█████                         |  ETA: 0:41:43\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  1.9860477691564874e11\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  18%|██████                        |  ETA: 0:40:29\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  8.3600105892617e10\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  20%|███████                       |  ETA: 0:39:15\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  3.499793110816193e10\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  22%|███████                       |  ETA: 0:38:02\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  1.4402176141004572e10\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  25%|████████                      |  ETA: 0:36:49\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  5.752504645854916e9\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  28%|█████████                     |  ETA: 0:35:35\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  2.1928631752660575e9\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  30%|██████████                    |  ETA: 0:34:22\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  7.77931111839157e8\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  32%|██████████                    |  ETA: 0:33:08\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  2.4671883903597137e8\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  35%|███████████                   |  ETA: 0:31:56\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6.546000438378808e7\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  38%|████████████                  |  ETA: 0:30:44\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  1.2994338656448854e7\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  40%|█████████████                 |  ETA: 0:29:33\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  1.61919208912352e6\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  42%|█████████████                 |  ETA: 0:28:22\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  130870.03244714117\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  45%|██████████████                |  ETA: 0:27:10\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  47544.24123510574\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  48%|███████████████               |  ETA: 0:25:58\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  46191.7737104646\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  50%|████████████████              |  ETA: 0:24:45\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  45311.74697486856\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  52%|████████████████              |  ETA: 0:23:33\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  26951.88541563232\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  55%|█████████████████             |  ETA: 0:22:20\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  24030.089761863386\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  58%|██████████████████            |  ETA: 0:21:07\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  22269.501100247177\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  60%|███████████████████           |  ETA: 0:19:54\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  21481.510945382477\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  62%|███████████████████           |  ETA: 0:18:41\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  22106.018206158955\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  65%|████████████████████          |  ETA: 0:17:27\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  23097.176892856074\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  68%|█████████████████████         |  ETA: 0:16:13\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  21266.235008736614\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  70%|██████████████████████        |  ETA: 0:14:59\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  21142.405806652016\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  72%|██████████████████████        |  ETA: 0:13:44\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  20783.778915989904\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  75%|███████████████████████       |  ETA: 0:12:30\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  20015.56607917035\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  78%|████████████████████████      |  ETA: 0:11:15\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  19368.926858833634\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  80%|█████████████████████████     |  ETA: 0:10:00\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  19537.88009996222\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  82%|█████████████████████████     |  ETA: 0:08:45\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  19041.569773248604\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  85%|██████████████████████████    |  ETA: 0:07:30\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  19546.626402149737\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  88%|███████████████████████████   |  ETA: 0:06:15\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  19560.433078338596\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  90%|████████████████████████████  |  ETA: 0:05:01\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  19411.088616142973\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  92%|████████████████████████████  |  ETA: 0:03:46\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  19987.473785416958\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  95%|█████████████████████████████ |  ETA: 0:02:31\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  19082.544023740924\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  98%|██████████████████████████████|  ETA: 0:01:16\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  19111.425096409526\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done 100%|██████████████████████████████| Time: 0:50:44\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  18984.288265831074\u001b[39m\u001b[K\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "shuffle_data = true\n",
    "η = 0.001\n",
    "β = 1f0\n",
    "λ = 0.01f0\n",
    "num_epochs = 40\n",
    "\n",
    "save_dir = \"trained_GNN/MNIST_ortho_inf\"\n",
    "# Define the model and create our data loader\n",
    "dataloader = get_train_loader(batch_size, shuffle_data)\n",
    "encoder_μ, encoder_logvar, W1, W2, W3, Q  = create_vae()\n",
    "train(encoder_μ, encoder_logvar, W1, W2, W3, Q, dataloader, num_epochs, λ, β, ADAM(η), save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26330dd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: @load not defined\nin expression starting at c:\\Users\\Babhru\\Documents\\GitHub\\GNN_Isometries\\train_vae_incoherent_orthogonal.ipynb:28",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: @load not defined\n",
      "in expression starting at c:\\Users\\Babhru\\Documents\\GitHub\\GNN_Isometries\\train_vae_incoherent_orthogonal.ipynb:28\n",
      "\n",
      "Stacktrace:\n",
      "  [1] top-level scope\n",
      "    @ :0\n",
      "  [2] eval\n",
      "    @ .\\boot.jl:373 [inlined]\n",
      "  [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base .\\loading.jl:1196\n",
      "  [4] #invokelatest#2\n",
      "    @ .\\essentials.jl:716 [inlined]\n",
      "  [5] invokelatest\n",
      "    @ .\\essentials.jl:714 [inlined]\n",
      "  [6] (::VSCodeServer.var\"#164#165\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer c:\\Users\\Babhru\\.vscode\\extensions\\julialang.language-julia-1.6.24\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:19\n",
      "  [7] withpath(f::VSCodeServer.var\"#164#165\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer c:\\Users\\Babhru\\.vscode\\extensions\\julialang.language-julia-1.6.24\\scripts\\packages\\VSCodeServer\\src\\repl.jl:184\n",
      "  [8] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer c:\\Users\\Babhru\\.vscode\\extensions\\julialang.language-julia-1.6.24\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:13\n",
      "  [9] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC c:\\Users\\Babhru\\.vscode\\extensions\\julialang.language-julia-1.6.24\\scripts\\packages\\JSONRPC\\src\\typed.jl:67\n",
      " [10] serve_notebook(pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; crashreporting_pipename::String)\n",
      "    @ VSCodeServer c:\\Users\\Babhru\\.vscode\\extensions\\julialang.language-julia-1.6.24\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:136\n",
      " [11] top-level scope\n",
      "    @ c:\\Users\\Babhru\\.vscode\\extensions\\julialang.language-julia-1.6.24\\scripts\\notebook\\notebook.jl:32\n",
      " [12] include(mod::Module, _path::String)\n",
      "    @ Base .\\Base.jl:418\n",
      " [13] exec_options(opts::Base.JLOptions)\n",
      "    @ Base .\\client.jl:292\n",
      " [14] _start()\n",
      "    @ Base .\\client.jl:495"
     ]
    }
   ],
   "source": [
    "function get_test_loader(batch_size, shuffle::Bool)\n",
    "    # The FashionMNIST test set is made up of 10k 28 by 28 greyscale images\n",
    "    test_x, test_y = MNIST.testdata(Float32)\n",
    "    test_x = 1 .- reshape(test_x, (784, :))\n",
    "    return DataLoader((test_x, test_y), batchsize=batch_size, shuffle=shuffle)\n",
    "end\n",
    "\n",
    "function save_to_images(x_batch, save_dir::String, prefix::String, num_images::Int64)\n",
    "    @assert num_images <= size(x_batch)[2]\n",
    "    for i=1:num_images\n",
    "        save(joinpath(save_dir, \"$prefix-$i.png\"), colorview(Gray, reshape(x_batch[:, i], 28,28)' ))\n",
    "    end\n",
    "end\n",
    "\n",
    "function reconstruct_images(encoder_μ, encoder_logvar, W1, W2, W3, Q, x)\n",
    "    # Forward propagate through mean encoder and std encoders\n",
    "    μ = encoder_μ(x)\n",
    "    logvar = encoder_logvar(x)\n",
    "    # Apply reparameterisation trick to sample latent\n",
    "    z = μ + randn(Float32, size(logvar)) .* exp.(0.5f0 * logvar)\n",
    "    # Reconstruct from latent sample\n",
    "    x̂ = Q*W3*relu(W2*relu(W1*z))\n",
    "    return clamp.(x̂, 0 ,1)\n",
    "end\n",
    "\n",
    "function load_model_identity(load_dir::String, epoch::Int)\n",
    "    print(\"Loading model...\")\n",
    "    @load joinpath(load_dir, \"model-$epoch.bson\") encoder_μ encoder_logvar W1 W2 W3 Q\n",
    "    println(\"Done\")\n",
    "    return encoder_μ, encoder_logvar, W1, W2, W3, Q\n",
    "end\n",
    "\n",
    "function visualise()\n",
    "    # Define some parameters\n",
    "    batch_size = 64\n",
    "    shuffle = true\n",
    "    num_images = 30\n",
    "    epoch_to_load = 30\n",
    "    # Load the model and test set loader\n",
    "    dir = \"trained_GNN/MNIST_ortho_inf\"\n",
    "    encoder_μ, encoder_logvar, W1, W2, W3, Q = load_model_identity(dir, epoch_to_load)\n",
    "    dataloader = get_test_loader(batch_size, shuffle)\n",
    "    # Reconstruct and save some images\n",
    "    for (x_batch, y_batch) in dataloader\n",
    "        save_to_images(x_batch, dir, \"test-image\", num_images)\n",
    "        x̂_batch = reconstruct_images(encoder_μ, encoder_logvar, W1, W2, W3, Q, x_batch)\n",
    "        save_to_images(x̂_batch, dir, \"reconstruction\", num_images)\n",
    "        break\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaf26eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: MNIST.testdata() is deprecated, use `MNIST(split=:test)[:]` instead.\n",
      "└ @ MLDatasets C:\\Users\\Babhru\\.julia\\packages\\MLDatasets\\Xb4Lh\\src\\datasets\\vision\\mnist.jl:195\n"
     ]
    }
   ],
   "source": [
    "visualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c4dff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAbtJREFUaAW9wb/LzAEAB+AHn0GSHwNFGRSxKmWwUu+mxMA/QFIGgzJZGMSmlGwim5kSg5QkZfAPvBnIgGyI4YbrutfdfW/4PE/+6oqyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyGOAjzuGVka04iF1YxWlcMVuURVmUxRwvcR+P8cek73hj7B0+4KH/i7Ioi7KY4ROO47e17cYGrBp7hGvYb21RFmVRFjO8xm/TruOqkZNYNWm//4uyKIuymOGusb24h2MmncUTY8fNFmVRFmUxwym8MPIAR017b9Izs0VZlEVZzHABV7EeB6zthmGiLMqiLObYiVvYbNo6k06YL8qiLMpijuf4iY1GfuEmfph23nxRFmVRFnPsMfIDr/ETe/HZtBXzRVmURVksaAtWjF026bDFRFmURVks6YhJby0myqIsymJJjy0nyqIsymJJT4zswBeLi7Ioi7JYwh9jZwwTZVEWZbGEr8a2GybKoizKYgl3jOzDNcNEWZRFWQx0G09xCSuGi7Ioi7IY6Cwu4iM24Ru2WVyURVmUxUC7jByynCiLsiiLsiiLsij7Bx/wNLqSmrQLAAAAAElFTkSuQmCC",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAbtJREFUaAW9wb/LzAEAB+AHn0GSHwNFGRSxKmWwUu+mxMA/QFIGgzJZGMSmlGwim5kSg5QkZfAPvBnIgGyI4YbrutfdfW/4PE/+6oqyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyGOAjzuGVka04iF1YxWlcMVuURVmUxRwvcR+P8cek73hj7B0+4KH/i7Ioi7KY4ROO47e17cYGrBp7hGvYb21RFmVRFjO8xm/TruOqkZNYNWm//4uyKIuymOGusb24h2MmncUTY8fNFmVRFmUxwym8MPIAR017b9Izs0VZlEVZzHABV7EeB6zthmGiLMqiLObYiVvYbNo6k06YL8qiLMpijuf4iY1GfuEmfph23nxRFmVRFnPsMfIDr/ETe/HZtBXzRVmURVksaAtWjF026bDFRFmURVks6YhJby0myqIsymJJjy0nyqIsymJJT4zswBeLi7Ioi7JYwh9jZwwTZVEWZbGEr8a2GybKoizKYgl3jOzDNcNEWZRFWQx0G09xCSuGi7Ioi7IY6Cwu4iM24Ru2WVyURVmUxUC7jByynCiLsiiLsiiLsij7Bx/wNLqSmrQLAAAAAElFTkSuQmCC\">"
      ],
      "text/plain": [
       "28×28 Array{Gray{N0f8},2} with eltype Gray{N0f8}:\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)  …  Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)  …  Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)  …  Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)  …  Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load(\"trained_GNN/MNIST_ortho/reconstruction-5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecd198a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAUlJREFUaAW9wUGO2kAABMCK1P9ieBn2y5h9GTnsAVkJYAPqqtx0RVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVl84IyJm/2iLMqiLD4wMRwTZVEWZXHAxMTA9OvkbmJ4LsqiLMpipzOmX6u7Fautm8eiLMqiLHY4Y/qOKIuyKIsXFkzfE2VRFmXxxILV1sD0viiLsiiLJ1Z3AxcMd3/86+K5KIuyKIsHFlsXDM8NLJ6LsiiLsnhg4AcnLPa5eC3KoizK4oGB4f8mpq0rhteiLMqiLN4wsdoa9omyKIuyeMNq62q/KIuyKIuDpq0Lhv2iLMqiLA462xqOibIoi7L4wMBwTJRFWZTFAYutk+OiLMqiLA5YbS2Oi7Ioi7LYafEdURZlURY7LfjB9OviPVEWZVEWB1x9LsqiLMqiLMqiLMr+AtSSJUGP87t2AAAAAElFTkSuQmCC",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAUlJREFUaAW9wUGO2kAABMCK1P9ieBn2y5h9GTnsAVkJYAPqqtx0RVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVl84IyJm/2iLMqiLD4wMRwTZVEWZXHAxMTA9OvkbmJ4LsqiLMpipzOmX6u7Fautm8eiLMqiLHY4Y/qOKIuyKIsXFkzfE2VRFmXxxILV1sD0viiLsiiLJ1Z3AxcMd3/86+K5KIuyKIsHFlsXDM8NLJ6LsiiLsnhg4AcnLPa5eC3KoizK4oGB4f8mpq0rhteiLMqiLN4wsdoa9omyKIuyeMNq62q/KIuyKIuDpq0Lhv2iLMqiLA462xqOibIoi7L4wMBwTJRFWZTFAYutk+OiLMqiLA5YbS2Oi7Ioi7LYafEdURZlURY7LfjB9OviPVEWZVEWB1x9LsqiLMqiLMqiLMr+AtSSJUGP87t2AAAAAElFTkSuQmCC\">"
      ],
      "text/plain": [
       "28×28 Array{Gray{N0f8},2} with eltype Gray{N0f8}:\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)  …  Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)  …  Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)  …  Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)  …  Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load(\"trained_GNN/MNIST_identity_v4/test-image-5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0f22c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...Done"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500-element Vector{Float64}:\n",
       " 34.66227367569999\n",
       " 34.234114128662966\n",
       " 33.70602652532124\n",
       " 33.52223257758422\n",
       " 33.38583885266336\n",
       " 33.22955528397366\n",
       " 33.19694283636157\n",
       " 32.79670871268523\n",
       " 32.64888375827221\n",
       " 32.43568728336859\n",
       "  ⋮\n",
       "  0.24839122226629223\n",
       "  0.22374869143107762\n",
       "  0.1636353531869051\n",
       "  0.14948293017470327\n",
       "  0.1341593408426007\n",
       "  0.0973370278492971\n",
       "  0.0843179295889063\n",
       "  0.018121437224013007\n",
       "  0.0053210094670351924"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function subspace_incoherence(F, A)\n",
    "    m, _ = size(A)\n",
    "    Q = Matrix(qr(A).Q)\n",
    "    temp = Q'*F'\n",
    "    return maximum(sqrt.(sum(temp.*temp, dims = 1)))\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "epoch_to_load = 20\n",
    "# Load the model and test set loader\n",
    "dir = \"trained_GNN/MNIST_ortho\"\n",
    "encoder_μ, encoder_logvar, W1, W2, W3, Q = load_model_identity(dir, epoch_to_load)\n",
    "\n",
    "colorview(Gray, reshape(Q*W3 *relu(W2*relu(W1*randn(20))), 28,28)' )\n",
    "F = dct(diagm(ones(784)),2);\n",
    "subspace_incoherence(F,Q)\n",
    "\n",
    "_,s,_, =svd(W3);\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b5ffec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999546021312976"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigmoid(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a1b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
